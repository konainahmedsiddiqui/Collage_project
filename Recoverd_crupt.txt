 2/1: import tensorflow.keras"
 2/2: import tensorflow.keras
 2/3: import cv2
 3/1: !git clone https://github.com/ayulockin/deepimageinpainting.git
 3/2: !git clone https://github.com/ayulockin/deepimageinpainting.git
 4/1: !git clone https://github.com/ayulockin/deepimageinpainting.git
 4/2:
import tensorflow as tf
print(tf.__version__)
from tensorflow import keras

print('[INFO]', tf.config.experimental.list_physical_devices('GPU')[0])
 4/3: len(tf.config.experimental.list_physical_devices('GPU')[0])
 5/1:
import tensorflow as tf
print(tf.__version__)
from tensorflow import keras

print('[INFO]', tf.config.experimental.list_physical_devices('GPU')[0])
 5/2:
import tensorflow as tf
print(tf.__version__)
from tensorflow import keras

print('[INFO]', tf.config.experimental.list_physical_devices('GPU')[0][0])
 5/3:
import tensorflow as tf
print(tf.__version__)
from tensorflow import keras

print('[INFO]', tf.config.experimental.list_physical_devices('GPU')[0][0][0])
 6/1: import cv2
 6/2:
import cv2
import os
 6/3:
path = r'C:\Users\Dell\Downloads\Eye iris dataset\Eye iris dataset'
for i in os.path(path):
    print(i)
 6/4:
path = r'C:\Users\Dell\Downloads\Eye iris dataset\Eye iris dataset'
for i in os.path.dir(path):
    print(i)
 6/5:
path = r'C:\Users\Dell\Downloads\Eye iris dataset\Eye iris dataset'
for i in os.lsdir(path):
    print(i)
 6/6:
path = r'C:\Users\Dell\Downloads\Eye iris dataset\Eye iris dataset'
for i in os.listdir(path):
    print(i)
 6/7: cv2.imread('C:\Users\Dell\Downloads\Eye iris dataset\Eye iris dataset\3522560455_bcabdfef90.jpg')
 6/8: cv2.imread(r'C:\Users\Dell\Downloads\Eye iris dataset\Eye iris dataset\3522560455_bcabdfef90.jpg')
 6/9:
l = 'sdfsf.jpeg'
l[:-4]
6/10:
l = 'sdfsf.jpeg'
l[:-5]
6/11:
l = 'sdfsf.jpeg'
l[-4:]
6/12:
l = 'sdfsf.jpeg'
l[:-5]
6/13:
l = 'sdfsf.jpg'
l[:-5]
6/14:
l = 'sdfsf.jpg'
l[:-4]
6/15:
path = r'C:\Users\Dell\Downloads\Eye iris dataset\Eye iris dataset'
dest = r'C:\Users\Dell\Downloads\Eye iris dataset\resized'
for i in os.listdir(path):
    img = cv2.imread(path+'\\'+i)
    resized = cv2.resize(img, (46,45), interpolation = cv2.INTER_AREA)
    if i[-4:] == 'jpeg':
        cv2.imwrite(dest+'\\'+i[[:-5]]+'_r.jpg',resized)
    else:
        cv2.imwrite(dest+'\\'+i[[:-4]]+'_r.jpg',resized)
6/16:
path = r'C:\Users\Dell\Downloads\Eye iris dataset\Eye iris dataset'
dest = r'C:\Users\Dell\Downloads\Eye iris dataset\resized'
for i in os.listdir(path):
    img = cv2.imread(path+'\\'+i)
    resized = cv2.resize(img, (46,45), interpolation = cv2.INTER_AREA)
    if i[-4:] == 'jpeg':
        cv2.imwrite(dest+'\\'+i[:-5]+'_r.jpg',resized)
    else:
        cv2.imwrite(dest+'\\'+i[:-4]+'_r.jpg',resized)
6/17:
path = r'C:\Users\Dell\Downloads\Eye iris dataset\Eye iris dataset'
dest = r'C:\Users\Dell\Downloads\Eye iris dataset\resized'
for i in os.listdir(path):
    img = cv2.imread(path+'\\'+i)
    resized = cv2.resize(img, (46,45), interpolation = cv2.INTER_AREA)
    if i[-4:] == 'jpeg':
        cv2.imwrite(dest+'\\'+i[:-5]+'_r.jpg',resized)
    else:
        cv2.imwrite(dest+'\\'+i[:-4]+'_r.jpg',resized)
print('done.')
6/18:
path = r'C:\Users\Dell\Downloads\Eye iris dataset\Eye iris dataset'
dest = r'C:\Users\Dell\Downloads\Eye iris dataset\resized'
for i in os.listdir(path):
    img = cv2.imread(path+'\\'+i)
    resized = cv2.resize(img, (47,46), interpolation = cv2.INTER_AREA)
    if i[-4:] == 'jpeg':
        cv2.imwrite(dest+'\\'+i[:-5]+'_r.jpg',resized)
    else:
        cv2.imwrite(dest+'\\'+i[:-4]+'_r.jpg',resized)
print('done.')
 9/1:
from __future__ import print_function
import matplotlib.pyplot as plt
%matplotlib inline

import os
# os.environ['CUDA_VISIBLE_DEVICES'] = '1'

import numpy as np
from models.resnet import ResNet
from models.unet import UNet
from models.skip import skip
import torch
import torch.optim

from utils.inpainting_utils import *

torch.backends.cudnn.enabled = True
torch.backends.cudnn.benchmark =True
dtype = torch.cuda.FloatTensor

PLOT = True
imsize = -1
dim_div_by = 64
10/1:
from __future__ import print_function
import matplotlib.pyplot as plt
%matplotlib inline

import os
# os.environ['CUDA_VISIBLE_DEVICES'] = '1'

import numpy as np
from models.resnet import ResNet
from models.unet import UNet
from models.skip import skip
import torch
import torch.optim

from utils.inpainting_utils import *

torch.backends.cudnn.enabled = True
torch.backends.cudnn.benchmark =True
dtype = torch.cuda.FloatTensor

PLOT = True
imsize = -1
dim_div_by = 64
10/2:
## Fig 6
# img_path  = 'data/inpainting/vase.png'
# mask_path = 'data/inpainting/vase_mask.png'

## Fig 8
# img_path  = 'data/inpainting/library.png'
# mask_path = 'data/inpainting/library_mask.png'

## Fig 7 (top)
img_path  = 'data/inpainting/kate.png'
mask_path = 'data/inpainting/kate_mask.png'

# Another text inpainting example
# img_path  = 'data/inpainting/peppers.png'
# mask_path = 'data/inpainting/peppers_mask.png'

NET_TYPE = 'skip_depth6' # one of skip_depth4|skip_depth2|UNET|ResNet
10/3:
img_pil, img_np = get_image(img_path, imsize)
img_mask_pil, img_mask_np = get_image(mask_path, imsize)
10/4:
img_mask_pil = crop_image(img_mask_pil, dim_div_by)
img_pil      = crop_image(img_pil,      dim_div_by)

img_np      = pil_to_np(img_pil)
img_mask_np = pil_to_np(img_mask_pil)
10/5:
img_mask_var = np_to_torch(img_mask_np).type(dtype)

plot_image_grid([img_np, img_mask_np, img_mask_np*img_np], 3,11);
11/1:
path = r'C:\Users\Dell\Downloads\Eye iris dataset\Eye iris dataset'
dest = r'C:\Users\Dell\Downloads\Eye iris dataset\resized'
for i in os.listdir(path):
    img = cv2.imread(path+'\\'+i)
    resized = cv2.resize(img, (64,64), interpolation = cv2.INTER_AREA)
    if i[-4:] == 'jpeg':
        cv2.imwrite(dest+'\\'+i[:-5]+'_r.jpg',resized)
    else:
        cv2.imwrite(dest+'\\'+i[:-4]+'_r.jpg',resized)
print('done.')
11/2:
import cv2
import os
11/3:
path = r'C:\Users\Dell\Downloads\Eye iris dataset\Eye iris dataset'
dest = r'C:\Users\Dell\Downloads\Eye iris dataset\resized'
for i in os.listdir(path):
    img = cv2.imread(path+'\\'+i)
    resized = cv2.resize(img, (64,64), interpolation = cv2.INTER_AREA)
    if i[-4:] == 'jpeg':
        cv2.imwrite(dest+'\\'+i[:-5]+'_r.jpg',resized)
    else:
        cv2.imwrite(dest+'\\'+i[:-4]+'_r.jpg',resized)
print('done.')
12/1:
import pandas as pd
data = pd.read_csv('data.csv')
data.head()
12/2: a = data['Machine Type']
12/3:
a = data['Machine Type']
a
12/4:
a = list(a)
a.unique
12/5:
a = list(a)
a.unique()
12/6:
import pandas as pd
import numpy as np
data = pd.read_csv('data.csv')
data.head()
12/7:
a = list(a)
np.unique(a)
12/8:
a = data['Additional Info']
a
12/9:
a = list(a)
np.unique(a)
12/10: data.tail()
12/11: data[46:50]
12/12: data[45:50]
12/13: data[46:51]
12/14: data[76:81]
12/15: data[96:101]
12/16: data[206:211]
12/17:
if 'Mill  is off' in a:
    print(a.index)
12/18:
if 'Mill  is off' in a:
    print(true)
12/19:
if 'Mill is off' in a:
    print(true)
12/20:
if ' Mill  is off' in a:
    print(a.index)
12/21:
if ' Mill  is off' in a:
    print(a.index())
12/22:
if ' Mill  is off' in a:
    print(True))
12/23:
if ' Mill  is off' in a:
    print(True)
12/24:
if ' Mill  is off' in a:
    print(a)
12/25:
if ' Mill  is off' in a:
    print(a.index())
12/26: data['Additional Info'] == ' Mill  is off'
12/27: type(b)
12/28: b = data['Additional Info'] == ' Mill  is off'
12/29: type(b)
12/30:
b = data['Additional Info'] == ' Mill  is off'
b = numpy.array(b)
np.where(b == True)
12/31:
b = data['Additional Info'] == ' Mill  is off'
b = np.array(b)
np.where(b == True)
12/32: data[230:241]
12/33:
b = data['Additional Info'] == 'mill stop'
b = np.array(b)
np.where(b == True)
12/34: data[30:36]
12/35: data[80:86]
12/36:
b = data['Additional Info'] == ' Mill  is off'
b = np.array(b)
np.where(b == True)
12/37:
b = data['Additional Info'] == 'Prolong shutdown of mill\r'
b = np.array(b)
np.where(b == True)
12/38:
import pandas as pd
import numpy as np
data = pd.read_csv('data.csv')
data.head(11)
12/39:
import seaborn as sns
corr = data.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="YlOrRd")
12/40: data[30:34]
12/41: data[57:58]
12/42: data[80:84]
12/43: filled = data.dropna
12/44: filled.info
12/45: filled.info()
12/46: filled.describe
12/47: filled.describe()
12/48: filled.describe
12/49: type(filled)
12/50: filled
12/51: filled = pd.DataFrame(data.dropna)
12/52: filled = data.dropna
12/53: pd.DataFrame(filled)
12/54: filled
12/55: data.ifo
12/56: data.info
12/57: data.describe
12/58: data.size
12/59: data.shape
12/60: data.dropna
12/61: data.shape
12/62: data.dropna.shape
12/63: data.dropna(inplace=True)
12/64: data.shape
12/65: data
12/66: data.drop([" Prolong shutdown of mill\r","mill stop","Mill is off"])
12/67: data.drop([[" Prolong shutdown of mill\r","mill stop","Mill is off"]])
12/68: data['Additional Info'].drop([" Prolong shutdown of mill\r","mill stop","Mill is off"])
12/69: data['Additional Info'].drop([" Prolong shutdown of mill\r"])
12/70: data['Additional Info'].drop(" Prolong shutdown of mill\r","mill stop","Mill is off")
12/71: data.drop(" Prolong shutdown of mill\r","mill stop","Mill is off")
12/72: data.dropna
12/73:
import pandas as pd
import numpy as np
data = pd.read_csv('data.csv')
data.head(11)
12/74: data['Inlet Pressure (Bars)'] > 98
12/75: data[['Inlet Pressure (Bars)']] > 98
12/76: data['Inlet Pressure (Bars)'] > 98
12/77: c = data['Inlet Pressure (Bars)'] > 98
12/78:
c = data['Inlet Pressure (Bars)'] > 98
c = np.array(c)
np.where(b == True)
12/79:
c = data['Inlet Pressure (Bars)'] > 98
c = np.array(c)
np.where(c == True)
12/80: data.shape
12/81: data[537]
12/82: data[538:539]
12/83:
c = data['Inlet Pressure (Bars)'] > 96
c = np.array(c)
np.where(c == True)
12/84:
c = data['Inlet Pressure (Bars)'] > 93
c = np.array(c)
np.where(c == True)
12/85:
c = data['Inlet Pressure (Bars)'] > 90
c = np.array(c)
np.where(c == True)
12/86:
c = data['Inlet Pressure (Bars)'] > 89
c = np.array(c)
np.where(c == True)
12/87:
c = data['Inlet Pressure (Bars)'] > 85
c = np.array(c)
np.where(c == True)
12/88:
c = data['Inlet Pressure (Bars)'] > 81
c = np.array(c)
np.where(c == True)
12/89:
c = data['Inlet Pressure (Bars)'] > 77
c = np.array(c)
np.where(c == True)
12/90:
c = data['Inlet Pressure (Bars)'] > 67
c = np.array(c)
np.where(c == True)
12/91:
c = data['Inlet Pressure (Bars)'] > 70
c = np.array(c)
np.where(c == True)
12/92:
c = data['Inlet Pressure (Bars)'] > 71
c = np.array(c)
np.where(c == True)
12/93:
c = data['Inlet Pressure (Bars)'] > 74
c = np.array(c)
np.where(c == True)
12/94:
c = data['Inlet Pressure (Bars)'] > 99
c = np.array(c)
np.where(c == True)
12/95: data[530:541]
12/96: data[538:548]
12/97: data['Remarks']
12/98: data.tail()
13/1:
import pandas as pd
import numpy as np
data = pd.read_csv('data.csv')
data.head(11)
13/2:
for i in data[:4]:
    print(i)
13/3:
for i in data[:3]:
    print(i)
13/4:
for i in data[:44]:
    print(i)
13/5:
for i in data[[:44]]:
    print(i)
13/6:
for i in data:
    print(i)
13/7:
for i in data(axis=1):
    print(i)
13/8:
for i in data['Timestamp'][:3]:
    print(i)
13/9:
# y = int(input('Foot: '))
# u = int(input('Inches: '))
# d = input('Weight: ')
# z = (y*12)+u
data[(data['Remarks'] == 'good') & (data['Remarks.1'] == d+'good') & (data['Remarks.2'] == 'good')]
13/10:
# y = int(input('Foot: '))
# u = int(input('Inches: '))
# d = input('Weight: ')
# z = (y*12)+u
data[(data['Remarks'] == 'good') & (data['Remarks.1'] == 'good') & (data['Remarks.2'] == 'good')]
13/11:
# y = int(input('Foot: '))
# u = int(input('Inches: '))
# d = input('Weight: ')
# z = (y*12)+u
good = data[(data['Remarks'] == 'good') & (data['Remarks.1'] == 'good') & (data['Remarks.2'] == 'good')]
13/12: good
13/13: good.shape
13/14: good.tail()
13/15:
IP = list(good['Inlet Pressure (Bars)'])
IP
13/16:
IP = list(good['Inlet Pressure (Bars)'])
max(IP)
13/17:
IP = list(good['Inlet Pressure (Bars)']) #  - 59.46261797
min(IP)
13/18:
IT = list(good['Inlet Temperature (Degree Celsius)']) # 30.2400491 - 59.46261797
min(IT)
13/19:
IT = list(good['Inlet Temperature (Degree Celsius)']) # 33.13476563 - 59.46261797
max(IT)
13/20:
OT = list(good['Outlet Temperature (Degree Celsius)']) # 33.13476563 - 54.21132406
min(OT)
13/21:
OT = list(good['Outlet Temperature (Degree Celsius)']) # 38.41053356 - 54.21132406
max(OT)
13/22: good.index
13/23: list(good.index)
13/24:
for i in good:
    print(i)
13/25:
for i in good['Remarks'] == 'good':
    print(i)
13/26:
for i in good[good['Remarks'] == 'good']:
    print(i)
13/27: good.loc[good.Remarks = 'good', 'Additional Info'] = 'Mill working'
13/28: good.loc[good.Remarks == 'good', 'Additional Info'] = 'Mill working'
13/29: good
13/30: good.loc[1]
13/31: good[1]
13/32: good[1,axia=1]
13/33: good[1,axis=1]
13/34: good.iloc[1]
13/35: good.loc[good.Remarks == 'good' and good.Remarks.1 == 'good', 'Additional Info'] = 'Mill working'
13/36: good.loc[good.Remarks == 'good' and good['Remarks.1'] == 'good', 'Additional Info'] = 'Mill working'
13/37:
import pandas as pd
import numpy as np
data = pd.read_csv('data.csv')
data.head(11)
13/38:
# y = int(input('Foot: '))
# u = int(input('Inches: '))
# d = input('Weight: ')
# z = (y*12)+u
good = data[(data['Remarks'] == 'good') & (data['Remarks.1'] == 'good') & (data['Remarks.2'] == 'good')]
13/39:
# y = int(input('Foot: '))
# u = int(input('Inches: '))
# d = input('Weight: ')
# z = (y*12)+u
good = data[(data['Remarks'] == 'good') & (data['Remarks1'] == 'good') & (data['Remarks2'] == 'good')]
13/40: good.shape
13/41: good.tail()
13/42: list(good.index)
13/43: good.loc[good.Remarks == 'good' and good.Remarks1 == 'good', 'Additional Info'] = 'Mill working'
13/44: good.loc[[good.Remarks == 'good' and good.Remarks1 == 'good'], 'Additional Info'] = 'Mill working'
13/45: good.loc[(good.Remarks == 'good') and (good.Remarks1 == 'good'), 'Additional Info'] = 'Mill working'
13/46: good.loc[good.Remarks and good.Remarks1 == 'good', 'Additional Info'] = 'Mill working'
13/47: good.loc[good.Remarks == 'good', good.Remarks1 == 'good', 'Additional Info'] = 'Mill working'
13/48: good.loc[((good.Remarks == 'good') and (good.Remarks1 == 'good')), 'Additional Info'] = 'Mill working'
13/49: good.loc[(good.Remarks == 'good') and (good.Remarks1 == 'good'), 'Additional Info'] = 'Mill working'
13/50: good.loc[good.Remarks == 'good', 'Additional Info'] = 'Mill working'
13/51: good.iloc[1]
13/52: type(good)
13/53: type(data)
13/54: type(good)
13/55: data.shape
13/56: good.shape
13/57:
data.dropna(inplace=True)
data.shape
13/58: 227+32
13/59: data.head()
13/60: data.head(7:14)
13/61: data.head(:14)
13/62: data.head(14)
13/63:
data.dropna('Additional Info',axis=1)
data.shape
13/64:
data['Additional Info'.dropna(axis=1)
data.shape
13/65:
data['Additional Info'.dropna(axis=1,inplace=True)
# data.shape
13/66:
data['Additional Info'].dropna(axis=1,inplace=True)
data.shape
13/67: data.info
13/68: data.describe
13/69: data.describe()
13/70: data['Additional Info'].describe()
13/71: data['Additional Info'].info()
13/72:
import pandas as pd
import numpy as np
data = pd.read_csv('data.csv')
data.head(11)
13/73: data['Additional Info'].info()
13/74: data.trail()
13/75: data.tail()
13/76: 573
13/77: 573-32
13/78: 573-32-227
13/79: good
13/80: data.merge(good, left_on='Timestamp')
13/81: data.merge(good, left_on='Timestamp', right_on='Machine Type')
13/82: data
13/83: pd.concat([data, good]
13/84: pd.concat([data, good])
13/85:
tryed =pd.concat([data, good])
tryed.dropna
tryed.shape
13/86:
tryed =pd.concat([data, good])
tryed.dropna(inplace=True)
tryed.shape
13/87: data.shape
13/88: data.tail(1)
13/89: data.tail()
13/90: data.loc(['Inlet Pressure (Bars)'])
13/91: data['Inlet Pressure (Bars)']
13/92: data['Inlet Pressure (Bars)']+data['Inlet Temperature (Degree Celsius)']
13/93: (data['Inlet Pressure (Bars)']+data['Inlet Temperature (Degree Celsius)']+data['Outlet Temperature (Degree Celsius)'])/3
13/94: round((data['Inlet Pressure (Bars)']+data['Inlet Temperature (Degree Celsius)']+data['Outlet Temperature (Degree Celsius)'])/3,2)
13/95: data['Average'] = average
13/96: average = round((data['Inlet Pressure (Bars)']+data['Inlet Temperature (Degree Celsius)']+data['Outlet Temperature (Degree Celsius)'])/3,2)
13/97: data['Average'] = average
13/98: data.head()
13/99: data.head(12)
13/100: data.head(13)
13/101: data[data['Remarks'] == 'good']
13/102: data[data['Average'] > 98]
13/103: data[data['Average'] > 72]
13/104: data[data['Average'] > 69]
13/105: data[data['Average'] > 50]
13/106: data[data['Average'] < 20]
13/107: data[data['Average'] < 30]
13/108: data['Average'].describe()
13/109: average = round(((data['Inlet Pressure (Bars)']*0.7)+(data['Inlet Temperature (Degree Celsius)']*0.15)+(data['Outlet Temperature (Degree Celsius)']*0.15))/3,2)
13/110: data.drop('Average',axis=1)
13/111: data
13/112: data['Average'] = average
13/113: data.head(13)
13/114: data[data['Average'] < 90]
13/115: data[data['Average'] > 90]
13/116: data[data['Average'] > 88]
13/117: data[data['Average'] > 48]
13/118: data[data['Average'] > 40]
13/119: average = round(((data['Inlet Pressure (Bars)']*1.4)+(data['Inlet Temperature (Degree Celsius)']*0.3)+(data['Outlet Temperature (Degree Celsius)']*0.3))/3,2)
13/120: data.drop('Average',axis=1,inplace=True)
13/121: data['Average'] = average
13/122: data.head(13)
13/123: data[data['Average'] > 90]
13/124: data[data['Average'] > 66]
13/125: data[data['Average'] > 59]
13/126: data[data['Average'] > 50]
13/127: data[data['Average'] > 45]
13/128: data[data['Average'] > 48]
13/129: average = round(((data['Inlet Pressure (Bars)']*2.1)+(data['Inlet Temperature (Degree Celsius)']*0.45)+(data['Outlet Temperature (Degree Celsius)']*0.45))/3,2)
13/130: data.drop('Average',axis=1,inplace=True)
13/131: data['Average'] = average
13/132: data.head(13)
13/133: data[data['Average'] > 48]
13/134: data[data['Average'] > 100]
13/135: data.drop('Average',axis=1,inplace=True)
13/136: 2.5*.75
13/137: 2.5*.15
13/138: average = round(((data['Inlet Pressure (Bars)']*1.875)+(data['Inlet Temperature (Degree Celsius)']*0.375)+(data['Outlet Temperature (Degree Celsius)']*0.375))/3,2)
13/139: data['Average'] = average
13/140: data.head(13)
13/141: data[data['Average'] > 100]
13/142: data.head(13)
13/143: data[data['Average'] > 71]
13/144: data[data['Average'] > 60]
13/145:
import pandas as pd
import numpy as np
data = pd.read_csv('data.csv')
data.head(11)
13/146:
# y = int(input('Foot: '))
# u = int(input('Inches: '))
# d = input('Weight: ')
# z = (y*12)+u
good = data[(data['Remarks'] == 'good') & (data['Remarks1'] == 'good') & (data['Remarks2'] == 'good')]
13/147: good.loc[good.Remarks == 'good', 'Additional Info'] = 'Mill working'
13/148: result = pd.concat([data, good], axis=1)
13/149: result.shape
13/150: result
13/151: pd.merge(data,good, on='Timestamp')
13/152:
xyz = pd.merge(data,good, on='Timestamp')
xyz.shape
13/153: xyz = pd.merge(data,good, on='Timestamp')
13/154: xyz
13/155:
xyz = pd.concate([data,good])
xyz.shape
13/156: xyz
13/157:
xyz = pd.concat([data,good])
xyz.shape
13/158: xyz
13/159:
xyz = pd.concat([data,good],on='Timestamp')
xyz.shape
13/160:
xyz = pd.concat([data,good],keys='Timestamp')
xyz.shape
13/161: xyz
13/162:
xyz = pd.join([data,good],on='Timestamp')
xyz.shape
13/163:
xyz = pd.concat([data,good],join="outer")
xyz.shape
13/164: xyz
13/165:
for i in xyz:
    print(i)
13/166: xyz.tail()
13/167:
xyz = pd.concat([data,good],join="Timestamp")
xyz.shape
13/168: data['Average'].describe()
13/169: average = round(((data['Inlet Pressure (Bars)']*1.875)+(data['Inlet Temperature (Degree Celsius)']*0.375)+(data['Outlet Temperature (Degree Celsius)']*0.375))/3,2)
13/170: data['Average'] = average
13/171: data[data['Average'] > 60]
13/172: data['Average']
13/173: data['Average'].describe()
13/174:
xyz = pd.concat([data,good],join="Timestamp")
xyz.shape
13/175:
xyz = pd.concat([data,good],join="Left")
xyz.shape
13/176:
xyz = pd.concat([data,good],join="_lLeft")
xyz.shape
13/177:
xyz = pd.concat([data,good],join="_l")
xyz.shape
13/178:
import pandas as pd
import numpy as np
data = pd.read_csv('data.csv')
data.head(11)
13/179:
# y = int(input('Foot: '))
# u = int(input('Inches: '))
# d = input('Weight: ')
# z = (y*12)+u
good = data[(data['Remarks'] == 'good') & (data['Remarks1'] == 'good') & (data['Remarks2'] == 'good')]
13/180: good.shape
13/181: good.tail()
13/182: good.loc[good.Remarks == 'good', 'Additional Info'] = 'Mill working'
13/183: good.iloc[1]
13/184: good.iloc[0]
13/185: good
13/186: data.head()
13/187: data['Average'] = average
13/188: data[data['Average'] > 100]
13/189: data['Average'].describe()
13/190: data.head()
13/191: data.Average
13/192: data[data.Average > 34]
13/193: data[data.Average > 30]
13/194: data[data.Average > 30].shape
13/195: 573-417
13/196: 1.875/3
13/197: 1.875*3
13/198: 1.875*1/3
13/199: 1.875/3
13/200: 1.875+0.375+0.375
13/201: 2.625*(1.875/2.625)
13/202: 2.625*(1.875/2.625)*100
13/203: .7*3
13/204: .7*3.5
13/205: .153
13/206: .15*3
13/207: .7*3
13/208: .7*2
13/209: 2.1*2
13/210: data[data['Average'] > 100]
13/211: data[data['Average'] > 70]
13/212: data[data['Average'] > 65]
13/213: data[data['Average'] > 62]
13/214: data[data['Average'] > 63]
13/215: 1.850+0.375+0.375
13/216: 1.840+0.375+0.375
13/217: 1.750+0.375+0.375
13/218: data.drop('Average',axis=1,inplace=True)
13/219: average = round(((data['Inlet Pressure (Bars)']*1.75)+(data['Inlet Temperature (Degree Celsius)']*0.375)+(data['Outlet Temperature (Degree Celsius)']*0.375))/3,2)
13/220: data['Average'] = average
13/221: data[data['Average'] > 63]
13/222: data[data['Average'] > 60]
13/223: data[data['Average'] > 30 and data['Average'] < 35]
13/224: data[(data['Average'] > 30) and (data['Average'] < 35)]
13/225: data[data['Average'] > 30][:5]
13/226: data[data['Average'] > 30][6:11]
13/227: data[data['Average'] > 30][7:12]
13/228: 2.5*.7
13/229: 2.5*.15
13/230: 2.5*.15
13/231: 2.5*.6
13/232: 2.5*.4
13/233: 2.5*.6
13/234: 2.5*.6 + 2.5*.4
13/235: data[data['Average'] > 50][7:12]
13/236: data[data['Average'] > 29][7:12]
13/237: data[data['Average'] > 29][12:18]
13/238: data[data['Average'] > 39][12:18]
13/239: data[data['Average'] > 30][12:18]
13/240: data[data['Inlet Pressure (Bars)'] > 30][12:18]
13/241: data[data['Inlet Pressure (Bars)'] > 30][:5]
13/242: data[data['Inlet Pressure (Bars)'] > 220][:5]
13/243: data[data['Inlet Pressure (Bars)'] > 22][:5]
13/244: data[data['Inlet Pressure (Bars)'] = 30][:5]
13/245: data[data['Inlet Pressure (Bars)'] == 30][:5]
13/246: data[round(data['Inlet Pressure (Bars)']) == 30][:5]
13/247: data[round(data['Average']) == 30][:5]
13/248: data[round(data['Average']) == 31][:5]
13/249: data[round(data['Average']) == 32][:5]
13/250: data[round(data['Average']) == 29][:5]
13/251: data[round(data['Average']) == 30][:5]
13/252: data[round(data['Average']) == 31][:5]
13/253: data[round(data['Average']) == 35][:5]
13/254: data[round(data['Average']) == 35][6:11]
13/255: data[round(data['Average']) == 35][11:16]
13/256: data[round(data['Average']) == 33][11:16]
13/257: data[round(data['Average']) == 33][5:11]
13/258: data[round(data['Average']) == 32][5:11]
13/259: data[round(data['Average']) == 33][5:11]
13/260: data[round(data['Average']) == 33][2:7]
13/261: data[round(data['Average']) == 33][4:9]
13/262: data[round(data['Average']) == 33][8:13]
13/263: data[round(data['Average']) == 33][-5:]
13/264: data[round(data['Average']) == 33][-5:]
13/265: data[round(data['Average']) == 63]#[-5:]
13/266: data[round(data['Average']) == 53]#[-5:]
13/267: data[round(data['Average']) > 63]#[-5:]
13/268: data[round(data['Average']) > 53]#[-5:]
13/269: data[round(data['Average']) > 57]#[-5:]
13/270: data[round(data['Average']) > 59]#[-5:]
13/271: data.head()
13/272:
for i in data['Average'][:1]:
    print(i)
13/273:
result = []
for i in data['Average'][:1]:
    if i in range(0,32):
        result.append('Mill on Stop')
    
    elif i in range(33,60):
        result.append('Good Condition')
    
    else:
        result.append('Excellent')
print(result)
13/274:
result = []
for i in data['Average'][:4]:
    if i in range(0,32):
        result.append('Mill on Stop')
    
    elif i in range(33,60):
        result.append('Good Condition')
    
    else:
        result.append('Excellent')
print(result)
13/275:
v = int(input('Num: '))
if v in range(0,10):
    print('Yes')
13/276:
v = int(input('Num: '))
if v in range(0,10):
    print('Yes')
13/277:
result = []
for i in data['Average'][:4]:
    if i in range(0.0,32.0):
        result.append('Mill on Stop')
    
    elif i in range(33,60):
        result.append('Good Condition')
    
    else:
        result.append('Excellent')
print(result)
13/278:
result = []
for i in data['Average'][:4]:
    if i in range(0,32):
        print(i)
        result.append('Mill on Stop')
    
    elif i in range(33,60):
        result.append('Good Condition')
    
    else:
        result.append('Excellent')
print(result)
13/279:
result = []
for i in data['Average'][:4]:
    print(i)
    if i in range(0,32):
        result.append('Mill on Stop')
    
    elif i in range(33,60):
        result.append('Good Condition')
    
    else:
        result.append('Excellent')
print(result)
13/280:
result = []
for i in data['Average'][:4]:
    print(i)
    if i in range(0.0,32.0):
        result.append('Mill on Stop')
    
    elif i in range(33.0,60.0):
        result.append('Good Condition')
    
    else:
        result.append('Excellent')
print(result)
13/281:
result = []
for i in data['Average'][:4]:
    if i > 0 and i < 32:
        result.append('Mill on Stop')
    
    elif i > 33 and i < 60:
        result.append('Good Condition')
    
    else:
        result.append('Excellent')
print(result)
13/282:
result = []
for i in data['Average']:
    if i > 0 and i < 32:
        result.append('Mill on Stop')
    
    elif i > 33 and i < 60:
        result.append('Good Condition')
    
    else:
        result.append('Excellent')
print(result)
13/283:
result = []
for i in data['Average']:
    if i > 0 and i < 32:
        result.append('Mill on Stop')
    
    elif i > 33 and i < 60:
        result.append('Good Condition')
    
    else:
        result.append('Excellent')
13/284: data['Result'] = result
13/285: data.head()
13/286: data[round(data['Average']) > 59]
13/287: data['Inlet Pressure (Bars)','Inlet Temperature (Degree Celsius)','Average']
13/288: data.drop(['Timestamp','Machine Type','Remarks','Remarks1','Remarks2','Additional Info'])
13/289: data.drop([['Timestamp','Machine Type','Remarks','Remarks1','Remarks2','Additional Info']])
13/290: data.drop[['Timestamp','Machine Type','Remarks','Remarks1','Remarks2','Additional Info']]
13/291: data.drop(['Timestamp','Machine Type','Remarks','Remarks1','Remarks2','Additional Info'],axis=1)
13/292: dataset = data.drop(['Timestamp','Machine Type','Remarks','Remarks1','Remarks2','Additional Info'],axis=1)
13/293:
dataset = data.drop(['Timestamp','Machine Type','Remarks','Remarks1','Remarks2','Additional Info'],axis=1)
dataset.shape
13/294: 574*3 #dataset[:]
13/295: 574*.8 #dataset[:]
13/296: 574*.75 #dataset[:]
13/297: dataset[423:431]
13/298: dataset[537:538]
13/299: dataset[538:539]
13/300: dataset[538:540]
13/301: dataset[538:539]
13/302: dataset[539:541]
13/303:
train = dataset[:539]
test = dataset[539:]
13/304: test.shape
13/305: train.shape
13/306:
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical
import random
13/307: data[data['Result'] == 'Excellent']
13/308: data.drop('Result',axis=1,inplace=True)
13/309:
result = []
for i in data['Average']:
    if i > 0 and i < 32.99:
        result.append('Mill on Stop')
    
    elif i > 33.00 and i < 60:
        result.append('Good Condition')
    
    else:
        result.append('Excellent')
13/310:
result = []
for i in data['Average']:
    if i > 0 and i < 32.99:
        result.append('Mill on Stop')
    
    elif i > 33.00 and i < 60.99:
        result.append('Good Condition')
    
    else:
        result.append('Excellent')
13/311: data['Result'] = result
13/312:
dataset = data.drop(['Timestamp','Machine Type','Remarks','Remarks1','Remarks2','Additional Info'],axis=1)
dataset.shape
13/313:
train = dataset[:539]
test = dataset[539:]
13/314: train.shape
13/315: test.shape
13/316: data[data['Result'] == 'Excellent']
13/317:
result = []
for i in data['Average']:
    if (i > 0 and i < 32.99) or i == 32.99:
        result.append('Mill on Stop')
    
    elif (i > 33.00 and i < 60.99) or i == 33.00:
        result.append('Good Condition')
    
    else:
        result.append('Excellent')
13/318: data.drop('Result',axis=1,inplace=True)
13/319: data['Result'] = result
13/320:
dataset = data.drop(['Timestamp','Machine Type','Remarks','Remarks1','Remarks2','Additional Info'],axis=1)
dataset.shape
13/321:
train = dataset[:539]
test = dataset[539:]
13/322: train.shape
13/323: test.shape
13/324: data[data['Result'] == 'Excellent']
13/325: data[538:539]
13/326: data[537:539]
13/327: data[537:538]
13/328: data[539]
13/329: 539-450 #data[539]
13/330: data[89:539].shape
13/331: dataset[89:539].shape
13/332: dataset[89:539][0]
13/333: dataset[89:539]
13/334: dataset[:89]
13/335: dataset[:89].shape
13/336: dataset[:89,4]
13/337: dataset[:89][4]
13/338: dataset[:89]
13/339: dataset[:89].drop('Result',axis=1)
13/340: dataset['Result'][:89]
13/341: dataset['Result'][89:539]
13/342: dataset['Result'][539:]
13/343:
val_x = dataset[:89].drop('Result',axis=1).astype(float)
val_y = dataset['Result'][:89]

x_train = dataset[89:539].drop('Result',axis=1).astype(float)
y_train = dataset['Result'][89:539]

x_test = dataset[539:].drop('Result',axis=1).astype(float)
y_test = dataset['Result'][539:]
13/344:
from sklearn.preprocessing import LabelEncoder
# encode class values as integers
encoder= LabelEncoder()
encoded_Y = encoder.fit_transform(y_train)
encoded_y_train = tf.keras.utils.to_categorical(encoded_Y)

encoded_Y = encoder.fit_transform(y_test)
encoded_y_test = tf.keras.utils.to_categorical(encoded_Y)

encoded_Y = encoder.fit_transform(val_y)
encoded_val = tf.keras.utils.to_categorical(encoded_Y)
13/345:
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
13/346:
model = Sequential()

model.add(Dense(10,input_shape=(4,),activation='tanh'))
model.add(Dense(8,activation='tanh'))
model.add(Dense(6,activation='tanh'))
model.add(Dense(3,activation='softmax'))
13/347: model.summary()
13/348: dataset.head()
13/349: model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
13/350: history = model.fit(x_train, encoded_y_train, epochs=130, verbose=1, validation_data=(val_x,encoded_val))
13/351: history = model.fit(x_train, encoded_y_train, epochs=50, verbose=1, validation_data=(val_x,encoded_val))
13/352: val_x,encoded_val
13/353: val_x
13/354: encoded_y_train
13/355: encoded_y_train.shape
13/356: encoded_y_train
13/357: history = model.fit(x_train, encoded_y_train, epochs=50, validation_data=(val_x,encoded_val))
13/358: encoded_y_train
13/359: history = model.fit(x_train, encoded_y_train, epochs=1, validation_data=(val_x,encoded_val))
13/360: history = model.fit(x_train, encoded_y_train, epochs=1, validation_data=(val_x,encoded_val))
13/361: encoded_y_train
13/362: history = model.fit(x_train, encoded_y_train, epochs=1, validation_data=(val_x,encoded_val))
13/363: val_x, val_y
13/364: val_x.shape, val_y.shape
13/365: val_x.shape, val_y.shape, x_train.shape, y_train.shape, x_test.shape, y_test.shape
13/366: encoded_val.shape
13/367: encoded_val[0]
13/368:
for i in encoded_val[1]:
    print(i)
13/369:
for i in encoded_val[0]:
    print(i)
13/370:
for i in encoded_val[3]:
    print(i)
13/371:
for i in encoded_val[0][0]:
    print(i)
13/372: encoded_val
13/373: encoded_val[0]
13/374: encoded_val[0].insert(0.,0)
13/375: encoded_val[0].insert(0,0.)
13/376: encoded_val[0].insert(0,0)
13/377: array(encoded_val[0]).insert(0,0)
13/378: list(encoded_val[0]).insert(0,0)
13/379: print(list(encoded_val[0]).insert(0,0))
13/380: list(encoded_val[0])#.insert(0,0))
13/381: f = list(encoded_val[0])#.insert(0,0))
13/382: f.insert(0,0)
13/383: f
13/384: f.insert(0,0.0)
13/385: f
13/386:
for i in encoded_val:
    print(i)
13/387:
f = list(encoded_val)
for i in f:
    f.insert(0,0.0)
f
# encoded_val = np.array(encoded_val)
13/388:
f = list(encoded_val)
# for i in range(encoded_val):
#     i.insert(0,0.0)
f
# encoded_val = np.array(encoded_val)
13/389:
f = list(encoded_val)
# for i in range(encoded_val):
#     i.insert(0,0.0)
f[0]
# encoded_val = np.array(encoded_val)
13/390:
f = list(encoded_val)
# for i in range(encoded_val):
#     i.insert(0,0.0)
f[0][0]
# encoded_val = np.array(encoded_val)
13/391:
f = list(encoded_val)
# for i in range(encoded_val):
#     i.insert(0,0.0)
f[0]
# encoded_val = np.array(encoded_val)
13/392:
f = list(encoded_val)
for i in range(encoded_val):
    f[i].insert(0,0.0)
# encoded_val = np.array(encoded_val)
13/393: len(encoded_val)
13/394: len(f)
13/395:
f = list(encoded_val)
for i in range(len(f)):
    f[i].insert(0,0.0)
# encoded_val = np.array(encoded_val)
13/396:
f = list(encoded_val)
for i in range(len(f)):
    f[i].insert(0,0.0)
# encoded_val = np.array(encoded_val)
13/397: type(f)
13/398: i
13/399:
f = list(encoded_val)
for i in range(len(f)):
    f[i].insert(0,0.0)
13/400:
f = list(encoded_val)
type(f)
for i in range(len(f)):
    f[i].insert(0,0.0)
13/401:
f = list(encoded_val)
type(f)
for i in range(len(f)):
    f[i].insert(0,0.0)
13/402:
f = list(encoded_val)
print(type(f))
for i in range(len(f)):
    f[i].insert(0,0.0)
13/403:
f = list(encoded_val)

for i in range(len(f)):
    f[i].insert(0,0.0)
13/404: type(i)
13/405: f[i]
13/406: f[i][0]
13/407:
result = []
for i in data['Average']:
    if (i > 0 and i < 32.99) or i == 32.99:
        result.append('Mill on Stop')
    
    elif (i > 33.00 and i < 150) or i == 33.00:
        result.append('Good Condition')
13/408: data.drop('Result',axis=1,inplace=True)
13/409: data['Result'] = result
13/410:
dataset = data.drop(['Timestamp','Machine Type','Remarks','Remarks1','Remarks2','Additional Info'],axis=1)
dataset.shape
13/411:
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical
import random
13/412:
val_x = dataset[:89].drop('Result',axis=1).astype(float)
val_y = dataset['Result'][:89]

x_train = dataset[89:539].drop('Result',axis=1).astype(float)
y_train = dataset['Result'][89:539]

x_test = dataset[539:].drop('Result',axis=1).astype(float)
y_test = dataset['Result'][539:]
13/413: val_x.shape, val_y.shape, x_train.shape, y_train.shape, x_test.shape, y_test.shape
13/414:
from sklearn.preprocessing import LabelEncoder
# encode class values as integers
encoder= LabelEncoder()
encoded_Y = encoder.fit_transform(y_train)
encoded_y_train = tf.keras.utils.to_categorical(encoded_Y)

encoded_Y = encoder.fit_transform(y_test)
encoded_y_test = tf.keras.utils.to_categorical(encoded_Y)

encoded_Y = encoder.fit_transform(val_y)
encoded_val = tf.keras.utils.to_categorical(encoded_Y)
13/415:
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
13/416:
model = Sequential()

model.add(Dense(10,input_shape=(4,),activation='tanh'))
model.add(Dense(8,activation='tanh'))
model.add(Dense(6,activation='tanh'))
model.add(Dense(3,activation='softmax'))
13/417: model.summary()
13/418: model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
13/419: history = model.fit(x_train, encoded_y_train, epochs=1, validation_data=(val_x,encoded_val))
13/420: val_x
13/421: val_x.shape
13/422: val_y.shape
13/423: encoded_val.shape
13/424: encoded_train.shape
13/425: encoded_train_y.shape
13/426: encoded_y_train.shape
13/427: val_x.shape,encoded_val.shape
13/428: history = model.fit(x_train, encoded_y_train, epochs=50, validation_data=encoded_val)
22/1:
import pandas as pd
import numpy as np
data = pd.read_csv('data.csv')
data.head(11)
22/2: average = round(((data['Inlet Pressure (Bars)']*1.75)+(data['Inlet Temperature (Degree Celsius)']*0.375)+(data['Outlet Temperature (Degree Celsius)']*0.375))/3,2)
22/3: data['Average'] = average
22/4:
result = []
for i in data['Average']:
    if (i > 0 and i < 32.99) or i == 32.99:
        result.append(0) # 'Mill on Stop'
    
    elif (i > 33.00 and i < 150) or i == 33.00:
        result.append(1) # 'Good Condition'
22/5: data['Result'] = result
22/6:
dataset = data.drop(['Timestamp','Machine Type','Remarks','Remarks1','Remarks2','Additional Info'],axis=1)
dataset.shape
22/7:
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical
import random
22/8:
val_x = dataset[:89].drop('Result',axis=1).astype(float)
val_y = dataset['Result'][:89]

x_train = dataset[89:539].drop('Result',axis=1).astype(float)
y_train = dataset['Result'][89:539]

x_test = dataset[539:].drop('Result',axis=1).astype(float)
y_test = dataset['Result'][539:]
22/9: history = model.fit(x_train, encoded_y_train, epochs=50, validation_data=val_x)
23/1:
import pandas as pd
import numpy as np
data = pd.read_csv('data.csv')
data.head(11)
24/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
24/2:
data = pd.read_csv('data.csv')
data.head()
24/3: data[data[0]]
24/4: data[0,axis=1]
24/5: data[0]
24/6: data['Timestamp','Inlet Pressure (Bars)']
24/7: data[['Timestamp','Inlet Pressure (Bars)']]
24/8: data = data[['Timestamp','Inlet Pressure (Bars)'],index_cal=[0]]
24/9:
data = data[['Timestamp','Inlet Pressure (Bars)']]
data = data(index_col=[0])
24/10:
data = data[['Timestamp','Inlet Pressure (Bars)']]
data.head()
24/11: data.set_index(0)
24/12: data.set_index('Timestamp')
24/13: data.tail()
24/14: data.set_index('Timestamp',inplace=True)
24/15: data.describe()
24/16: data.plot()
24/17: data.plot(bar)
24/18: bar.plot(data)
24/19: data.plot()
24/20: from statsmodels.graphics.tsaplots import plot_act,plot_pacf
24/21: from statsmodels.graphics.tsaplots import plot_acf,plot_pacf
24/22: plot_acf(data)
24/23: plot_apcf(data)
24/24: plot_pacf(data)
24/25: 330/365
24/26: data.size
24/27: data.size*.9
24/28: data.size
24/29: 574-516
24/30: data[574:575]
24/31: data[573:574]
24/32: data.tail(1)
24/33: data[515:516]
24/34: data[:516].size
24/35: data[515:516]
24/36: data[516:].size
24/37: data[:516].size
24/38:
train_data = data[:516]
test_data = data[516:]
24/39: from statsmodels.tsa.arima_model import ARIMA
24/40: model = ARIMA(train_data, order=(2,1,2))
24/41: model = ARIMA(train_data, order=(2,1,3))
24/42: model = ARIMA(train_data, order=(1,1,1))
24/43: model = ARIMA(train_data, order=(2,1,2))
24/44: model = ARIMA(train_data, order=(1,0,1))
24/45: model = ARIMA(train_data, order=(2,0,2))
24/46: model = ARIMA(train_data, order=(1,2,3))
24/47: from statsmodels.tsa.arima.model import ARIMA
24/48: model = ARIMA(train_data, order=(2,1,2))
24/49: model = ARIMA(train_data, order=(1,0,2))
24/50: model_fit = model.fit()
24/51: model_fit.aic
24/52: model = ARIMA(train_data, order=(1,0,1))
24/53: model_fit = model.fit()
24/54: model_fit.aic
24/55: model = ARIMA(train_data, order=(2,0,2))
24/56: model_fit = model.fit()
24/57: model_fit.aic
24/58: model = ARIMA(train_data, order=(1,0,2))
24/59: model_fit = model.fit()
24/60: model_fit.aic
24/61: forecast = model_fit.forecast(steps = 35)[0]
24/62: forecast = model_fit.forecast(steps = 35)
24/63: forecast[0]
24/64: forecast
24/65: type(forecast)
24/66: forecast
24/67: forecast.size
24/68: forecast
24/69: np.sqrt(mean_squared_error())
24/70: from sklearn.metrics import mean_squared_error
24/71: np.sqrt(mean_squared_error(test_data,forecast))
24/72: 58+35
24/73: forecast = model_fit.forecast(steps = 58)
24/74: forecast
24/75: from sklearn.metrics import mean_squared_error
24/76: np.sqrt(mean_squared_error(test_data,forecast))
24/77: model = ARIMA(train_data, order=(0,0,1))
24/78: model_fit = model.fit()
24/79: model_fit.aic
24/80: model = ARIMA(train_data, order=(2,0,1))
24/81: model_fit = model.fit()
24/82: model_fit.aic
24/83: forecast = model_fit.forecast(steps = 58)
24/84: forecast
24/85: from sklearn.metrics import mean_squared_error
24/86: np.sqrt(mean_squared_error(test_data,forecast))
24/87: model = ARIMA(train_data, order=(1,1,1))
24/88: model_fit = model.fit()
24/89: model_fit.aic
24/90: forecast = model_fit.forecast(steps = 58)
24/91: forecast
24/92: from sklearn.metrics import mean_squared_error
24/93: np.sqrt(mean_squared_error(test_data,forecast))
24/94: model = ARIMA(train_data, order=(2,2,1))
24/95: model_fit = model.fit()
24/96: model_fit.aic
24/97: model = ARIMA(train_data, order=(0,0,0))
24/98: model_fit = model.fit()
24/99: model_fit.aic
24/100: model = ARIMA(train_data, order=(4,0,5))
24/101: model_fit = model.fit()
24/102: model_fit.aic
24/103: model = ARIMA(train_data, order=(5,0,3))
24/104: model_fit = model.fit()
24/105: model_fit.aic
24/106: forecast = model_fit.forecast(steps = 58)
24/107: forecast
24/108: from sklearn.metrics import mean_squared_error
24/109: np.sqrt(mean_squared_error(test_data,forecast))
24/110: model = ARIMA(train_data, order=(9,0,4))
24/111: model_fit = model.fit()
24/112: model_fit.aic
24/113: model = ARIMA(train_data, order=(6,0,3))
24/114: model_fit = model.fit()
24/115: model_fit.aic
24/116: model = ARIMA(train_data, order=(5,0,3))
24/117: model_fit = model.fit()
24/118: model_fit.aic
24/119: model = ARIMA(train_data, order=(5,0,2))
24/120: model_fit = model.fit()
24/121: model_fit.aic
24/122: model = ARIMA(train_data, order=(5,0,4))
24/123: model_fit = model.fit()
24/124: model_fit.aic
24/125: model = ARIMA(train_data, order=(5,0,3))
24/126: model_fit = model.fit()
24/127: model_fit.aic
24/128: forecast = model_fit.forecast(steps = 58)
24/129: forecast
24/130: from sklearn.metrics import mean_squared_error
24/131: np.sqrt(mean_squared_error(test_data,forecast))
24/132: model = ARIMA(train_data, order=(3,0,1))
24/133: model_fit = model.fit()
24/134: model_fit.aic
24/135: model = ARIMA(train_data, order=(6,0,4))
24/136: model_fit = model.fit()
24/137: model_fit.aic
24/138: model = ARIMA(train_data, order=(5,1,3))
24/139: model_fit = model.fit()
24/140: model_fit.aic
24/141: forecast = model_fit.forecast(steps = 58)
24/142: forecast
24/143: from sklearn.metrics import mean_squared_error
24/144: np.sqrt(mean_squared_error(test_data,forecast))
24/145: model = ARIMA(train_data, order=(5,2,3))
24/146: model_fit = model.fit()
24/147: model_fit.aic
24/148: model = ARIMA(train_data, order=(10,2,6))
24/149: model_fit = model.fit()
24/150: model_fit.aic
24/151: model = ARIMA(train_data, order=(5,1,3))
24/152: model_fit = model.fit()
24/153: model_fit.aic
24/154: forecast = model_fit.forecast(steps = 58)
24/155: forecast
24/156: from sklearn.metrics import mean_squared_error
24/157: np.sqrt(mean_squared_error(test_data,forecast))
24/158:
import pandas as pd
import numpy as np
import atplotlib.pyplot as plt
24/159:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
24/160:
data =pd.read_csv('data.csv',index_col='Timestamp',parse_dates=True)

data.head()
24/161:
data = data[['Inlet Pressure (Bars)']]
data.head()
24/162: data['Timestamp'].plot(figsize=(12,5))
24/163: data['Inlet Pressure (Bars)'].plot(figsize=(12,5))
24/164: data['Inlet Pressure (Bars)'].plot(figsize=(16,5))
24/165: data['Inlet Pressure (Bars)'].plot(figsize=(14,5))
24/166:
from statsmodels.tsa.stattools import adfuller

def adf_test(dataset):
  dftest = adfuller(dataset, autolag = 'AIC')
  print("1. ADF : ",dftest[0])
  print("2. P-Value : ", dftest[1])
  print("3. Num Of Lags : ", dftest[2])
  print("4. Num Of Observations Used For ADF Regression and Critical Values Calculation :", dftest[3])
  print("5. Critical Values :")
  for key, val in dftest[4].items():
      print("\t",key, ": ", val)
24/167:
from statsmodels.tsa.stattools import adfuller

def adf_test(dataset):
    dftest = adfuller(dataset, autolag = 'AIC')
    print("1. ADF : ",dftest[0])
    print("2. P-Value : ", dftest[1])
    print("3. Num Of Lags : ", dftest[2])
    print("4. Num Of Observations Used For ADF Regression and Critical Values Calculation :", dftest[3])
    print("5. Critical Values :")
    for key, val in dftest[4].items():
        print("\t",key, ": ", val)
24/168: adf_test(data['Inlet Pressure (Bars)'])
24/169:
from pmdarima import auto_arima
# Ignore harmless warnings
import warnings
warnings.filterwarnings("ignore")
24/170:
from pmdarima import auto_arima
# Ignore harmless warnings
import warnings
warnings.filterwarnings("ignore")
24/171:
stepwise_fit = auto_arima(data['Inlet Pressure (Bars)'], 
                          suppress_warnings=True)           

stepwise_fit.summary()
24/172: from statsmodels.tsa.arima.model import ARIMA
24/173:
stepwise_fit = auto_arima(data['Inlet Pressure (Bars)'], trace=True,
                          suppress_warnings=True)        

stepwise_fit.summary()
24/174:
print(data.shape)
train = data.[:516]
test = df.iloc[516:]
print(train.shape,test.shape)
print(test.iloc[0],test.iloc[-1])
24/175:
print(data.shape)
train = data.iloc[:516]
test = df.iloc[516:]
print(train.shape,test.shape)
print(test.iloc[0],test.iloc[-1])
24/176:
print(data.shape)
train = data.iloc[:516]
test = data.iloc[516:]
print(train.shape,test.shape)
print(test.iloc[0],test.iloc[-1])
24/177:
from statsmodels.tsa.arima_model import ARIMA
model=ARIMA(data['Inlet Pressure (Bars)'],order=(5,1,3))
model=model.fit()
model.summary()
24/178:
from statsmodels.tsa.arima.model import ARIMA
model=ARIMA(data['Inlet Pressure (Bars)'],order=(5,1,3))
model=model.fit()
model.summary()
24/179:
start = len(train)
end = len(train)+len(test)-1
#if the predicted values dont have date values as index, you will have to uncomment the following two commented lines to plot a graph
#index_future_dates=pd.date_range(start='2018-12-01',end='2018-12-30')
pred = model.predict(start=start,end=end,typ='levels').rename('ARIMA predictions')
#pred.index=index_future_dates
pred.plot(legend=True)
test['AvgTemp'].plot(legend=True)
24/180:
start = len(train)
end = len(train)+len(test)-1
#if the predicted values dont have date values as index, you will have to uncomment the following two commented lines to plot a graph
#index_future_dates=pd.date_range(start='2018-12-01',end='2018-12-30')
pred = model.predict(start=start,end=end,typ='levels').rename('ARIMA predictions')
#pred.index=index_future_dates
pred.plot(legend=True)
test['Inlet Pressure (Bars)'].plot(legend=True)
24/181:
start = len(train)
end = len(train)+len(test)-1
#if the predicted values dont have date values as index, you will have to uncomment the following two commented lines to plot a graph
#index_future_dates=pd.date_range(start='2018-12-01',end='2018-12-30')
pred = model.predict(start=start,end=end,typ='levels').rename('ARIMA predictions')
#pred.index=index_future_dates
pred.plot(legend=True)
test['Inlet Pressure (Bars)'].plot(figsize=(14,5), legend=True)
24/182: print(pred)
24/183: data.tail(1)
24/184: data.tail(2)
24/185: data.tail(5)
24/186: pred.tail(5)
24/187: pd.DataFrame(pred.tail(5))
24/188:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
24/189:
data =pd.read_csv('data.csv',index_col='Timestamp',parse_dates=True)

data.head()
24/190:
data = data[['Inlet Pressure (Bars)','Inlet Temperature (Degree Celsius)','Outlet Temperature (Degree Celsius)']]
data.head()
24/191: data['Inlet Pressure (Bars)','Inlet Temperature (Degree Celsius)','Outlet Temperature (Degree Celsius)'].plot(figsize=(14,5))
24/192: data['Outlet Temperature (Degree Celsius)'].plot(figsize=(14,5))
24/193: data['Inlet Temperature (Degree Celsius)'].plot(figsize=(14,5))
24/194:
from statsmodels.tsa.stattools import adfuller # Augmented dikki for test

def adf_test(dataset):
    dftest = adfuller(dataset, autolag = 'AIC')
    print("1. ADF : ",dftest[0])
    print("2. P-Value : ", dftest[1])
    print("3. Num Of Lags : ", dftest[2])
    print("4. Num Of Observations Used For ADF Regression and Critical Values Calculation :", dftest[3])
    print("5. Critical Values :")
    for key, val in dftest[4].items():
        print("\t",key, ": ", val)
24/195: adf_test(data['Inlet Pressure (Bars)','Inlet Pressure (Bars)','Inlet Temperature (Degree Celsius)',])
24/196:
stepwise_fit = auto_arima(data['Inlet Pressure (Bars)','Inlet Pressure (Bars)','Inlet Temperature (Degree Celsius)'], trace=True,
                          suppress_warnings=True)        

stepwise_fit.summary()
24/197:
stepwise_fit = auto_arima(data['Inlet Pressure (Bars)'], trace=True,
                          suppress_warnings=True)        

stepwise_fit.summary()
24/198: adf_test(data['Inlet Pressure (Bars)'])
24/199: data['Inlet Pressure (Bars)'].plot(figsize=(14,5))
24/200:
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.statespace.varmax import VARMAX
from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import grangercausalitytests, adfuller
from tqdm import tqdm_notebook
from itertools import product

import matplotlib.pyplot as plt
import statsmodels.api as sm
import pandas as pd
import numpy as np

import warnings
warnings.filterwarnings('ignore')
24/201:
data = data[['Inlet Pressure (Bars)']]
data.head()
24/202:
data =pd.read_csv('data.csv',index_col='Timestamp',parse_dates=True)

data = data[['Inlet Pressure (Bars)','Inlet Temperature (Degree Celsius)','Outlet Temperature (Degree Celsius)']]
data.head()
24/203:
fig, axes = plt.subplots(nrows=2, ncols=2, dpi=120, figsize=(10,6))
for i, ax in enumerate(axes.flatten()):
    data_col = data[data.columns[i]]
    ax.plot(data_col, color='red', linewidth=1)
    # Decorations
    ax.set_title(data.columns[i])
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines["top"].set_alpha(0)
    ax.tick_params(labelsize=6)

plt.tight_layout()
24/204:
fig, axes = plt.subplots(nrows=2, ncols=2, dpi=120, figsize=(10,6))
for i, ax in enumerate(axes.flatten()):
    data_col = data[data.columns[i]]
    ax.plot(data_col, color='green', linewidth=1)
    # Decorations
    ax.set_title(data.columns[i])
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines["top"].set_alpha(0)
    ax.tick_params(labelsize=6)

plt.tight_layout()
24/205:
fig, axes = plt.subplots(nrows=3, ncols=1, dpi=120, figsize=(10,6))
for i, ax in enumerate(axes.flatten()):
    data_col = data[data.columns[i]]
    ax.plot(data_col, color='green', linewidth=1)
    # Decorations
    ax.set_title(data.columns[i])
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines["top"].set_alpha(0)
    ax.tick_params(labelsize=6)

plt.tight_layout()
24/206:
fig, axes = plt.subplots(nrows=3, ncols=1, dpi=120, figsize=(12,8))
for i, ax in enumerate(axes.flatten()):
    data_col = data[data.columns[i]]
    ax.plot(data_col, color='green', linewidth=1)
    # Decorations
    ax.set_title(data.columns[i])
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines["top"].set_alpha(0)
    ax.tick_params(labelsize=6)

plt.tight_layout()
24/207:
fig, axes = plt.subplots(nrows=3, ncols=1, dpi=120, figsize=(12,8))
for i, ax in enumerate(axes.flatten()):
    data_col = data[data.columns[i]]
    ax.plot(data_col, color='green', linewidth=1)
    # Decorations
    ax.set_title(data.columns[i])
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines["top"].set_alpha(0)
    ax.tick_params(labelsize=7)

plt.tight_layout()
24/208:
fig, axes = plt.subplots(nrows=3, ncols=1, dpi=120, figsize=(12,8))
for i, ax in enumerate(axes.flatten()):
    data_col = data[data.columns[i]]
    ax.plot(data_col, color='cyan', linewidth=1)
    # Decorations
    ax.set_title(data.columns[i])
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines["top"].set_alpha(0)
    ax.tick_params(labelsize=7)

plt.tight_layout()
24/209:
fig, axes = plt.subplots(nrows=3, ncols=1, dpi=120, figsize=(12,8))
for i, ax in enumerate(axes.flatten()):
    data_col = data[data.columns[i]]
    ax.plot(data_col, color='magenta', linewidth=1)
    # Decorations
    ax.set_title(data.columns[i])
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines["top"].set_alpha(0)
    ax.tick_params(labelsize=7)

plt.tight_layout()
24/210:
fig, axes = plt.subplots(nrows=3, ncols=1, dpi=120, figsize=(12,8))
for i, ax in enumerate(axes.flatten()):
    data_col = data[data.columns[i]]
    ax.plot(data_col, color='magenta', linewidth=2)
    # Decorations
    ax.set_title(data.columns[i])
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines["top"].set_alpha(0)
    ax.tick_params(labelsize=7)

plt.tight_layout()
24/211:
fig, axes = plt.subplots(nrows=3, ncols=1, dpi=120, figsize=(12,8))
for i, ax in enumerate(axes.flatten()):
    data_col = data[data.columns[i]]
    ax.plot(data_col, color='magenta', linewidth=1)
    # Decorations
    ax.set_title(data.columns[i-1])
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines["top"].set_alpha(0)
    ax.tick_params(labelsize=7)

plt.tight_layout()
24/212:
ad_fuller_result_1 = adfuller(data['Inlet Pressure (Bars)'].diff()[1:])

print('realgdp')
print(f'ADF Statistic: {ad_fuller_result_1[0]}')
print(f'p-value: {ad_fuller_result_1[1]}')

print('\n---------------------\n')

ad_fuller_result_2 = adfuller(data['Inlet Temperature (Degree Celsius)'].diff()[1:])

print('realcons')
print(f'ADF Statistic: {ad_fuller_result_2[0]}')
print(f'p-value: {ad_fuller_result_2[1]}')
24/213:
ad_fuller_result_1 = adfuller(data['Intlet Temperature (Degree Celsius)'].diff()[1:])

print('realgdp')
print(f'ADF Statistic: {ad_fuller_result_1[0]}')
print(f'p-value: {ad_fuller_result_1[1]}')

print('\n---------------------\n')

ad_fuller_result_2 = adfuller(data['Outlet Temperature (Degree Celsius)'].diff()[1:])

print('realcons')
print(f'ADF Statistic: {ad_fuller_result_2[0]}')
print(f'p-value: {ad_fuller_result_2[1]}')
24/214:
ad_fuller_result_1 = adfuller(data['Inlet Temperature (Degree Celsius)'].diff()[1:])

print('realgdp')
print(f'ADF Statistic: {ad_fuller_result_1[0]}')
print(f'p-value: {ad_fuller_result_1[1]}')

print('\n---------------------\n')

ad_fuller_result_2 = adfuller(data['Outlet Temperature (Degree Celsius)'].diff()[1:])

print('realcons')
print(f'ADF Statistic: {ad_fuller_result_2[0]}')
print(f'p-value: {ad_fuller_result_2[1]}')
24/215:
ad_fuller_result_1 = adfuller(data['Inlet Pressure (Bars)'].diff()[1:])

print('Inlet Pressure (Bars)')
print(f'ADF Statistic: {ad_fuller_result_1[0]}')
print(f'p-value: {ad_fuller_result_1[1]}')

print('\n---------------------\n')

ad_fuller_result_2 = adfuller(data['Inlet Temperature (Degree Celsius)'].diff()[1:])

print('Inlet Temperature (Degree Celsius)')
print(f'ADF Statistic: {ad_fuller_result_2[0]}')
print(f'p-value: {ad_fuller_result_2[1]}')

print('\n---------------------\n')

ad_fuller_result_3 = adfuller(data['Outlet Temperature (Degree Celsius)'].diff()[1:])

print('Inlet Temperature (Degree Celsius)')
print(f'ADF Statistic: {ad_fuller_result_3[0]}')
print(f'p-value: {ad_fuller_result_3[1]}')
24/216:
ad_fuller_result_1 = adfuller(data['Inlet Pressure (Bars)'].diff()[1:])

print('Inlet Pressure (Bars)')
print(f'ADF Statistic: {ad_fuller_result_1[0]}')
print(f'p-value: {ad_fuller_result_1[1]}')

print('\n---------------------\n')

ad_fuller_result_2 = adfuller(data['Inlet Temperature (Degree Celsius)'].diff()[1:])

print('Inlet Temperature (Degree Celsius)')
print(f'ADF Statistic: {ad_fuller_result_2[0]}')
print(f'p-value: {ad_fuller_result_2[1]}')

print('\n---------------------\n')

ad_fuller_result_3 = adfuller(data['Outlet Temperature (Degree Celsius)'].diff()[1:])

print('Outlet Temperature (Degree Celsius)')
print(f'ADF Statistic: {ad_fuller_result_3[0]}')
print(f'p-value: {ad_fuller_result_3[1]}')
24/217:
print('rgnp causes ulc?\n')
print('------------------')
granger_1 = grangercausalitytests(data[['Inlet Pressure (Bars)','Inlet Temperature (Degree Celsius)','Outlet Temperature (Degree Celsius)']], 4)

# print('\nulc causes rgnp?\n')
# print('------------------')
# granger_2 = grangercausalitytests(macro_data[['rgnp', 'ulc']], 4)
24/218:
print('rgnp causes ulc?\n')
print('------------------')
granger_1 = grangercausalitytests(data[['Inlet Pressure (Bars)','Inlet Temperature (Degree Celsius)','Outlet Temperature (Degree Celsius)']], 1)

# print('\nulc causes rgnp?\n')
# print('------------------')
# granger_2 = grangercausalitytests(macro_data[['rgnp', 'ulc']], 4)
24/219:
print('rgnp causes ulc?\n')
print('------------------')
granger_1 = grangercausalitytests(data[['Inlet Pressure (Bars)','Inlet Temperature (Degree Celsius)','Outlet Temperature (Degree Celsius)']], 4)

# print('\nulc causes rgnp?\n')
# print('------------------')
# granger_2 = grangercausalitytests(macro_data[['rgnp', 'ulc']], 4)
24/220:
print('rgnp causes ulc?\n')
print('------------------')
granger_1 = grangercausalitytests(data[['Inlet Pressure (Bars)','Inlet Temperature (Degree Celsius)']], 4)

# print('\nulc causes rgnp?\n')
# print('------------------')
# granger_2 = grangercausalitytests(macro_data[['rgnp', 'ulc']], 4)
24/221:
print('rgnp causes ulc?\n')
print('------------------')
granger_1 = grangercausalitytests(data[['Inlet Pressure (Bars)','Inlet Temperature (Degree Celsius)']], 4)

print('\nulc causes rgnp?\n')
print('------------------')
granger_2 = grangercausalitytests(macro_data[['Inlet Temperature (Degree Celsius)','Inlet Pressure (Bars)']], 4)
24/222:
print('rgnp causes ulc?\n')
print('------------------')
granger_1 = grangercausalitytests(data[['Inlet Pressure (Bars)','Inlet Temperature (Degree Celsius)']], 4)

print('\nulc causes rgnp?\n')
print('------------------')
granger_2 = grangercausalitytests(data[['Inlet Temperature (Degree Celsius)','Inlet Pressure (Bars)']], 4)
24/223:

granger_1 = grangercausalitytests(data[['Inlet Pressure (Bars)','Inlet Temperature (Degree Celsius)']], 4)
granger_2 = grangercausalitytests(data[['Inlet Temperature (Degree Celsius)','Inlet Pressure (Bars)']], 4)
24/224:
train_data = data[:516]
test_data = data[516:]
24/225: model = VAR(train_data.diff()[1:])
24/226:
sorted_order=model.select_order(maxlags=20)
print(sorted_order.summary())
24/227: print(-*9)
24/228: print('-'*9)
24/229: print('-'*27)
24/230: sorted_order.AIC
24/231: sorted_order
24/232: sorted_order().AIC
24/233: sorted_order()[0]
24/234:
var_model = VARMAX(train_data, order=(4,0),enforce_stationarity= True)
fitted_model = var_model.fit(disp=False)
print(fitted_model.summary())
24/235: len(train_data)
24/236: len(data)
24/237: data[574:575]
24/238: data[573:574]
24/239:
n_forecast = 12
predict = fitted_model.get_prediction(start=len(train_data),end=len(data))

predictions=predict.predicted_mean
24/240:

predictions
24/241:

predictions.tail()
24/242:
data.tail()
predictions.tail()
24/243: data.tail()
24/244:
abc = np.array([1,2,3,4,5,65,1,77,88,99,99,45])
abc = np.where(abc == 1, 33333, abc)
print(abc)
25/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
25/2:
import warnings; 
warnings.simplefilter('ignore')
25/3: !pip install fbprophet
25/4: import fbprophet
25/5: from fbprophet import Prophet
28/1:
import numpy as np
import pandas as pd
28/2: data = pd.read_csv('data.csv')
28/3: perc_data = pd.read_csv('percentange.csv')
28/4: perc_data = pd.read_csv('percentage.csv')
28/5: perc_data[5]
28/6: perc_data(5)
28/7: perc_data.head(3)
28/8: perc_data[['Machine Type','Inlet Pressure (Bars)']]
28/9: perc_data.drop[['Machine Type','Inlet Pressure (Bars)']]
28/10: perc_data.drop([['Machine Type','Inlet Pressure (Bars)']])
28/11: perc_data.drop(['Machine Type','Inlet Pressure (Bars)'])
28/12: perc_data.drop(perc_data['Machine Type','Inlet Pressure (Bars)'],'Axis' = 1)
28/13: perc_data.drop(['Machine Type','Inlet Pressure (Bars)'],'Axis' = 1)
28/14: perc_data.drop(['Machine Type','Inlet Pressure (Bars)'],Axis = 1)
28/15: perc_data.drop(['Machine Type','Inlet Pressure (Bars)'], axis= 1)
28/16: perc_data.head(1)
28/17: perc_data.drop(['Machine Type','Unnamed: 6','Unnamed: 7'], axis= 1,inplace=True)
28/18: perc_data.head(#)
28/19: perc_data.head(3)
28/20: perc_data['Inlet Pressure (Bars)']
28/21:
for i in perc_data['Inlet Pressure (Bars)']:
    print(i)
28/22: data['Machine Type'].shape
28/23: np.zeros(data['Machine Type'].shape)
28/24: empty_col = np.zeros(data['Machine Type'].shape)
28/25: data['throughput (target is 4000 a day)'] = empty_col
28/26: type(data['Inlet Pressure (Bars)'])
28/27:
for i in perc_data['Inlet Pressure (Bars)']:
    if i in data['Inlet Pressure (Bars)']:
        pritn(True)
28/28:
for i in perc_data['Inlet Pressure (Bars)']:
    if i in data['Inlet Pressure (Bars)']:
        print(True)
28/29:
for i in perc_data['Inlet Pressure (Bars)']:
    print(i)
    if i in data['Inlet Pressure (Bars)']:
        print(True)
28/30:
for i in perc_data['Inlet Pressure (Bars)']:
    print(i)
    for i in data['Inlet Pressure (Bars)']:
        print(True)
28/31:
a = []
for i in perc_data['Inlet Pressure (Bars)']:
    print(i)
    for i in data['Inlet Pressure (Bars)']:
        a.append(True)
len(a)
28/32: 11480/574
28/33:
a = []
for i in perc_data['Inlet Pressure (Bars)']:
    if i in data['Inlet Pressure (Bars)']:
        a.append(True)
len(a)
28/34:
a = []
for i in perc_data['Inlet Pressure (Bars)']:
    if i in data['Inlet Pressure (Bars)']:
        a.append(i)
len(a)
28/35:
a = []
for i in perc_data['Inlet Pressure (Bars)']:
    if i in data['Inlet Pressure (Bars)']:
        a.append(i)
len(a)
28/36:
a = []
for i in perc_data['Inlet Pressure (Bars)']:
    if i in data['Inlet Pressure (Bars)']:
        a.append(i)
len(a)
28/37:
a = []
for i in perc_data['Inlet Pressure (Bars)']:
    print(i)
    if i in data['Inlet Pressure (Bars)']:
        a.append(i)
len(a)
28/38:
a = []
for i in perc_data['Inlet Pressure (Bars)']:
#     print(i)
    if i in data['Inlet Pressure (Bars)']:
        print(i)
        a.append(i)
len(a)
28/39:
a = []
for i in perc_data['Inlet Pressure (Bars)']:
#     print(i)
    if i in data['Inlet Pressure (Bars)']:
        print(i)
        a.append(True)
len(a)
28/40: data = pd.read_csv('data2.csv')
28/41: data.head()
28/42: data['throughput (target is 4000 a day)'].interpolate()
28/43: data['throughput (target is 4000 a day)'].interpolate(inplace=True)
28/44: data.tail()
28/45: data.head(4:9)
28/46: data[4:9]
28/47: data[55:61]
28/48: data[45:51]
28/49: data[25:31]
28/50: data[15:21]
28/51: data[10:18]
28/52: data[20:38]
28/53: data[250:255]
28/54: data.tail()
28/55: data[400:405]
28/56: data[300:305]
28/57: data[350:355]
28/58: data[380:385]
28/59: data[370:375]
28/60: data[375:381]
28/61: data.head()
28/62: data[['Inlet Pressure (Bars)','Inlet Temperature (Degree Celsius)','Outlet Temperature (Degree Celsius)']]
28/63:
Interpolation = data[['Inlet Pressure (Bars)','Inlet Temperature (Degree Celsius)','Outlet Temperature (Degree Celsius)']].interpolate()
Interpolation
28/64:
data['rolled'] = zip(data['Inlet Pressure (Bars)'], data['Inlet Temperature (Degree Celsius)'], data['Outlet Temperature (Degree Celsius)'])
%timeit data['interped_temp'] = df['rolled'].map(lambda x: np.interp(x[0], probe_alts.values(), x[1]))
del df['rolled']
28/65:
data['rolled'] = zip(data['Inlet Pressure (Bars)'], zip(data['Inlet Temperature (Degree Celsius)'], data['Outlet Temperature (Degree Celsius)']))
%timeit df['interped_temp'] = df['rolled'].map(lambda x: np.interp(x[0], probe_alts.values(), x[1]))
del df['rolled']
28/66:
data['rolled'] = zip(data[['Inlet Pressure (Bars)']], zip(data[['Inlet Temperature (Degree Celsius)']], data[['Outlet Temperature (Degree Celsius)']]))
%timeit df['interped_temp'] = df['rolled'].map(lambda x: np.interp(x[0], probe_alts.values(), x[1]))
del df['rolled']
28/67:
from scipy import interpolate

f = interpolate.interp1d(data[['Inlet Pressure (Bars)']], data[['Inlet Temperature (Degree Celsius)']],kind=5, fill_value='extrapolate')
28/68:
from scipy import interpolate

f = interpolate.interp1d(data['Inlet Pressure (Bars)'], data['Inlet Temperature (Degree Celsius)'],kind=5, fill_value='extrapolate')
32/1:
import numpy as np
import pandas as pd
import neuralprophet as NeuralProphet
import pickle
32/2:
import numpy as np
import pandas as pd
import neuralprophet as NeuralProphet
import pickle
import matplotlib.pyplot as plt
32/3:
df = pd.read_csv('weather.csv')
df.head()
32/4: df.Location.unique()
32/5: df.columns
32/6: df.dtypes
32/7:
melb = df[df['Location'] == 'Melbourne']
melb.head()
32/8:
melb = df[df['Location'] == 'Melbourne']
pd.to_datetime(melb['Date'],inplace=True)
32/9:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
32/10: melb.dtype()
32/11: melb.dtype
32/12: melb.dtypes
32/13: plt.plot(melb['Date'], melb['melb3pm'])
32/14:
plt.plot(melb['Date'], melb['melb3pm'])
plt.show()
32/15:
plt.plot(melb['Date'], melb['Temp3pm'])
# plt.show()
34/1:
import numpy as np
import pandas as pd
import neuralprophet as NeuralProphet
import pickle
import matplotlib.pyplot as plt
34/2:
df = pd.read_csv('weather.csv')
df.head()
34/3: df.Location.unique()
34/4: df.columns
34/5: df.dtypes
34/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
34/7: melb.dtypes
34/8:
plt.plot(melb['Date'], melb['Temp3pm'])
plt.show()
35/1: melb['Date']
36/1:
import numpy as np
import pandas as pd
import neuralprophet as NeuralProphet
import pickle
import matplotlib.pyplot as plt
36/2:
df = pd.read_csv('weather.csv')
df.head()
36/3: df.Location.unique()
36/4: df.columns
36/5: df.dtypes
36/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
36/7: melb.dtypes
36/8:
# plt.plot(melb['Date'], melb['Temp3pm'])
# plt.show()
36/9: melb['Date']
36/10: melb['Temp3pm']
36/11: melb['Temp3pm'].size
36/12: melb['Date'].size
36/13: melb['Date'].shape()
36/14: melb['Date'].shape
36/15: melb['Temp3pm'].shape
36/16:
plt.plot(melb['Date'], melb['Temp3pm'])
plt.show()
37/1: import matplotlib.pyplot as plt
38/1:
import numpy as np
import pandas as pd
import neuralprophet as NeuralProphet
import pickle
import matplotlib.pyplot as plt
38/2:
df = pd.read_csv('weather.csv')
df.head()
38/3: df.Location.unique()
38/4: df.columns
38/5: df.dtypes
38/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
38/7: melb.dtypes
38/8:
plt.plot(melb['Date'], melb['Temp3pm'])
plt.show()
40/1:
import numpy as np
import pandas as pd
import neuralprophet as NeuralProphet
import pickle
import matplotlib.pyplot as plt
40/2:
df = pd.read_csv('weather.csv')
df.head()
40/3: df.Location.unique()
40/4: df.columns
40/5: df.dtypes
40/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
40/7: melb.dtypes
40/8:
plt.plot(list(melb['Date']), list(melb['Temp3pm']))
plt.show()
41/1:
plt.plot((melb['Date'], melb['Temp3pm'])
plt.show()
42/1:
import numpy as np
import pandas as pd
import neuralprophet as NeuralProphet
import pickle
import matplotlib.pyplot as plt
42/2:
df = pd.read_csv('weather.csv')
df.head()
42/3: df.Location.unique()
42/4: df.columns
42/5: df.dtypes
42/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
42/7: melb.dtypes
42/8:
plt.plot((melb['Date'], melb['Temp3pm'])
plt.show()
42/9:
plt.plot(melb['Date'], melb['Temp3pm'])
plt.show()
44/1:
import numpy as np
import pandas as pd
import neuralprophet as NeuralProphet
import pickle
import matplotlib.pyplot as plt
44/2:
df = pd.read_csv('weather.csv')
df.head()
44/3: df.Location.unique()
44/4: df.columns
44/5: df.dtypes
44/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
44/7: melb.dtypes
44/8:
plt.plot(melb['Date'], melb['Temp3pm'])
plt.show()
45/1:
import matplotlib.pyplot as plt
plt.(melb['Date'], melb['Temp3pm'])
plt.show()
46/1:
import numpy as np
import pandas as pd
import neuralprophet as NeuralProphet
import pickle
import matplotlib.pyplot as plt
46/2:
df = pd.read_csv('weather.csv')
df.head()
46/3: df.Location.unique()
46/4: df.columns
46/5: df.dtypes
46/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
46/7: melb.dtypes
46/8:
import matplotlib.pyplot as plt
plt.(melb['Date'], melb['Temp3pm'])
plt.show()
47/1:
import numpy as np
import pandas as pd
import neuralprophet as NeuralProphet
import pickle
import matplotlib.pyplot as plt
47/2:
df = pd.read_csv('weather.csv')
df.head()
47/3: df.Location.unique()
47/4: df.columns
47/5: df.dtypes
47/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
47/7: melb.dtypes
47/8:
import matplotlib.pyplot as plt
plt.plot(melb['Date'], melb['Temp3pm'])
plt.show()
49/1:
import numpy as np
import pandas as pd
import neuralprophet as NeuralProphet
import pickle
import matplotlib.pyplot as plt
49/2:
df = pd.read_csv('weather.csv')
df.head()
49/3: df.Location.unique()
49/4: df.columns
49/5: df.dtypes
49/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
49/7: melb.dtypes
49/8:
import matplotlib.pyplot as plt
plt.plot(melb['Date'], melb['Temp3pm'])
plt.show()
51/1:
import numpy as np
import pandas as pd
import neuralprophet as NeuralProphet
import pickle
import matplotlib.pyplot as plt
51/2:
df = pd.read_csv('weather.csv')
df.head()
51/3: df.Location.unique()
51/4: df.columns
51/5: df.dtypes
51/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
51/7: melb.dtypes
51/8:
# import matplotlib.pyplot as plt
# plt.plot(melb['Date'], melb['Temp3pm'])
# plt.show()
51/9:
# melb['Year'] = melb['Date'].apply(lambda x: x.year)
# melb = melb[melb['Year']<=2015]
# plt.plot(melb['Date'], melb['Temp3pm'])
# plt.show()
51/10: type(melb['Date'])
51/11: np.array(melb['Date'])
51/12: np.array(melb['Date']).size
51/13:
import matplotlib.pyplot as plt
plt.plot(np.array(melb['Date']), np.array(melb['Temp3pm']))
plt.show()
53/1:
import numpy as np
import pandas as pd
import neuralprophet as NeuralProphet
import pickle
import matplotlib.pyplot as plt
53/2:
df = pd.read_csv('weather.csv')
df.head()
53/3: df.Location.unique()
53/4: df.columns
53/5: df.dtypes
53/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
53/7: melb.dtypes
53/8:
import matplotlib.pyplot as plt
import sys
plt.plot(np.array(melb['Date']), np.array(melb['Temp3pm']))
plt.show()

plt.savefig(sys.stdout.buffer)
sys.stdout.flush()
55/1:
import matplotlib.pyplot as plt
plt.plot([0,1], [0,1])
55/2: type([0,1])
55/3: type(melb['Date'])
55/4:
import numpy as np
import pandas as pd
import neuralprophet as NeuralProphet
import pickle
import matplotlib.pyplot as plt
55/5:
df = pd.read_csv('weather.csv')
df.head()
55/6: df.Location.unique()
55/7: df.columns
55/8: df.dtypes
55/9:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
55/10: melb.dtypes
55/11:
import matplotlib.pyplot as plt
import sys
plt.plot(np.array(melb['Date']), np.array(melb['Temp3pm']))
plt.show()

plt.savefig(sys.stdout.buffer)
sys.stdout.flush()
55/12:
import matplotlib.pyplot as plt
import sys
plt.plot(np.array(melb['Date']), np.array(melb['Temp3pm']))
plt.show()

# plt.savefig(sys.stdout.buffer)
# sys.stdout.flush()
55/13:
plt.plot(np.array(melb['Date']), np.array(melb['Temp3pm']))
plt.show()
55/14:
melb['Year'] = melb['Date'].apply(lambda x: x.year)
melb = melb[melb['Year']<=2015]
plt.plot(melb['Date'], melb['Temp3pm'])
plt.show()
55/15: type(melb['Date'])
55/16:
melb['Year'] = melb['Date'].apply(lambda x: x.year)
melb = melb[melb['Year']<=2015]
plt.plot(melb['Date'], melb['Temp3pm'])
plt.show()
55/17:
data = melb[['Date', 'Temp3pm']]
data.dropna(inplace=True)
data.columns = ['ds', 'y']
data.head()
55/18: data.head(@)
55/19: data.head(2)
55/20:
model = NeuralProphet()
model.fit(data, freq='D', epochs=1000)
55/21:
m = NeuralProphet()
m.fit(data, freq='D', epochs=1000)
55/22:
m = NeuralProphet()
# m.fit(data, freq='D', epochs=1000)
metrics = m.fit(data, freq="D")
55/23: import neuralprophets
55/24: import neuralprophet
55/25: a = neuralprophet(})
55/26: a = neuralprophet()
55/27:
m = NeuralProphet(
    n_lags=12,
    changepoints_range=0.95,
    n_changepoints=30,
    weekly_seasonality=False,
)
metrics = m.fit(data)
55/28: m = neuralprophet
55/29:
m = NeuralProphet
# m.fit(data, freq='D', epochs=1000)
metrics = m.fit(data, freq="D")
55/30: m = NeuralProphet().fit(data, freq="D")
55/31: m = NeuralProphet().fit(data, freq='D', epochs=1000)
55/32: m = NeuralProphet().fit(data, freq='D', epochs=1000)
55/33:
m = NeuralProphet().fit(data, freq='D', epochs=1000)
df_future = m.make_future_dataframe(data, periods=30)
55/34:
import prophet
m = prophet().fit(data, freq='D', epochs=1000)
55/35:
o = '2021-01-01 11:00:00'
o[:10]
55/36: import neuralprophet
55/37:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
import matplotlib.pyplot as plt
56/1:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
import matplotlib.pyplot as plt
56/2:
df = pd.read_csv('weather.csv')
df.head()
56/3: df.Location.unique()
56/4: df.columns
56/5: df.dtypes
56/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
56/7: melb.dtypes
56/8:
plt.plot(np.array(melb['Date']), np.array(melb['Temp3pm']))
plt.show()
58/1:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
import matplotlib.pyplot as plt
58/2:
df = pd.read_csv('weather.csv')
df.head()
58/3: df.Location.unique()
58/4: df.columns
58/5: df.dtypes
58/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
58/7: melb.dtypes
58/8:
plt.plot(np.array(melb['Date']), np.array(melb['Temp3pm']))
plt.show()
60/1:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
import matplotlib.pyplot as plt
60/2:
df = pd.read_csv('weather.csv')
df.head()
60/3: df.Location.unique()
60/4: df.columns
60/5: df.dtypes
60/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
60/7: melb.dtypes
60/8:
import time

time.sleep(3)

plt.plot(np.array(melb['Date']), np.array(melb['Temp3pm']))
plt.show()

time.sleep(5)
62/1:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
import matplotlib.pyplot as plt
62/2:
df = pd.read_csv('weather.csv')
df.head()
62/3: df.Location.unique()
62/4: df.columns
62/5: df.dtypes
62/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
62/7: melb.dtypes
62/8:
plt.plot(np.array(melb['Date']), np.array(melb['Temp3pm']))
plt.show()
63/1:
plt.plot(np.array(melb['Date']), np.array(melb['Temp3pm']))
plt.show()
63/2:
melb['Year'] = melb['Date'].apply(lambda x: x.year)
melb = melb[melb['Year']<=2015]
plt.plot(melb['Date'], melb['Temp3pm'])
plt.show()
64/1:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
import matplotlib.pyplot as plt
64/2:
df = pd.read_csv('weather.csv')
df.head()
64/3: df.Location.unique()
64/4: df.columns
64/5: df.dtypes
64/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
64/7: melb.dtypes
64/8: melb['Date'][:]
64/9: melb['Date'][:2]
64/10: np.array(list(melb['Date'][:2]))
64/11: np.array(list(melb['Date'][:1]))
64/12: type(np.array(list(melb['Date'][:1])))
64/13:
plt.plot(np.array(melb['Date'][:]), np.array(melb['Temp3pm'][:]))
plt.show()
66/1:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
import matplotlib.pyplot as plt
66/2:
df = pd.read_csv('weather.csv')
df.head()
66/3: df.Location.unique()
66/4: df.columns
66/5: df.dtypes
66/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
66/7: melb.dtypes
66/8:
plt.plot(np.array(melb['Date']), np.array(melb['Temp3pm']))
plt.show()
68/1:
import matplotlib.pyplot as plt
plt.plot(np.array(melb['Date']), np.array(melb['Temp3pm']))
plt.show()
68/2:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
import matplotlib.pyplot as plt
68/3:
df = pd.read_csv('weather.csv')
df.head()
68/4: df.Location.unique()
68/5: df.columns
68/6: df.dtypes
68/7:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
68/8: melb.dtypes
68/9:
import matplotlib.pyplot as plt
plt.plot(melb['Date'], melb['Temp3pm'])
plt.show()
70/1:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
import matplotlib.pyplot as plt
70/2:
df = pd.read_csv('weather.csv')
df.head()
70/3: df.Location.unique()
70/4: df.columns
70/5: df.dtypes
70/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
70/7: melb.dtypes
70/8:
import matplotlib.pyplot as plt
plt.plot(melb['Date'], melb['Temp3pm'])
plt.show()
72/1:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
import matplotlib.pyplot as plt
72/2:
df = pd.read_csv('weather.csv')
df.head()
72/3: df.Location.unique()
72/4: df.columns
72/5: df.dtypes
72/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
72/7: melb.dtypes
72/8:
import matplotlib.pyplot as plt
plt.plot(melb['Date'], melb['Temp3pm'])
plt.show()
74/1:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
import matplotlib.pyplot as plt
74/2:
df = pd.read_csv('weather.csv')
df.head()
74/3: df.Location.unique()
74/4: df.columns
74/5: df.dtypes
74/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
74/7: melb.dtypes
74/8:
import matplotlib.pyplot as plt
plt.plot(np.array(melb['Date']), np.array(melb['Temp3pm']))
plt.show()
77/1:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
import matplotlib.pyplot as plt
77/2:
df = pd.read_csv('weather.csv')
df.head()
77/3: df.Location.unique()
77/4: df.columns
77/5: df.dtypes
77/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
77/7: melb.dtypes
77/8:
# import matplotlib.pyplot as plt
# plt.plot(np.array(melb['Date']), np.array(melb['Temp3pm']))
# plt.show()
77/9:
melb['Year'] = melb['Date'].apply(lambda x: x.year)
melb = melb[melb['Year']<=2015]
plt.plot(melb['Date'], melb['Temp3pm'])
plt.show()
79/1:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
import matplotlib.pyplot as plt
79/2:
df = pd.read_csv('weather.csv')
df.head()
79/3: df.Location.unique()
79/4: df.columns
79/5: df.dtypes
79/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
79/7: melb.dtypes
79/8:
# import matplotlib.pyplot as plt
# plt.plot(np.array(melb['Date']), np.array(melb['Temp3pm']))
# plt.show()
79/9:
melb['Year'] = melb['Date'].apply(lambda x: x.year)
melb = melb[melb['Year']<=2015]
# plt.plot(melb['Date'], melb['Temp3pm'])
# plt.show()
79/10:
data = melb[['Date', 'Temp3pm']]
data.dropna(inplace=True)
data.columns = ['ds', 'y']
data.head()
79/11: data.head(2)
79/12:
model = NeuralProphet()
model.fit(data, freq='D')
79/13:
future = model.make_future_dataframe(data, periods=900)
forecast = model.predict(future)
forecast.head()
79/14: forecast.tail()
79/15: data.tail()
79/16: forecast.head()
79/17: plot1 = model.plot(forecast)
80/1:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
import plotly.plotly as py
80/2:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
import plotly.chart_studio.plotly as py
80/3:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
import chart_studio.plotly as py
80/4:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
from chart_studio import plotly as py
81/1:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
from chart_studio import plotly as py
81/2:
df = pd.read_csv('weather.csv')
df.head()
81/3: df.Location.unique()
81/4: df.columns
81/5: df.dtypes
81/6:
melb = df[df['Location'] == 'Melbourne']
melb['Date'] = pd.to_datetime(melb['Date'])
melb.head()
81/7: melb.dtypes
81/8:
# import matplotlib.pyplot as plt
# plt.plot(np.array(melb['Date']), np.array(melb['Temp3pm']))
# plt.show()
81/9:
melb['Year'] = melb['Date'].apply(lambda x: x.year)
melb = melb[melb['Year']<=2015]
# plt.plot(melb['Date'], melb['Temp3pm'])
# plt.show()
81/10:
data = melb[['Date', 'Temp3pm']]
data.dropna(inplace=True)
data.columns = ['ds', 'y']
data.head()
81/11: py.plot(data, filename='my plot')
81/12: py.plot(Data(data), filename='my plot')
81/13:
data = pd.read_csv('data2.csv')
data.head()
81/14: data[0]
81/15: data[[0]]
81/16: data[0:1]
81/17: data[0:1][0]
81/18: data[0:1]['Timestamp']
81/19: data[0:1]['Timestamp'].slip
81/20: data[0:1]['Timestamp'].split(' ')[0]
81/21: data[0:1]['Timestamp'].split(' ')
81/22: str(data[0:1]['Timestamp']).split(' ')[0]
81/23: str(data[0:1]['Timestamp']).split(' ')
81/24: data[0:1]['Timestamp'].split(' ')
81/25: str(data[0:1]['Timestamp']).split(' ')
81/26: str(data[0:1]['Timestamp']).split(' ')[4]
81/27:
for i in range(len(data)):
    print(i)
# data.drop()
81/28: data.drop(0)
81/29: data.head(1)
81/30: str(data[0:1]['Timestamp']).split(' ')[4] == str(data[1:2]['Timestamp']).split(' ')[4]
81/31: data.tail(2)
81/32: data[572:573]
81/33: data[573:574]
81/34: data[574:575]
81/35: 574-4
81/36: 574-2
81/37: data[573:575]
81/38: data[573:574]
81/39: data[573:574]
81/40:
for i in range(len(data)):
    
    if str(data[i:i+1]['Timestamp']).split(' ')[4] == str(data[i+1:i+2]['Timestamp']).split(' ')[4]:
        data.drop(i, inplace=True)
        
    if i = 573:
        break
81/41:
for i in range(len(data)):
    
    if str(data[i:i+1]['Timestamp']).split(' ')[4] == str(data[i+1:i+2]['Timestamp']).split(' ')[4]:
        data.drop(i, inplace=True)
        
    if i == 573:
        break
81/42: data.head()
81/43: data.tail()
81/44:
data = pd.read_csv('data2.csv')
data.head()
81/45: data.drop_duplicates(["Timestamp"], keep='last', inplace=True)
81/46: data.head()
81/47: data['Timestamp'][0]
81/48: data['Timestamp'][0].split(' ')[4]
81/49: data['Timestamp'][0].split(' ')[0]
81/50:
for i in range(len(data)):
    data['Timestamp'][i] = data['Timestamp'][i].split(' ')[0]

data.head()
81/51: data.head()
81/52: data.drop_duplicates(["Timestamp"], keep='last', inplace=True)
81/53: data.head()
81/54:
data = pd.read_csv('data2.csv')
data.head()
81/55:
data = pd.read_csv('data2.csv')
data.head()
81/56: data['Timestamp'][0]
81/57: data['Timestamp'][0].split(' ')[0]
81/58:
for i in range(len(data)):
    data['Timestamp'][i] = data['Timestamp'][i].split(' ')[0]

data.head()
81/59: data.drop_duplicates(["Timestamp"], keep='last', inplace=True)
81/60: data.head()
83/1:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
from chart_studio import plotly as py
83/2:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
from chart_studio import plotly as py
83/3:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
from chart_studio import plotly as py
83/4:
data = pd.read_csv('data2.csv')
data.head()
83/5: data['Timestamp'][0].split(' ')[0]
83/6:
for i in range(len(data)):
    data['Timestamp'][i] = data['Timestamp'][i].split(' ')[0]

data.head()
83/7: data.loc(::2,)
83/8: data.loc(0::2,)
83/9: data.loc(::2,)
83/10: data.loc(::2,:)
83/11: data.loc(:,:,2)
83/12: data.loc(::2)
83/13: data.loc(:,::2)
83/14: data.loc(:::2)
83/15: data.loc(:2)
83/16: data.loc(0:2,:)
83/17: data.iloc(0:2,:)
83/18: list(range(0,4))
83/19: list(range(0,12,1))
83/20: list(range(0,12))
83/21: list(range(0,12,2))
83/22: data.iloc(:,:,2)
83/23: data.loc(:,:,2)
83/24: data.iloc(:,:,2)
83/25: data.iloc([:,:,2],2)
83/26: data.iloc([,,2],2)
83/27: data.iloc([0,,2],2)
83/28: data.iloc([0,-1,2],2)
83/29: data.iloc([0,-1,2],:)
83/30: data.iloc([0,-1,2])
83/31: data.iloc([0,-1])
83/32: data.loc([0,-1,2],:)
83/33: data.loc([0,-1,2])
83/34: data.loc([0,-1,2],0:-1)
83/35: data.loc([0,-1],2)
83/36: data.loc([0,-1],:)
83/37: data.loc([0,2],:)
83/38: data.loc[[0,2],:]
83/39: data.loc[[0,-1],:]
83/40: data.loc[[0,],:]
83/41: data.loc[,:]
83/42: data.loc[:,:]
83/43: data.loc[:,:,2]
83/44: data.loc[[:,:],2]
83/45: data.loc[:,:]
83/46: data.loc[:2,:]
83/47: data.loc[:2,2:]
83/48: data.loc[:2,0:2]
83/49: data.loc[:2,2]
83/50: data.loc[[0,2],:]
83/51: data.loc[[0,4],:]
83/52: data.iloc[[0,4],:]
83/53: data.loc[0:4,:]
83/54: data.loc[0:4[2],:]
83/55: data.loc[0:4:2,:]
83/56: data.loc[0::2,:]
83/57: data['Inlet Pressure (Bars)'].loc[0::2,:]
83/58: data[data['Inlet Pressure (Bars)']].loc[0::2,:]
83/59: data.loc[0::2,'Inlet Pressure (Bars)']
83/60:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data['Inlet Pressure (Bars)']['06.04.2021 12:00:00']
83/61:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data[data['Inlet Pressure (Bars)']['06.04.2021 12:00:00']]
83/62:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data[data['Inlet Pressure (Bars)']== ['06.04.2021 12:00:00']]
83/63:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data[data['Inlet Pressure (Bars)'] == '06.04.2021 12:00:00']
83/64:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data[data['Inlet Pressure (Bars)'] == '06.04.2021']
83/65:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data[data['Inlet Pressure (Bars)'] == '06.04.2021']
83/66:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data[data['Inlet Pressure (Bars)'] == '02.01.2021']
83/67: data.head()
83/68:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data[data['Inlet Pressure (Bars)'] == ' 01.01.2021']
83/69:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data[data['Inlet Pressure (Bars)'] == ' 01.01.2021']
83/70:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data[data['Inlet Pressure (Bars)'] == ' 01.01.2021 ']
83/71:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data[data['Inlet Pressure (Bars)'] == '01.01.2021 ']
83/72:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data[data['Inlet Pressure (Bars)'] == '01.01.2021']
83/73: # data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
83/74:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data[data['Timestamp'] == '01.01.2021']
83/75:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data[data['Timestamp'] == '06.04.2021']
83/76:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
len(data[data['Timestamp'] == '06.04.2021'])
83/77:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data[data['Timestamp'] == '06.04.2021']
83/78: data[200]
83/79: data[data[200]]
83/80: data[,200]
83/81: data[0200]
83/82: data[0,200]
83/83: data.iloc[200]
83/84: data.iloc[201]
83/85: data.Timestamp.duplicated()
83/86: data.Timestamp.duplicated() == True
83/87:
if data.Timestamp.duplicated() == True:
    True
83/88:
if data.Timestamp.duplicated():
    print(True)
83/89: data['Timestamp']
83/90: data['Timestamp'].anunique()
83/91: data['Timestamp'].nunique()
83/92: data['Timestamp'].duplicated()
83/93: data['Timestamp'].duplicated()['Tur']
83/94: data['Timestamp'].duplicated()['Ture']
83/95: data['Timestamp'].duplicated().Ture
83/96: data['Timestamp'].duplicated().True
83/97: data['Timestamp'].duplicated()['True']
83/98: data['Timestamp'].duplicated()
84/1:
import numpy as np
import cv2
84/2:
img1 = r'C:\Users\Dell\Downloads\traing_mask\1.png'
img2 = r'C:\Users\Dell\Downloads\traing_mask\2.png'
img3 = r'C:\Users\Dell\Downloads\traing_mask\3.png'
img4 = r'C:\Users\Dell\Downloads\traing_mask\4.png'
84/3:
h_img1 = np.hstack((img1,img2))
h_img2 = np.hstack((img3,img4))
h_img = np.hstack((h_img1,h_img2))
84/4: cv2.imwrite('training_mask.png',h_img)
84/5:
h_img1 = np.hstack((img1,img2))
h_img2 = np.hstack((img3,img4))
h_img = np.hstack((h_img1,h_img2))
h_img = cv2.imread(h_img)
84/6:
img1 = r'C:\Users\Dell\Downloads\traing_mask\1.png'
img1 = cv2.imread(img1)
img2 = r'C:\Users\Dell\Downloads\traing_mask\2.png'
img3 = r'C:\Users\Dell\Downloads\traing_mask\3.png'
img4 = r'C:\Users\Dell\Downloads\traing_mask\4.png'
84/7:
img1 = r'C:\Users\Dell\Downloads\traing_mask\1.png'
img1 = cv2.imread(img1)
img2 = r'C:\Users\Dell\Downloads\traing_mask\2.png'
img2 = cv2.imread(img2)
img3 = r'C:\Users\Dell\Downloads\traing_mask\3.png'
img3 = cv2.imread(img3)
img4 = r'C:\Users\Dell\Downloads\traing_mask\4.png'
img4 = cv2.imread(img4)
84/8:
h_img1 = np.hstack((img1,img2))
h_img2 = np.hstack((img3,img4))
h_img = np.hstack((h_img1,h_img2))
h_img = cv2.imread(h_img)
84/9:
h_img1 = np.hstack((img1,img2))
h_img2 = np.hstack((img3,img4))
h_img = np.hstack((h_img1,h_img2))
84/10:
img1 = r'C:\Users\Dell\Downloads\traing_mask\1.png'
img1 = cv2.imread(img1)
img2 = r'C:\Users\Dell\Downloads\traing_mask\2.png'
img2 = cv2.imread(img2)
img3 = r'C:\Users\Dell\Downloads\traing_mask\3.png'
img3 = cv2.imread(img3)
img4 = r'C:\Users\Dell\Downloads\traing_mask\4.png'
img4 = cv2.imread(img4)
84/11:
h_img1 = np.hstack((img1,img2))
h_img2 = np.hstack((img3,img4))
h_img = np.hstack((h_img1,h_img2))
84/12: cv2.imwrite('training_mask.png',h_img)
84/13:
img1 = r'C:\Users\Dell\Downloads\mask\1.jpg'
img1 = cv2.imread(img1)
img2 = r'C:\Users\Dell\Downloads\mask\2.jpg'
img2 = cv2.imread(img2)
img3 = r'C:\Users\Dell\Downloads\mask\3.jpg'
img3 = cv2.imread(img3)
img4 = r'C:\Users\Dell\Downloads\mask\4.jpg'
img4 = cv2.imread(img4)
img5 = r'C:\Users\Dell\Downloads\mask\5.jpg'
img5 = cv2.imread(img5)
img6 = r'C:\Users\Dell\Downloads\mask\6.jpg'
img6 = cv2.imread(img6)
img7 = r'C:\Users\Dell\Downloads\mask\7.jpg'
img7 = cv2.imread(img7)
img8 = r'C:\Users\Dell\Downloads\mask\8.jpg'
img8 = cv2.imread(img8)
img9 = r'C:\Users\Dell\Downloads\mask\9.jpg'
img9 = cv2.imread(img9)
img10 = r'C:\Users\Dell\Downloads\mask\10.jpg'
img10 = cv2.imread(img10)
img11 = r'C:\Users\Dell\Downloads\mask\11.jpg'
img11 = cv2.imread(img11)
img12 = r'C:\Users\Dell\Downloads\mask\12.jpg'
img12 = cv2.imread(img12)
img13 = r'C:\Users\Dell\Downloads\mask\13.jpg'
img13 = cv2.imread(img13)
img14 = r'C:\Users\Dell\Downloads\mask\14.jpg'
img14 = cv2.imread(img14)
img15 = r'C:\Users\Dell\Downloads\mask\15.jpg'
img15 = cv2.imread(img15)
img16 = r'C:\Users\Dell\Downloads\mask\16.jpg'
img16 = cv2.imread(img16)
84/14:
center = img1.shape / 2
x = center[1] - w/2
y = center[0] - h/2

crop_img = img1[int(y):int(y+h), int(x):int(x+w)]
cv2.imshow('img',crop_img)
cv2.waitKey(0)
84/15:
img1 = r'C:\Users\Dell\Downloads\mask\1.jpg'
img1cv = cv2.imread(img1)
img2 = r'C:\Users\Dell\Downloads\mask\2.jpg'
img2cv = cv2.imread(img2)
img3 = r'C:\Users\Dell\Downloads\mask\3.jpg'
img3cv = cv2.imread(img3)
img4 = r'C:\Users\Dell\Downloads\mask\4.jpg'
img4cv = cv2.imread(img4)
img5 = r'C:\Users\Dell\Downloads\mask\5.jpg'
img5cv = cv2.imread(img5)
img6 = r'C:\Users\Dell\Downloads\mask\6.jpg'
img6cv = cv2.imread(img6)
img7 = r'C:\Users\Dell\Downloads\mask\7.jpg'
img7cv = cv2.imread(img7)
img8 = r'C:\Users\Dell\Downloads\mask\8.jpg'
img8cv = cv2.imread(img8)
img9 = r'C:\Users\Dell\Downloads\mask\9.jpg'
img9cv = cv2.imread(img9)
img10 = r'C:\Users\Dell\Downloads\mask\10.jpg'
img10cv = cv2.imread(img10)
img11 = r'C:\Users\Dell\Downloads\mask\11.jpg'
img11cv = cv2.imread(img11)
img12 = r'C:\Users\Dell\Downloads\mask\12.jpg'
img12cv = cv2.imread(img12)
img13 = r'C:\Users\Dell\Downloads\mask\13.jpg'
img13cv = cv2.imread(img13)
img14 = r'C:\Users\Dell\Downloads\mask\14.jpg'
img14cv = cv2.imread(img14)
img15 = r'C:\Users\Dell\Downloads\mask\15.jpg'
img15cv = cv2.imread(img15)
img16 = r'C:\Users\Dell\Downloads\mask\16.jpg'
img16cv = cv2.imread(img16cv)
84/16:
img1 = r'C:\Users\Dell\Downloads\mask\1.jpg'
img1cv = cv2.imread(img1)
img2 = r'C:\Users\Dell\Downloads\mask\2.jpg'
img2cv = cv2.imread(img2)
img3 = r'C:\Users\Dell\Downloads\mask\3.jpg'
img3cv = cv2.imread(img3)
img4 = r'C:\Users\Dell\Downloads\mask\4.jpg'
img4cv = cv2.imread(img4)
img5 = r'C:\Users\Dell\Downloads\mask\5.jpg'
img5cv = cv2.imread(img5)
img6 = r'C:\Users\Dell\Downloads\mask\6.jpg'
img6cv = cv2.imread(img6)
img7 = r'C:\Users\Dell\Downloads\mask\7.jpg'
img7cv = cv2.imread(img7)
img8 = r'C:\Users\Dell\Downloads\mask\8.jpg'
img8cv = cv2.imread(img8)
img9 = r'C:\Users\Dell\Downloads\mask\9.jpg'
img9cv = cv2.imread(img9)
img10 = r'C:\Users\Dell\Downloads\mask\10.jpg'
img10cv = cv2.imread(img10)
img11 = r'C:\Users\Dell\Downloads\mask\11.jpg'
img11cv = cv2.imread(img11)
img12 = r'C:\Users\Dell\Downloads\mask\12.jpg'
img12cv = cv2.imread(img12)
img13 = r'C:\Users\Dell\Downloads\mask\13.jpg'
img13cv = cv2.imread(img13)
img14 = r'C:\Users\Dell\Downloads\mask\14.jpg'
img14cv = cv2.imread(img14)
img15 = r'C:\Users\Dell\Downloads\mask\15.jpg'
img15cv = cv2.imread(img15)
img16 = r'C:\Users\Dell\Downloads\mask\16.jpg'
img16cv = cv2.imread(img16)
84/17:
center = img1.shape / 2
x = center[1] - w/2
y = center[0] - h/2

crop_img = img1[int(y):int(y+h), int(x):int(x+w)]
cv2.imshow('img',crop_img)
cv2.waitKey(0)
84/18:
img1 = r'C:\Users\Dell\Downloads\mask\1.jpg'
img1 = cv2.imread(img1)
img2 = r'C:\Users\Dell\Downloads\mask\2.jpg'
img2 = cv2.imread(img2)
img3 = r'C:\Users\Dell\Downloads\mask\3.jpg'
img3 = cv2.imread(img3)
img4 = r'C:\Users\Dell\Downloads\mask\4.jpg'
img4 = cv2.imread(img4)
img5 = r'C:\Users\Dell\Downloads\mask\5.jpg'
img5 = cv2.imread(img5)
img6 = r'C:\Users\Dell\Downloads\mask\6.jpg'
img6 = cv2.imread(img6)
img7 = r'C:\Users\Dell\Downloads\mask\7.jpg'
img7 = cv2.imread(img7)
img8 = r'C:\Users\Dell\Downloads\mask\8.jpg'
img8 = cv2.imread(img8)
img9 = r'C:\Users\Dell\Downloads\mask\9.jpg'
img9 = cv2.imread(img9)
img10 = r'C:\Users\Dell\Downloads\mask\10.jpg'
img10 = cv2.imread(img10)
img11 = r'C:\Users\Dell\Downloads\mask\11.jpg'
img11 = cv2.imread(img11)
img12 = r'C:\Users\Dell\Downloads\mask\12.jpg'
img12 = cv2.imread(img12)
img13 = r'C:\Users\Dell\Downloads\mask\13.jpg'
img13 = cv2.imread(img13)
img14 = r'C:\Users\Dell\Downloads\mask\14.jpg'
img14 = cv2.imread(img14)
img15 = r'C:\Users\Dell\Downloads\mask\15.jpg'
img15 = cv2.imread(img15)
img16 = r'C:\Users\Dell\Downloads\mask\16.jpg'
img16 = cv2.imread(img16)
84/19:
center = img1.shape / 2
x = center[1] - w/2
y = center[0] - h/2

crop_img = img1[int(y):int(y+h), int(x):int(x+w)]
cv2.imshow('img',crop_img)
cv2.waitKey(0)
84/20: img1.shape
84/21:
x = center[1] - w/2
y = center[0] - h/2

center = (x, y, 3)

crop_img = img1[int(y):int(y+h), int(x):int(x+w)]
cv2.imshow('img',crop_img)
cv2.waitKey(0)
85/1:
data = pd.read_csv('data2.csv')
data.head()
85/2:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
from chart_studio import plotly as py
85/3:
import
data = pd.read_csv('data2.csv')
data.head()
85/4:
data = pd.read_csv('data2.csv')
data.head()
85/5:
data = pd.read_csv('two_columns.csv')
data.head()
85/6: data['Timestamp'][0].split(' ')[0]
85/7:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data[data['Timestamp'] == '06.04.2021']
85/8: data['Timestamp'][0]
85/9:
for i in range(len(data)):
    data['Timestamp'][i] = data['Timestamp'][i].split(' ')[0]

data.head()
85/10:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data[data['Timestamp'] == '06.04.2021']
85/11: data[data.loc[0::2,'Inlet Pressure (Bars)']]
85/12: data.loc[0::2,:]
85/13: data.loc[1::2,'Inlet Pressure (Bars)']
85/14: data.loc[1::2,'Inlet Pressure (Bars)'] #Endding
85/15: data.loc[1::2,'Inlet Temperature (Degree Celsius)'] #Endding
85/16: data.loc[1::2,'Outlet Temperature (Degree Celsius)'] #Endding
85/17: data.loc[1::2]
85/18: data.loc[0::2]
85/19: data.loc[0::2,'Inlet Pressure (Bars)'] #Endding
85/20: data.loc[0::2,'Inlet Temperature (Degree Celsius)'] #Endding
85/21: data.loc[0::2,'Outlet Temperature (Degree Celsius)'] #Endding
85/22: data.loc[1::2]
85/23:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = data.loc[0::2,'Inlet Pressure (Bars)']
new_data['Inlet Temperature (endding)'] = data.loc[0::2,'Inlet Temperature (Degree Celsius)']
new_data['Outlet Temperature (endding)'] = data.loc[0::2,'Outlet Temperature (Degree Celsius)']
new_data.head()
86/1:
import numpy as np
import pandas as pd
from neuralprophet import NeuralProphet
import pickle
from chart_studio import plotly as py
86/2:
data = pd.read_csv('percentage.csv')
data.head()
86/3:
data = pd.read_csv('percentage2.csv')
data.head()
86/4:
data = pd.read_csv('two_columns.csv')
data.head()
86/5:
data = pd.read_csv('two_columns.csv')
data.dropna()
86/6:
data = pd.read_csv('two_columns.csv')
data.head()
86/7:
data = pd.read_csv('two_columns.csv')
data['throughput (target is 4000 a day)'].head()
86/8:
data = pd.read_csv('two_columns.csv')
data['throughput (target is 4000 a day)'].dropna()
86/9:
data = pd.read_csv('two_columns.csv')
data[data['throughput (target is 4000 a day)'].dropna()]
86/10:
data = pd.read_csv('two_columns.csv')
data['throughput (target is 4000 a day)'].dropna()
86/11: data[180:181]
86/12: data[181:182]
86/13:
data = pd.read_csv('two_columns.csv')
data[data['throughput (target is 4000 a day)']].dropna()
86/14:
data = pd.read_csv('two_columns.csv')
data['throughput (target is 4000 a day)'].dropna()
86/15:
data = pd.read_csv('two_columns.csv')
data['throughput (target is 4000 a day)'].dropna(inplace=True)
86/16: data.head()
86/17: data.head()
86/18: data
86/19: data.head()
86/20:
data = pd.read_csv('two_columns.csv')
data['throughput (target is 4000 a day)'].dropna(inplace=True)
data.head()
86/21:
data = pd.read_csv('two_columns.csv')
data[data['throughput (target is 4000 a day)'].dropna()]
86/22:
data = pd.read_csv('two_columns.csv')
data['throughput (target is 4000 a day)'].dropna()
86/23:
data = pd.read_csv('two_columns.csv')
data['throughput (target is 4000 a day)'].dropna().index()
86/24:
data = pd.read_csv('two_columns.csv')
data['throughput (target is 4000 a day)'].dropna()
86/25:
data = pd.read_csv('two_columns.csv')
list(data['throughput (target is 4000 a day)'].dropna())
86/26:
data = pd.read_csv('two_columns.csv')
for i in data.head():
    print(i)
for i in lis?t(data['throughput (target is 4000 a day)'].dropna()):
86/27:
data = pd.read_csv('two_columns.csv')
for i in data.head():
    print(i)
# for i in list(data['throughput (target is 4000 a day)'].dropna()):
86/28:
data = pd.read_csv('two_columns.csv')
for i in data[:4]:
    print(i)
# for i in list(data['throughput (target is 4000 a day)'].dropna()):
86/29:
data = pd.read_csv('two_columns.csv')
for i in data[data[:4]]:
    print(i)
# for i in list(data['throughput (target is 4000 a day)'].dropna()):
86/30:
data = pd.read_csv('two_columns.csv')
for i in data[:4]:
    print(i)
# for i in list(data['throughput (target is 4000 a day)'].dropna()):
86/31:
data = pd.read_csv('two_columns.csv')
for i in data[data[:4]]:
    print(i)
# for i in list(data['throughput (target is 4000 a day)'].dropna()):
86/32: data[:4]
86/33: data[:]
86/34: data[:][2]
86/35: data[:][:2]
86/36:
data = pd.read_csv('two_columns.csv')
for i in data[:][:4]:
    print(i)
# for i in list(data['throughput (target is 4000 a day)'].dropna()):
86/37:
data = pd.read_csv('two_columns.csv')
for i in data:
    print(i)
# for i in list(data['throughput (target is 4000 a day)'].dropna()):
86/38:
data = pd.read_csv('two_columns.csv')
for i in data.series[:4]:
    print(i)
# for i in list(data['throughput (target is 4000 a day)'].dropna()):
86/39:
data = pd.read_csv('two_columns.csv')
for i in data.Series[:4]:
    print(i)
# for i in list(data['throughput (target is 4000 a day)'].dropna()):
86/40:
data = pd.read_csv('two_columns.csv')
for i in data:
    print(i)
# for i in list(data['throughput (target is 4000 a day)'].dropna()):
86/41:
data = pd.read_csv('two_columns.csv')
for i in data[:2]:
    print(i)
# for i in list(data['throughput (target is 4000 a day)'].dropna()):
86/42:
for i in data[:][:2]:
    print(i)
86/43: data.head()
86/44: data.dropna(any='throughput (target is 4000 a day)')
86/45: data.dropna(subset=['throughput (target is 4000 a day)'])
86/46:
data = pd.read_csv('two_columns.csv')
data['throughput (target is 4000 a day)'].dropna()
# for i in list(data['throughput (target is 4000 a day)'].dropna()):
86/47:
data = data.dropna(subset=['throughput (target is 4000 a day)'])
data.shape
86/48: data = pd.read_csv('two_columns.csv')
86/49: data = pd.read_csv('two_columns.csv')
86/50: data['Timestamp'][0].split(' ')[0]
86/51:
for i in range(len(data)):
    data['Timestamp'][i] = data['Timestamp'][i].split(' ')[0]

data.head()
86/52:
new_data = data.loc[1::2]
# new_data['Inlet Pressure (endding)'] = data.loc[0::2,'Inlet Pressure (Bars)']
# new_data['Inlet Temperature (endding)'] = data.loc[0::2,'Inlet Temperature (Degree Celsius)']
# new_data['Outlet Temperature (endding)'] = data.loc[0::2,'Outlet Temperature (Degree Celsius)']
# new_data.head()
86/53:
new_data = data.loc[1::2]
# new_data['Inlet Pressure (endding)'] = data.loc[0::2,'Inlet Pressure (Bars)']
# new_data['Inlet Temperature (endding)'] = data.loc[0::2,'Inlet Temperature (Degree Celsius)']
# new_data['Outlet Temperature (endding)'] = data.loc[0::2,'Outlet Temperature (Degree Celsius)']
new_data.head()
86/54:
new_data = data.loc[1::2]
# new_data['Inlet Pressure (endding)'] = data.loc[0::2,'Inlet Pressure (Bars)']
# new_data['Inlet Temperature (endding)'] = data.loc[0::2,'Inlet Temperature (Degree Celsius)']
# new_data['Outlet Temperature (endding)'] = data.loc[0::2,'Outlet Temperature (Degree Celsius)']
new_data.size
86/55:
new_data = data.loc[1::2]
# new_data['Inlet Pressure (endding)'] = data.loc[0::2,'Inlet Pressure (Bars)']
# new_data['Inlet Temperature (endding)'] = data.loc[0::2,'Inlet Temperature (Degree Celsius)']
# new_data['Outlet Temperature (endding)'] = data.loc[0::2,'Outlet Temperature (Degree Celsius)']
new_data.shape
86/56: data.loc[0::2,'Inlet Pressure (Bars)'].shape #Endding
86/57: data.loc[0::2,'Inlet Pressure (Bars)'] #Endding
86/58: data.loc[0::2,'Inlet Temperature (Degree Celsius)'] #Endding
86/59: data.loc[0::2,'Outlet Temperature (Degree Celsius)'] #Endding
86/60:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = data.loc[0::2,'Inlet Pressure (Bars)']
# new_data['Inlet Temperature (endding)'] = data.loc[0::2,'Inlet Temperature (Degree Celsius)']
# new_data['Outlet Temperature (endding)'] = data.loc[0::2,'Outlet Temperature (Degree Celsius)']
new_data.shape
86/61:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = data.loc[0::2,'Inlet Pressure (Bars)']
# new_data['Inlet Temperature (endding)'] = data.loc[0::2,'Inlet Temperature (Degree Celsius)']
# new_data['Outlet Temperature (endding)'] = data.loc[0::2,'Outlet Temperature (Degree Celsius)']
new_data.head()
86/62:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = data.loc[0::2,'Inlet Pressure (Bars)']
# new_data['Inlet Temperature (endding)'] = data.loc[0::2,'Inlet Temperature (Degree Celsius)']
# new_data['Outlet Temperature (endding)'] = data.loc[0::2,'Outlet Temperature (Degree Celsius)']
new_data.drop(new_data['Unnamed: 12','Unnamed: 13','Unnamed: 14'])
86/63:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = data.loc[0::2,'Inlet Pressure (Bars)']
# new_data['Inlet Temperature (endding)'] = data.loc[0::2,'Inlet Temperature (Degree Celsius)']
# new_data['Outlet Temperature (endding)'] = data.loc[0::2,'Outlet Temperature (Degree Celsius)']
new_data.drop(new_data[new_data['Unnamed: 12','Unnamed: 13','Unnamed: 14']])
86/64:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = data.loc[0::2,'Inlet Pressure (Bars)']
# new_data['Inlet Temperature (endding)'] = data.loc[0::2,'Inlet Temperature (Degree Celsius)']
# new_data['Outlet Temperature (endding)'] = data.loc[0::2,'Outlet Temperature (Degree Celsius)']
new_data.drop(new_data[['Unnamed: 12','Unnamed: 13','Unnamed: 14']])
86/65:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = data.loc[0::2,'Inlet Pressure (Bars)']
# new_data['Inlet Temperature (endding)'] = data.loc[0::2,'Inlet Temperature (Degree Celsius)']
# new_data['Outlet Temperature (endding)'] = data.loc[0::2,'Outlet Temperature (Degree Celsius)']
new_data.drop('Unnamed: 12','Unnamed: 13','Unnamed: 14')
86/66:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = data.loc[0::2,'Inlet Pressure (Bars)']
# new_data['Inlet Temperature (endding)'] = data.loc[0::2,'Inlet Temperature (Degree Celsius)']
# new_data['Outlet Temperature (endding)'] = data.loc[0::2,'Outlet Temperature (Degree Celsius)']
new_data.drop('Unnamed: 12')
86/67:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = data.loc[0::2,'Inlet Pressure (Bars)']
# new_data['Inlet Temperature (endding)'] = data.loc[0::2,'Inlet Temperature (Degree Celsius)']
# new_data['Outlet Temperature (endding)'] = data.loc[0::2,'Outlet Temperature (Degree Celsius)']
new_data.drop('Unnamed: 12',axis=1)
86/68:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = data.loc[0::2,'Inlet Pressure (Bars)']
# new_data['Inlet Temperature (endding)'] = data.loc[0::2,'Inlet Temperature (Degree Celsius)']
# new_data['Outlet Temperature (endding)'] = data.loc[0::2,'Outlet Temperature (Degree Celsius)']
new_data.drop(['Unnamed: 12','Unnamed: 13','Unnamed: 14'],,axis=1)
86/69:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = data.loc[0::2,'Inlet Pressure (Bars)']
# new_data['Inlet Temperature (endding)'] = data.loc[0::2,'Inlet Temperature (Degree Celsius)']
# new_data['Outlet Temperature (endding)'] = data.loc[0::2,'Outlet Temperature (Degree Celsius)']
new_data.drop(['Unnamed: 12','Unnamed: 13','Unnamed: 14'],axis=1)
86/70: data.loc[0::2,'Inlet Pressure (Bars)']
86/71: data.loc[0::2,'Inlet Pressure (Bars)'].shape
86/72: data['Inlet Temperature (Degree Celsius)']
86/73: sum(data['Inlet Temperature (Degree Celsius)'])
86/74: sum(new_data['Inlet Pressure (endding)'])
86/75: ip_end = data.loc[0::2,'Inlet Pressure (Bars)'] #Endding
86/76: it_end = data.loc[0::2,'Inlet Temperature (Degree Celsius)'] #Endding
86/77: ot_end = data.loc[0::2,'Outlet Temperature (Degree Celsius)'] #Endding
86/78: type(ip_end)
86/79:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = list(data.loc[0::2,'Inlet Pressure (Bars)'])
# new_data['Inlet Temperature (endding)'] = data.loc[0::2,'Inlet Temperature (Degree Celsius)']
# new_data['Outlet Temperature (endding)'] = data.loc[0::2,'Outlet Temperature (Degree Celsius)']
new_data.drop(['Unnamed: 12','Unnamed: 13','Unnamed: 14'],axis=1)
86/80:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = list(data.loc[0::2,'Inlet Pressure (Bars)'])
new_data['Inlet Temperature (endding)'] = list(data.loc[0::2,'Inlet Temperature (Degree Celsius)'])
new_data['Outlet Temperature (endding)'] = list(data.loc[0::2,'Outlet Temperature (Degree Celsius)'])
new_data.drop(['Unnamed: 12','Unnamed: 13','Unnamed: 14'],axis=1)
86/81: new_data.shape
86/82: new_data = data.loc[1::2]
86/83:
new_data = data.loc[1::2]
new_data.head()
86/84:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = list(data.loc[0::2,'Inlet Pressure (Bars)'])
new_data['Inlet Temperature (endding)'] = list(data.loc[0::2,'Inlet Temperature (Degree Celsius)'])
new_data['Outlet Temperature (endding)'] = list(data.loc[0::2,'Outlet Temperature (Degree Celsius)'])
new_data.drop(['Unnamed: 12','Unnamed: 13','Unnamed: 14'],axis=1)
new_data.dropna(subset=['throughput (target is 4000 a day)'])
86/85:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = list(data.loc[0::2,'Inlet Pressure (Bars)'])
new_data['Inlet Temperature (endding)'] = list(data.loc[0::2,'Inlet Temperature (Degree Celsius)'])
new_data['Outlet Temperature (endding)'] = list(data.loc[0::2,'Outlet Temperature (Degree Celsius)'])
new_data.dropna(subset=['throughput (target is 4000 a day)'])
new_data.drop(['Unnamed: 12','Unnamed: 13','Unnamed: 14'],axis=1)
86/86:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = list(data.loc[0::2,'Inlet Pressure (Bars)'])
new_data['Inlet Temperature (endding)'] = list(data.loc[0::2,'Inlet Temperature (Degree Celsius)'])
new_data['Outlet Temperature (endding)'] = list(data.loc[0::2,'Outlet Temperature (Degree Celsius)'])
new_data.drop(['Unnamed: 12','Unnamed: 13','Unnamed: 14'],axis=1)
new_data.dropna(subset=['throughput (target is 4000 a day)'])
new_data.drop(['Unnamed: 12','Unnamed: 13','Unnamed: 14'],axis=1)
86/87:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = list(data.loc[0::2,'Inlet Pressure (Bars)'])
new_data['Inlet Temperature (endding)'] = list(data.loc[0::2,'Inlet Temperature (Degree Celsius)'])
new_data['Outlet Temperature (endding)'] = list(data.loc[0::2,'Outlet Temperature (Degree Celsius)'])
new_data.dropna(subset=['throughput (target is 4000 a day)'])
new_data.drop(['Unnamed: 12','Unnamed: 13','Unnamed: 14'],axis=1)
86/88:
new_data = data.loc[1::2]
new_data['Inlet Pressure (endding)'] = list(data.loc[0::2,'Inlet Pressure (Bars)'])
new_data['Inlet Temperature (endding)'] = list(data.loc[0::2,'Inlet Temperature (Degree Celsius)'])
new_data['Outlet Temperature (endding)'] = list(data.loc[0::2,'Outlet Temperature (Degree Celsius)'])
86/89:
new_data.dropna(subset=['throughput (target is 4000 a day)'])
new_data.drop(['Unnamed: 12','Unnamed: 13','Unnamed: 14'],axis=1)
86/90:
new_data.dropna(subset=['throughput (target is 4000 a day)'])
new_data.drop(['Unnamed: 12','Unnamed: 13','Unnamed: 14'],axis=1)
new_data.dropna(subset=['throughput (target is 4000 a day)'])
86/91: new_data.shape
86/92:
new_data.dropna(subset=['throughput (target is 4000 a day)'],inplace=True)
new_data.drop(['Unnamed: 12','Unnamed: 13','Unnamed: 14'],axis=1,inplace=True)
86/93: new_data.shape
86/94: new_data.to_csv('final.csv')
86/95: new_data.to_csv('final.csv',index=None)
86/96: df = df = pd.read_csv('final.csv', names=['Timestamp', 'Machine Type', 'Inlet Pressure (Bars)', 'Inlet Pressure (endding)', 'Remarks', 'Inlet Temperature (Degree Celsius)', 'Inlet Temperature (endding)', 'Remarks1', 'Outlet Temperature (Degree Celsius)', 'Outlet Temperature (endding)',  'Remarks2', 'Additional Info',  'throughput (target is 4000 a day)',    'At What % is the machine functioning?',    'Bellow 55 perc'])
86/97: df
86/98: df.to_csv('final.csv',index=None)
86/99: df = pd.read_csv('final.csv', names=['Timestamp', 'Machine Type', 'Inlet Pressure (Bars)', 'Inlet Pressure (endding)', 'Remarks', 'Inlet Temperature (Degree Celsius)', 'Inlet Temperature (endding)',  'Remarks1', 'Outlet Temperature (Degree Celsius)', 'Outlet Temperature (endding)',  'Remarks2', 'Additional Info',  'throughput (target is 4000 a day)',    'At What % is the machine functioning?',    'Bellow 55 perc'])
86/100: df.to_csv('final.csv',index=None)
86/101: df.to_csv('final.csv',index=None)
86/102: df
86/103: df.to_csv('final.csv',index=None,columns=None)
86/104: df.to_csv('final.csv',index=None,columns=None)
86/105: df
86/106: df.columns(None)
86/107: df.columns()
86/108: df.columns
86/109: new_data.head()
86/110:
new_data = data.loc[1::2]
new_data['Inlet Pressure (starting)'] = list(data.loc[0::2,'Inlet Pressure (Bars)'])
new_data['Inlet Temperature (starting)'] = list(data.loc[0::2,'Inlet Temperature (Degree Celsius)'])
new_data['Outlet Temperature (starting)'] = list(data.loc[0::2,'Outlet Temperature (Degree Celsius)'])
86/111:
new_data.dropna(subset=['throughput (target is 4000 a day)'],inplace=True)
new_data.drop(['Unnamed: 12','Unnamed: 13','Unnamed: 14'],axis=1,inplace=True)
86/112: df = pd.read_csv('final.csv', names=['Timestamp', 'Machine Type', 'Inlet Pressure (Bars)', 'Inlet Pressure (endding)', 'Remarks', 'Inlet Temperature (Degree Celsius)', 'Inlet Temperature (endding)',  'Remarks1', 'Outlet Temperature (Degree Celsius)', 'Outlet Temperature (endding)',  'Remarks2', 'Additional Info',  'throughput (target is 4000 a day)',    'At What % is the machine functioning?',    'Bellow 55 perc'])
86/113: df.to_csv('final.csv',index=None,columns=None)
86/114: df.columns
86/115:
new_data = data.loc[1::2]
new_data['Inlet Pressure (starting)'] = list(data.loc[0::2,'Inlet Pressure (Bars)'])
new_data['Inlet Temperature (starting)'] = list(data.loc[0::2,'Inlet Temperature (Degree Celsius)'])
new_data['Outlet Temperature (starting)'] = list(data.loc[0::2,'Outlet Temperature (Degree Celsius)'])
86/116:
new_data.dropna(subset=['throughput (target is 4000 a day)'],inplace=True)
new_data.drop(['Unnamed: 12','Unnamed: 13','Unnamed: 14'],axis=1,inplace=True)
86/117: new_data.to_csv('final.csv',index=None)
86/118: df = pd.read_csv('final.csv', names=['Timestamp', 'Machine Type', 'Inlet Pressure (Bars)', 'Inlet Pressure (starting)', 'Remarks', 'Inlet Temperature (Degree Celsius)', 'Inlet Temperature (starting)',    'Remarks1', 'Outlet Temperature (Degree Celsius)', 'Outlet Temperature (starting)', 'Remarks2', 'Additional Info',  'throughput (target is 4000 a day)',    'At What % is the machine functioning?',    'Bellow 55 perc'])
86/119: df.to_csv('final.csv',index=None,columns=None)
86/120: df.columns
86/121: new_data.head()
86/122: df.head()
86/123: new_data.head()
86/124:
new_data.drop(['Machine Type','Remarks','Remarks','Additional Info','At What % is the machine functioning?','Bellow 55 perc'],inplace=True)
new_data.head()
86/125:
new_data.drop(new_data['Machine Type','Remarks','Remarks','Additional Info','At What % is the machine functioning?','Bellow 55 perc'],inplace=True)
new_data.head()
86/126:
new_data.drop(['Machine Type','Remarks','Remarks','Additional Info','At What % is the machine functioning?','Bellow 55 perc'], axis=1, inplace=True)
new_data.head()
86/127: data = pd.read_csv('two_columns.csv')
86/128: data['Timestamp'][0].split(' ')[0]
86/129:
for i in range(len(data)):
    data['Timestamp'][i] = data['Timestamp'][i].split(' ')[0]

data.head()
86/130:
# data.drop_duplicates(["Timestamp"], keep='last', inplace=True) # This line helps to remove other
data[data['Timestamp'] == '06.04.2021']
86/131:
new_data = data.loc[1::2]
new_data['Inlet Pressure (starting)'] = list(data.loc[0::2,'Inlet Pressure (Bars)'])
new_data['Inlet Temperature (starting)'] = list(data.loc[0::2,'Inlet Temperature (Degree Celsius)'])
new_data['Outlet Temperature (starting)'] = list(data.loc[0::2,'Outlet Temperature (Degree Celsius)'])
86/132:
new_data.dropna(subset=['throughput (target is 4000 a day)'],inplace=True)
new_data.drop(['Unnamed: 12','Unnamed: 13','Unnamed: 14'],axis=1,inplace=True)
86/133:
new_data.drop(['Machine Type','Remarks','Remarks1','Remarks2','Additional Info','At What % is the machine functioning?','Bellow 55 perc'], axis=1, inplace=True)
new_data.head()
86/134: arrangement = ['Timestamp', 'Inlet Pressure (starting)', 'Inlet Pressure (Bars)', 'Inlet Temperature (starting)', Inlet Temperature (Degree Celsius), 'Outlet Temperature (starting)', 'Outlet Temperature (Degree Celsius)', 'throughput (target is 4000 a day)']
86/135: arrangement = ['Timestamp', 'Inlet Pressure (starting)', 'Inlet Pressure (Bars)', 'Inlet Temperature (starting)', 'Inlet Temperature (Degree Celsius)', 'Outlet Temperature (starting)', 'Outlet Temperature (Degree Celsius)', 'throughput (target is 4000 a day)']
86/136:
arrangement = ['Timestamp', 'Inlet Pressure (starting)', 'Inlet Pressure (Bars)', 'Inlet Temperature (starting)', 'Inlet Temperature (Degree Celsius)', 'Outlet Temperature (starting)', 'Outlet Temperature (Degree Celsius)', 'throughput (target is 4000 a day)']
new_data = new_data[arrangement]
new_data.head()
86/137: new_data.to_csv('train_data.csv')
86/138: new_data.to_csv('train_data.csv',index=None)
86/139: new_data.rename(columns = {'Inlet Pressure (Bars)':'Inlet Pressure (endding)','Inlet Temperature (Degree Celsius)':'Inlet Temperature (endding)','Outlet Temperature (Degree Celsius)':'Outlet Temperature (endding)','throughput (target is 4000 a day)':'Throughput'},inplace=True)
86/140: new_data.to_csv('train_data.csv',index=None)
86/141: new_data.to_csv('train_data.csv',index=None)
86/142:
test = pd.read_csv('two_columns.csv')
test.head()
86/143:
test = pd.read_csv('data.csv')
test.head()
86/144: test.drop(['Machine Type', 'Remarks', 'Remarks1', 'Remarks2', 'Additional Info',],axis=1,inplace=True)
86/145:
for i in range(len(test)):
    test['Timestamp'][i] = test['Timestamp'][i].split(' ')[0]

test.head()
86/146:
test = pd.read_csv('two_columns.csv')
test.head()
86/147: test.drop(['Machine Type', 'Remarks', 'Remarks1', 'Remarks2', 'Additional Info', 'throughput (target is 4000 a day)', 'At What % is the machine functioning?', 'Bellow 55 perc', 'Unnamed: 12','Unnamed: 13', 'Unnamed: 14'],axis=1,inplace=True)
86/148:
for i in range(len(test)):
    test['Timestamp'][i] = test['Timestamp'][i].split(' ')[0]

test.head()
86/149: odd_data = data.loc[0::2]
86/150:
odd_data = data.loc[0::2]
odd_data.head(1)
86/151:
odd_data = data.loc[0::2]
odd_data.head(2)
86/152:
odd_data = test.loc[0::2]
odd_data.head(2)
86/153:
odd_data = test.loc[0::2]
odd_data['Inlet Pressure (ending)'] = list(test.loc[1::2,'Inlet Pressure (Bars)'])
odd_data['Inlet Temperature (ending)'] = list(test.loc[1::2,'Inlet Temperature (Degree Celsius)'])
odd_data['Outlet Temperature (ending)'] = list(test.loc[1::2,'Outlet Temperature (Degree Celsius)'])
odd_data.head()
86/154: test.shape, odd_data.shape
86/155: 273*2
86/156: odd_data.rename(columns = {'Inlet Pressure (Bars)':'Inlet Pressure (starting)','Inlet Temperature (Degree Celsius)':'Inlet Temperature (starting)','Outlet Temperature (Degree Celsius)':'Outlet Temperature (starting)'},inplace=True)
86/157: odd_data.head()
86/158:
test_arrangement = ['Timestamp', 'Inlet Pressure (starting)', 'Inlet Pressure (ending)', 'Inlet Temperature (starting)', 'Inlet Temperature (ending)', 'Outlet Temperature (starting)', 'Outlet Temperature (ending)']
odd_data = new_data[test_arrangement]
odd_data.head()
86/159:
test_arrangement = ['Timestamp', 'Inlet Pressure (starting)', 'Inlet Pressure (ending)', 'Inlet Temperature (starting)', 'Inlet Temperature (ending)', 'Outlet Temperature (starting)', 'Outlet Temperature (ending)']
odd_data = odd_data[test_arrangement]
odd_data.head()
86/160: odd_data.to_csv('test_data.csv')
86/161:
#train data nhi nikala

if '26.51047134' in odd['Inlet Pressure (starting)']:
    print(True)
86/162:
#train data nhi nikala

if '26.51047134' in odd_data['Inlet Pressure (starting)']:
    print(True)
86/163:
#train data nhi nikala

if '26.510471' in odd_data['Inlet Pressure (starting)']:
    print(True)
86/164:
#train data nhi nikala

if '57.52155779' in odd_data['Inlet Pressure (starting)']:
    print(True)
86/165:
#train data nhi nikala

if '57.521558' in odd_data['Inlet Pressure (starting)']:
    print(True)
86/166:
#train data nhi nikala

if '57.521557' in odd_data['Inlet Pressure (starting)']:
    print(True)
86/167: odd_data['Inlet Pressure (starting)']
86/168:
#train data nhi nikala

if '50.124875' in odd_data['Inlet Pressure (starting)']:
    print(True)
86/169:
#train data nhi nikala

if '50.124875' in odd_data['Inlet Pressure (starting)']:
    print(True)
86/170: odd_data['Inlet Pressure (starting)'][0]
86/171:
#train data nhi nikala

for i in odd_data['Inlet Pressure (starting)']:
    if '26.51047134' in i:
        print(True)
86/172:
#train data nhi nikala

for i in odd_data['Inlet Pressure (starting)']:
    if '26.51047134' == i:
        print(True)
86/173:
#train data nhi nikala

for i in odd_data['Inlet Pressure (starting)']:
    if i == 26.51047134:
        print(True)
86/174:
#train data nhi nikala

for i in df['Inlet Pressure (starting)']:
    if i in 26.51047134:
        print(True)
86/175:
#train data nhi nikala

for i in df['Inlet Pressure (starting)']:
    if i == 26.51047134:
        print(True)
86/176: df.head()
86/177:
#train data nhi nikala

for i in data['Inlet Pressure (starting)']:
    if i == 26.51047134:
        print(True)
86/178: data['Inlet Pressure (starting)']
86/179: data['Inlet Pressure(starting)']
86/180: new_data['Inlet Pressure (starting)']
86/181:
#train data nhi nikala

for i in data['Inlet Pressure (starting)']:
    if i in list(new_data['Inlet Pressure (starting)']):
        print(True)
86/182:
#train data nhi nikala

for i in odd_data['Inlet Pressure (starting)']:
    if i in list(new_data['Inlet Pressure (starting)']):
        print(True)
86/183:
#train data nhi nikala

for i in odd_data['Inlet Pressure (starting)']:
    if i in list(new_data['Inlet Pressure (starting)']):
        print(i)
86/184:
#train data nhi nikala

for i in odd_data['Inlet Pressure (starting)']:
    if i in list(new_data['Inlet Pressure (starting)']):
        odd_data.drop(i,inplace=True)
86/185:
#train data nhi nikala

for i in odd_data['Inlet Pressure (starting)']:
    if i in list(new_data['Inlet Pressure (starting)']):
        odd_data.drop(float(i),inplace=True)
86/186:
#train data nhi nikala

for i in odd_data['Inlet Pressure (starting)']:
    if i in list(new_data['Inlet Pressure (starting)']):
        odd_data.drop(i,columns=['Inlet Pressure (starting)',inplace=True)
86/187:
#train data nhi nikala

for i in odd_data['Inlet Pressure (starting)']:
    if i in list(new_data['Inlet Pressure (starting)']):
        odd_data.drop(i,columns=['Inlet Pressure (starting)'],inplace=True)
86/188:
#train data nhi nikala

for i in odd_data['Inlet Pressure (starting)']:
    if i in list(new_data['Inlet Pressure (starting)']):
        odd_data.drop(i,columns='Inlet Pressure (starting)',inplace=True)
86/189:
#train data nhi nikala

for i in odd_data['Inlet Pressure (starting)']:
    if i in list(new_data['Inlet Pressure (starting)']):
        odd_data.drop(index=odd_data[odd_data['Inlet Pressure (starting)']==i],inplace=True)
86/190:
#train data nhi nikala

for i in odd_data['Inlet Pressure (starting)']:
    if i in list(new_data['Inlet Pressure (starting)']):
        odd_data.drop(index=odd_data[odd_data['Inlet Pressure (starting)']==i].index,inplace=True)
86/191: odd_data['Inlet Pressure (starting)'][0]
86/192: odd_data.head()
86/193: new_data['Inlet Pressure (starting)']
86/194:
for i in odd_data['Inlet Pressure (starting)']:
    if i in list(new_data['Inlet Pressure (starting)']):
        print(i)
86/195: odd_data.to_csv('test_data.csv')
86/196: odd_data.to_csv('test_data.csv',index=None)
86/197: new_data.shape
86/198: new_data.head()
86/199: new_data.shape
87/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
87/2: dataframe = pd.read_csv("train_data.csv")
87/3: dataframe
87/4: dataframe.shape
87/5: dataframe.shape[0]
87/6: 43*.75
87/7: 43*.7
87/8: 43*.69
87/9: 43*.71
87/10: 43*.72
87/11: 43*.73
87/12: 43*.74
87/13: 43*.75
87/14: 43*.6
87/15: 43*.65
87/16: 43*.61
87/17: 43*.6
87/18: 43*.29
87/19: 43*.73
87/20: 43*.95
87/21: 43*.85
87/22: 43*.82
87/23: 43*.76
87/24: 43*.74
87/25: 43-11-11
87/26: 43-11
87/27: 11/43
87/28: datframe.corr()
87/29: dataframe.corr()
87/30:
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt
87/31: sns.distplot(dataset['Throughput'])
87/32: sns.distplot(dataframe['Throughput'])
87/33: sns.regplot(x=dataframe['Inlet Pressure (endding)'],y=dataframe['Throughput'])
87/34: sns.regplot(x=dataframe['Throughput'],y=dataframe['Inlet Pressure (endding)'])
87/35: sns.regplot(x=dataframe['Inlet Pressure (starting)','Throughput'],y=dataframe['Inlet Pressure (endding)'])
87/36: sns.regplot(x=dataframe['Throughput'],y=dataframe['Inlet Pressure (endding)'])
87/37: sns.regplot(x=dataframe['Throughput~Inlet Pressure (starting)'],y=dataframe['Inlet Pressure (endding)'])
87/38: sns.regplot(x=dataframe['Throughput'],y=dataframe['Inlet Pressure (endding)'])
87/39: model=smf.ols("Throughput~Inlet Pressure (starting)~Inlet Pressure (endding)~Inlet Temperature (starting)~Inlet Temperature (endding)~Outlet Temperature (starting)~Outlet Temperature (endding)",data=dataframe).fit()
87/40: model=smf.ols("Throughput~Inlet Pressure (endding)",data=dataframe).fit()
87/41: model=smf.ols("Throughput~'Inlet Pressure (endding)'",data=dataframe).fit()
87/42: dataframe.rename(columns = {'Inlet Pressure (starting)':'a','Inlet Pressure (endding)':'b','Inlet Temperature (starting)':'c','Inlet Temperature (endding)':'d','Outlet Temperature (starting)':'e','Outlet Temperature (endding)':'f'},inplace=True)
87/43: model=smf.ols("Throughput~a~b~c~d~e~f",data=dataset).fit()
87/44: model=smf.ols("Throughput~a~b~c~d~e~f",data=dataframe).fit()
87/45: model=smf.ols("Throughput~b",data=dataframe).fit()
91/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
91/2:
data=pd.read_csv('/content/train_data.csv')
df=pd.DataFrame(data)
df.tail(10)
91/3:
data=pd.read_csv('train_data.csv')
df=pd.DataFrame(data)
df.tail(10)
91/4:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
91/5:
data=pd.read_csv('combine.csv')
df=pd.DataFrame(data)
# df.tail(10)
91/6: data.head()
91/7:
import seaborn as sns
corr = data.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="YlOrRd")
91/8:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
91/9:
data=pd.read_csv('cause_failure.csv')
data.head(3)
91/10:
data=pd.read_csv('cause_failure.csv')
data.head(3)
91/11:
import seaborn as sns
corr = data.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="YlOrRd")
91/12:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
91/13:
data=pd.read_csv('cause_failure.csv')
df.head(3)
91/14:
data=pd.read_csv('cause_failure.csv')
data.head(3)
91/15:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

X = df['Throughput'] 
Y = df['Label']
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, test_size=0.832,  shuffle=False)
reg=linear_model.LinearRegression()
reg.fit(x_train,y_train)
91/16: data.values.head()
91/17: data.values
91/18: data.values[0]
91/19: data.values
91/20: data.values[0]
91/21: list(data.values[0])
91/22: list(data.values)
91/23: list(data.values())
91/24: list(data.values)
91/25: list(data.values.head)
91/26: list(data.values)
91/27: list(data.values[0])
91/28: data.values
91/29: data.shape
91/30: data.shape[0]*0.6
91/31: data.shape
91/32:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

X = data['Throughput'] 
Y = data['Label']
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, test_size=0.2,  shuffle=False)
# reg=linear_model.LinearRegression()
# reg.fit(x_train,y_train)
91/33:
from sklearn.preprocessing import LabelEncoder
# encode class values as integers
encoder= LabelEncoder()
encoded_Y = encoder.fit_transform(y_train)
encoded_y_train = tf.keras.utils.to_categorical(encoded_Y)

encoded_Y = encoder.fit_transform(y_test)
encoded_y_test = tf.keras.utils.to_categorical(encoded_Y)
91/34:
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
# encode class values as integers
encoder= LabelEncoder()
encoded_Y = encoder.fit_transform(y_train)
encoded_y_train = tf.keras.utils.to_categorical(encoded_Y)

encoded_Y = encoder.fit_transform(y_test)
encoded_y_test = tf.keras.utils.to_categorical(encoded_Y)
91/35:
reg = linear_model.LinearRegression()
reg.fit(x_train, encoded_y_train)
91/36: 250*.2
91/37: 250*.8
91/38: data.reshape(1, -1)
91/39: type(data)#.reshape(1, -1)
91/40: np.array(data).reshape(1, -1)
91/41: np.array(data).reshape(-1, 1)
91/42: encoded_y_train
91/43: encoded_y_train.shape
91/44: x_train.shape
91/45:
reg = linear_model.LinearRegression()
reg.fit(x_train, y_train)
91/46: x_train[4:6]
91/47: data[0]
91/48: data[0][0]
91/49: data[data[0]]
91/50: data[data['Throughput']]
91/51: data['Throughput']
91/52: data.Throughput
91/53: data.Throughput.head"
91/54: data.Throughput.head
91/55:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
91/56:
data=pd.read_csv('cause_failure.csv')
data.head(3)
91/57:
plt.scatter(data['Label'],df['Throughput'],color='red')
plt.title("Throughput Vs P1")
plt.xlabel("Throughput")
plt.xlabel("P1")
plt.grid()
plt.show()
91/58: data.shape
91/59:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

X = data['Throughput'] 
Y = data['Label']
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, test_size=0.2,  shuffle=False)

reg = linear_model.LinearRegression()
reg.fit(x_train, y_train)
91/60:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

Y = data['Label'] 
X = data['Throughput']
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
reg=linear_model.LinearRegression()
reg.fit(x_train,y_train)
91/61:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

Y = data['Label'] 
X = data['Throughput']
 
# x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
# reg=linear_model.LinearRegression()
# reg.fit(x_train,y_train)
X.size, Y.size
91/62:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

Y = data['Label'] 
X = data['Throughput']
 
# x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
# reg=linear_model.LinearRegression()
# reg.fit(x_train,y_train)
X.shape, Y.shape
91/63:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

Y = data['Label'] 
X = data['Throughput']
 
# x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
# reg=linear_model.LinearRegression()
# reg.fit(x_train,y_train)
x_train.tail()
91/64:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

Y = data['Label'] 
X = data['Throughput']
 
# x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
# reg=linear_model.LinearRegression()
# reg.fit(x_train,y_train)
x_test.tail()
91/65:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

Y = data['Label'] 
X = data['Throughput']
 
# x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
# reg=linear_model.LinearRegression()
# reg.fit(x_train,y_train)
data.tail()
91/66:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

Y = data['Label'] 
X = data['Throughput']
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
reg=linear_model.LinearRegression()
reg.fit(x_train,y_train)
91/67:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

data.reshape(-1, 1)

Y = data['Label'] 
X = data['Throughput']
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
reg=linear_model.LinearRegression()
reg.fit(x_train,y_train)
91/68:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

df = np.array(data.reshape(-1, 1))
df
# Y = data['Label'] 
# X = data['Throughput']
 
# x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
# reg=linear_model.LinearRegression()
# reg.fit(x_train,y_train)
91/69:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

df = np.array(data).reshape(-1, 1))
df
# Y = data['Label'] 
# X = data['Throughput']
 
# x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
# reg=linear_model.LinearRegression()
# reg.fit(x_train,y_train)
91/70:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

df = np.array(data).reshape(-1, 1)
df
# Y = data['Label'] 
# X = data['Throughput']
 
# x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
# reg=linear_model.LinearRegression()
# reg.fit(x_train,y_train)
91/71: data
91/72: np.reshape(data)
91/73: np.array(data)
91/74: data['Label']
96/1:
import tensorflow as tf

gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
96/2:
import win32com.client
import os
96/3:
import win32com.client
import os

word = win32com.client.Dispatch("word.Application")
word.visible=0

doc_pdf = r"C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf"
input_file = os.path.abspath(doc_pdf)

wb = word.Documents.Open(input_file)
output_file = os.path.abspath(doc_pdf[0:-4] +"docx".format())
wb.SaveAs2(output_file, FileFormat=16)
print("PDF to DOCx is complete")
wb.Close()

word.Quit()
97/1: print('i')
97/2:
import fitz

file = fitz.open(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf')

for pageNumber, page in enumerate(file.pages(), start = 1):

    text = page.getText()

    txt = open(f'report_Page_{pageNumber}.txt', 'a')
    txt.writelines(text)
98/1:
import fitz

file = fitz.open(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf')

for pageNumber, page in enumerate(file.pages(), start = 1):

    text = page.getText()

    txt = open(f'report_Page_{pageNumber}.txt', 'a')
    txt.writelines(text)
99/1:
import fitz

file = fitz.open(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf')

for pageNumber, page in enumerate(file.pages(), start = 1):

    text = page.getText()

    txt = open(f'report_Page_{pageNumber}.txt', 'a')
    txt.writelines(text)
99/2:
import fitz

file = fitz.open(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf')

for pageNumber, page in zip(enumerate(file.pages(), start = 1)):

    text = page.getText()

    txt = open(f'report_Page_{pageNumber}.txt', 'a')
    txt.writelines(text)
99/3:
import fitz

file = fitz.open(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf')

for pageNumber, page in zip(enumerate(file.pages()), start = 1):

    text = page.getText()

    txt = open(f'report_Page_{pageNumber}.txt', 'a')
    txt.writelines(text)
99/4:
import fitz

file = fitz.open(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf')

for pageNumber, page in zip(enumerate(file.pages(), start = 1):

    text = page.getText()

    txt = open(f'report_Page_{pageNumber}.txt', 'a')
    txt.writelines(text)
99/5:
import fitz

file = fitz.open(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf')

for pageNumber, page in zip(enumerate(file.pages()), start = 1:

    text = page.getText()

    txt = open(f'report_Page_{pageNumber}.txt', 'a')
    txt.writelines(text)
99/6:
import fitz

file = fitz.open(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf')

for pageNumber, page in zip(enumerate(file.pages()):

    text = page.getText()

    txt = open(f'report_Page_{pageNumber}.txt', 'a')
    txt.writelines(text)
99/7:
import fitz

file = fitz.open(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf')

for pageNumber, page in enumerate(file.pages(), start = 1):

    text = page.getText()

    txt = open(f'report_Page_{pageNumber}.txt', 'a')
    txt.writelines(text)
100/1:
import fitz

file = fitz.open(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf')

for pageNumber, page in enumerate(file.pages(), start = 1):

    text = page.getText()

    txt = open(f'report_Page_{pageNumber}.txt', 'a')
    txt.writelines(text)
101/1:
import fitz

file = fitz.open(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf')

for pageNumber, page in enumerate(file.pages(), start = 1):

    text = page.getText()

    txt = open(f'report_Page_{pageNumber}.txt', 'a')
    txt.writelines(text)
102/1:
import fitz

file = fitz.open(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf')

for pageNumber, page in enumerate(file.pages(), start = 1):

    text = page.getText()

    txt = open(f'report_Page_{pageNumber}.txt', 'a')
    txt.writelines(text)
103/1:
import fitz

file = fitz.open(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf')

for pageNumber, page in enumerate(file.pages(), start = 1):

    text = page.getText()

    txt = open(f'report_Page_{pageNumber}.txt', 'a')
    txt.writelines(text)
103/2:
from tika import parser # pip install tika

raw = parser.from_file(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf')
print(raw['content'])
103/3:
import fitz # install using: pip install PyMuPDF

with fitz.open(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf') as doc:
    text = ""
    for page in doc:
        text += page.get_text()

print(text)
103/4:
import pdfplumber

    pdf_obj = pdfplumber.open(doc_path)
    page = pdf_obj.pages[page_no]
    images_in_page = page.images
    page_height = page.height
    image = images_in_page[0] # assuming images_in_page has at least one element, only for understanding purpose. 
    image_bbox = (image['x0'], page_height - image['y1'], image['x1'], page_height - image['y0'])
    cropped_page = page.crop(image_bbox)
    image_obj = cropped_page.to_image(resolution=400)
    image_obj.save(path_to_save_image)
103/5:
import pdfplumber

pdf_obj = pdfplumber.open(doc_path)
page = pdf_obj.pages[page_no]
images_in_page = page.images
page_height = page.height
image = images_in_page[0] # assuming images_in_page has at least one element, only for understanding purpose. 
image_bbox = (image['x0'], page_height - image['y1'], image['x1'], page_height - image['y0'])
cropped_page = page.crop(image_bbox)
image_obj = cropped_page.to_image(resolution=400)
image_obj.save(path_to_save_image)
103/6:
import pdfplumber

pdf_obj = pdfplumber.open(doc_path)
page = pdf_obj.pages[page_no]
images_in_page = page.images
page_height = page.height
image = images_in_page[0] # assuming images_in_page has at least one element, only for understanding purpose. 
image_bbox = (image['x0'], page_height - image['y1'], image['x1'], page_height - image['y0'])
cropped_page = page.crop(image_bbox)
image_obj = cropped_page.to_image(resolution=400)
image_obj.save(path_to_save_image)
103/7:
import pdfplumber

doc_path = r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf'

pdf_obj = pdfplumber.open(doc_path)
page = pdf_obj.pages[page_no]
images_in_page = page.images
page_height = page.height
image = images_in_page[0] # assuming images_in_page has at least one element, only for understanding purpose. 
image_bbox = (image['x0'], page_height - image['y1'], image['x1'], page_height - image['y0'])
cropped_page = page.crop(image_bbox)
image_obj = cropped_page.to_image(resolution=400)
image_obj.save(path_to_save_image)
103/8:
import pdfplumber

doc_path = r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf'

pdf_obj = pdfplumber.open(doc_path)
page = pdf_obj.pages[:]
images_in_page = page.images
page_height = page.height
image = images_in_page[0] # assuming images_in_page has at least one element, only for understanding purpose. 
image_bbox = (image['x0'], page_height - image['y1'], image['x1'], page_height - image['y0'])
cropped_page = page.crop(image_bbox)
image_obj = cropped_page.to_image(resolution=400)
image_obj.save(path_to_save_image)
103/9:
# STEP 1
# import libraries
import fitz
import io
from PIL import Image

# STEP 2
# file path you want to extract images from
file = "r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf'"

# open the file
pdf_file = fitz.open(file)

# STEP 3
# iterate over PDF pages
for page_index in range(len(pdf_file)):
    
    # get the page itself
    page = pdf_file[page_index]
    image_list = page.getImageList()
    
    # printing number of images found in this page
    if image_list:
        print(f"[+] Found a total of {len(image_list)} images in page {page_index}")
    else:
        print("[!] No images found on page", page_index)
    for image_index, img in enumerate(page.getImageList(), start=1):
        
        # get the XREF of the image
        xref = img[0]
        
        # extract the image bytes
        base_image = pdf_file.extractImage(xref)
        image_bytes = base_image["image"]
        
        # get the image extension
        image_ext = base_image["ext"]
103/10:
# STEP 1
# import libraries
import fitz
import io
from PIL import Image

# STEP 2
# file path you want to extract images from
file = "rC:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf"

# open the file
pdf_file = fitz.open(file)

# STEP 3
# iterate over PDF pages
for page_index in range(len(pdf_file)):
    
    # get the page itself
    page = pdf_file[page_index]
    image_list = page.getImageList()
    
    # printing number of images found in this page
    if image_list:
        print(f"[+] Found a total of {len(image_list)} images in page {page_index}")
    else:
        print("[!] No images found on page", page_index)
    for image_index, img in enumerate(page.getImageList(), start=1):
        
        # get the XREF of the image
        xref = img[0]
        
        # extract the image bytes
        base_image = pdf_file.extractImage(xref)
        image_bytes = base_image["image"]
        
        # get the image extension
        image_ext = base_image["ext"]
103/11:
# STEP 1
# import libraries
import fitz
import io
from PIL import Image

# STEP 2
# file path you want to extract images from
file = r"C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf"

# open the file
pdf_file = fitz.open(file)

# STEP 3
# iterate over PDF pages
for page_index in range(len(pdf_file)):
    
    # get the page itself
    page = pdf_file[page_index]
    image_list = page.getImageList()
    
    # printing number of images found in this page
    if image_list:
        print(f"[+] Found a total of {len(image_list)} images in page {page_index}")
    else:
        print("[!] No images found on page", page_index)
    for image_index, img in enumerate(page.getImageList(), start=1):
        
        # get the XREF of the image
        xref = img[0]
        
        # extract the image bytes
        base_image = pdf_file.extractImage(xref)
        image_bytes = base_image["image"]
        
        # get the image extension
        image_ext = base_image["ext"]
104/1:
# STEP 1
# import libraries
import fitz
import io
from PIL import Image

# STEP 2
# file path you want to extract images from
file = r"C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf"

# open the file
pdf_file = fitz.open(file)

# STEP 3
# iterate over PDF pages
for page_index in range(len(pdf_file)):
    
    # get the page itself
    page = pdf_file[page_index]
    image_list = page.getImageList()
    
    # printing number of images found in this page
    if image_list:
        print(f"[+] Found a total of {len(image_list)} images in page {page_index}")
    else:
        print("[!] No images found on page", page_index)
    for image_index, img in enumerate(page.getImageList(), start=1):
        
        # get the XREF of the image
        xref = img[0]
        
        # extract the image bytes
        base_image = pdf_file.extractImage(xref)
        image_bytes = base_image["image"]
        
        # get the image extension
        image_ext = base_image["ext"]
104/2:
# STEP 1
# import libraries
import fitz
import io
from PIL import Image

# STEP 2
# file path you want to extract images from
file = r"C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf"

# open the file
pdf_file = fitz.open(file)

# STEP 3
# iterate over PDF pages
for page_index in range(len(pdf_file)):
    
    # get the page itself
    page = pdf_file[page_index]
    image_list = page_index[page].getImageList()
    
    # printing number of images found in this page
    if image_list:
        print(f"[+] Found a total of {len(image_list)} images in page {page_index}")
    else:
        print("[!] No images found on page", page_index)
    for image_index, img in enumerate(page.getImageList(), start=1):
        
        # get the XREF of the image
        xref = img[0]
        
        # extract the image bytes
        base_image = pdf_file.extractImage(xref)
        image_bytes = base_image["image"]
        
        # get the image extension
        image_ext = base_image["ext"]
104/3:
# STEP 1
# import libraries
import fitz
import io
from PIL import Image

# STEP 2
# file path you want to extract images from
file = r"C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf"

# open the file
pdf_file = fitz.open(file)

# STEP 3
# iterate over PDF pages
for page_index in range(len(pdf_file)):
    
    # get the page itself
    page = pdf_file[page_index]
    image_list = page.getImageList()
    
    # printing number of images found in this page
    if image_list:
        print(f"[+] Found a total of {len(image_list)} images in page {page_index}")
    else:
        print("[!] No images found on page", page_index)
    for image_index, img in enumerate(page.getImageList(), start=1):
        
        # get the XREF of the image
        xref = img[0]
        
        # extract the image bytes
        base_image = pdf_file.extractImage(xref)
        image_bytes = base_image["image"]
        
        # get the image extension
        image_ext = base_image["ext"]
104/4:
# STEP 1
# import libraries
import fitz
import io
from PIL import Image

# STEP 2
# file path you want to extract images from
file = r"C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf"

# open the file
pdf_file = fitz.open(file)

# STEP 3
# iterate over PDF pages
for page_index in range(len(pdf_file)):
    
    # get the page itself
    page = pdf_file[page_index]
    print(type(page))
    print(page)
    image_list = page.getImageList()
    
    # printing number of images found in this page
    if image_list:
        print(f"[+] Found a total of {len(image_list)} images in page {page_index}")
    else:
        print("[!] No images found on page", page_index)
    for image_index, img in enumerate(page.getImageList(), start=1):
        
        # get the XREF of the image
        xref = img[0]
        
        # extract the image bytes
        base_image = pdf_file.extractImage(xref)
        image_bytes = base_image["image"]
        
        # get the image extension
        image_ext = base_image["ext"]
104/6:
# STEP 1
# import libraries
import fitz
import io
from PIL import Image

# STEP 2
# file path you want to extract images from
file = r"C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf"

# open the file
pdf_file = fitz.open(file)

# STEP 3
# iterate over PDF pages
for page_index in range(len(pdf_file)):
    
    # get the page itself
    page = pdf_file[page_index]
    print(type(page))
    print(page)
    image_list = page.getImageList()

    # printing number of images found in this page
    if image_list:
        print(f"[+] Found a total of {len(image_list)} images in page {page_index}")
    else:
        print("[!] No images found on page", page_index)
    for image_index, img in enumerate(page.getImageList(), start=1):

        # get the XREF of the image
        xref = img[0]

        # extract the image bytes
        base_image = pdf_file.extractImage(xref)
        image_bytes = base_image["image"]

        # get the image extension
        image_ext = base_image["ext"]
104/7:
import PyPDF2
import re

#Assign the pdf file
file_name = "C:/Users/Dell/Downloads/MostlyText/CollegePanda-3tests.pdf"
doc = PyPDF2.PdfFileReader(file_name)

#Number of pages
pages = doc.getNumPages()

#Search term
search = "independent"

#List of tuples (all occurrences, page number)
list_pages = []

for i in range(pages):
    current_page = doc.getPage(i)
    text = current_page.extractText()
    if re.findall(search, text):
        #findall returns a list
        count_page = len(re.findall(search, text))
        list_pages.append((count_page, i))

#Number of pages that contain search term at least once
count = len(list_pages)

print(list_pages)

#Total word count
total = sum([tup[0] for tup in list_pages])
print(total)


print(f"The word '{search}' was found {total} times on {count} pages.")
105/1:
import PyPDF2
import re

#Assign the pdf file
file_name = "C:/Users/Dell/Downloads/MostlyText/CollegePanda-3tests.pdf"
doc = PyPDF2.PdfFileReader(file_name)

#Number of pages
pages = doc.getNumPages()

#Search term
search = "independent"

#List of tuples (all occurrences, page number)
list_pages = []

for i in range(pages):
    current_page = doc.getPage(i)
    text = current_page.extractText()
    if re.findall(search, text):
        #findall returns a list
        count_page = len(re.findall(search, text))
        list_pages.append((count_page, i))

#Number of pages that contain search term at least once
count = len(list_pages)

print(list_pages)

#Total word count
total = sum([tup[0] for tup in list_pages])
print(total)


print(f"The word '{search}' was found {total} times on {count} pages.")
105/2: text
105/3:
import PyPDF2
import re

#Assign the pdf file
file_name = "C:/Users/Dell/Downloads/MostlyText/CollegePanda-3tests.pdf"
doc = PyPDF2.PdfFileReader(file_name)

#Number of pages
pages = doc.getNumPages()

#Search term
search = "independent"

#List of tuples (all occurrences, page number)
list_pages = []

for i in range(pages):
    current_page = doc.getPage(i)
    text = current_page.extractText()
    print(text)
105/4:
import PyPDF2
import re

#Assign the pdf file
file_name = "C:/Users/Dell/Downloads/MostlyText/ComparativeGovernment-1test.pdf"
doc = PyPDF2.PdfFileReader(file_name)

#Number of pages
pages = doc.getNumPages()

#Search term
search = "independent"

#List of tuples (all occurrences, page number)
list_pages = []

for i in range(pages):
    current_page = doc.getPage(i)
    text = current_page.extractText()
    print(text)
105/5:
import PyPDF2

from PIL import Image

if __name__ == '__main__':
    input1 = PyPDF2.PdfFileReader(open("C:/Users/Dell/Downloads/MostlyText/ComparativeGovernment-1test.pdf", "rb"))
    page0 = input1.getPage(0)
    xObject = page0['/Resources']['/XObject'].getObject()

    for obj in xObject:
        if xObject[obj]['/Subtype'] == '/Image':
            size = (xObject[obj]['/Width'], xObject[obj]['/Height'])
            data = xObject[obj].getData()
            if xObject[obj]['/ColorSpace'] == '/DeviceRGB':
                mode = "RGB"
            else:
                mode = "P"

            if xObject[obj]['/Filter'] == '/FlateDecode':
                img = Image.frombytes(mode, size, data)
                img.save(obj[1:] + ".png")
            elif xObject[obj]['/Filter'] == '/DCTDecode':
                img = open(obj[1:] + ".jpg", "wb")
                img.write(data)
                img.close()
            elif xObject[obj]['/Filter'] == '/JPXDecode':
                img = open(obj[1:] + ".jp2", "wb")
                img.write(data)
                img.close()
105/6:
import PyPDF2

from PIL import Image

if __name__ == '__main__':
    input1 = PyPDF2.PdfFileReader(open("C:/Users/Dell/Downloads/MostlyText/ComparativeGovernment-1test.pdf", "rb"))
    page0 = input1.getPage(0)
    xObject = page0['/Resources']['/Object'].getObject()

    for obj in xObject:
        if xObject[obj]['/Subtype'] == '/Image':
            size = (xObject[obj]['/Width'], xObject[obj]['/Height'])
            data = xObject[obj].getData()
            if xObject[obj]['/ColorSpace'] == '/DeviceRGB':
                mode = "RGB"
            else:
                mode = "P"

            if xObject[obj]['/Filter'] == '/FlateDecode':
                img = Image.frombytes(mode, size, data)
                img.save(obj[1:] + ".png")
            elif xObject[obj]['/Filter'] == '/DCTDecode':
                img = open(obj[1:] + ".jpg", "wb")
                img.write(data)
                img.close()
            elif xObject[obj]['/Filter'] == '/JPXDecode':
                img = open(obj[1:] + ".jp2", "wb")
                img.write(data)
                img.close()
105/7:
from pdf2docx import Converter, parse

pdf_file = 'C:/Users/Dell/Downloads/MostlyText/ComparativeGovernment-1test.pdf'
word_file = 'demo.docx'

cv = Converter(pdf_file)
cv.convert(word_file, start=0, end=None)
cv.close()

parse(pdf_file, word_file, start=0, end=None)
105/8:
from pdf2docx import Converter, parse

pdf_file = 'C:/Users/Dell/Downloads/MostlyText/English-Test1.pdf'
word_file = 'demo.docx'

cv = Converter(pdf_file)
cv.convert(word_file, start=0, end=None)
cv.close()

parse(pdf_file, word_file, start=0, end=None)
106/1:
import fitz

file = fitz.open(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf')

for pageNumber, page in enumerate(file.pages(), start = 1):

    text = page.getText()

    txt = open(f'report_Page_{pageNumber}.txt', 'a')
    txt.writelines(text)
106/2:
import fitz

file = fitz.open(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf')

for pageNumber, page in enumerate(file.pages():

    text = page.getText()

    txt = open(f'report_Page_{pageNumber}.txt', 'a')
    txt.writelines(text)
106/3:
import fitz

file = fitz.open(r'C:\Users\Dell\Downloads\MostlyText\CollegePanda-3tests.pdf')

for pageNumber, page in enumerate(file.pages(), start = 1):

    text = page.getText()

    txt = open(f'report_Page_{pageNumber}.txt', 'a')
    txt.writelines(text)
106/4:
import win32com.client
import os

word = win32com.client.Dispatch("word.Application")
word.visible=0

doc_pdf = r"C:\Users\Dell\Downloads\MostlyText\English-Test1.pdf"
input_file = os.path.abspath(doc_pdf)

wb = word.Documents.Open(input_file)
output_file = os.path.abspath(doc_pdf[0:-4] +"docx".format())
wb.SaveAs2(output_file, FileFormat=16)
print("PDF to DOCx is complete")
wb.Close()
107/1:
import PyPDF2
import re

#Assign the pdf file
file_name = "C:/Users/Dell/Downloads/MostlyText/CollegePanda-3tests.pdf"
doc = PyPDF2.PdfFileReader(file_name)

#Number of pages
pages = doc.getNumPages()

#Search term
search = "independent"

#List of tuples (all occurrences, page number)
list_pages = []

for i in range(pages):
    current_page = doc.getPage(i)
    text = current_page.extractText()
    print(text)
107/2:
import PyPDF2
import re

#Assign the pdf file
file_name = "C:/Users/Dell/Downloads/MostlyText/Stats-review1.pdf"
doc = PyPDF2.PdfFileReader(file_name)

#Number of pages
pages = doc.getNumPages()

#List of tuples (all occurrences, page number)
list_pages = []

for i in range(pages):
    current_page = doc.getPage(i)
    text = current_page.extractText()
    print(text)
107/3: test
107/4: text
107/5:
if '1.' in text:
    print(text['1.'].index)
107/6:
if '1.' in text:
    print(text.index['1.'])
107/7:
if '1.' in text:
    print(text.index)
107/8:
if '1.' in text:
    print(text.index['1.'])
107/9:
if '1.' in text:
    print(text.capitalize)
107/10:
if '1.' in text:
    print(text.capitalize())
107/11: test
107/12: text
107/13:
import PyPDF2
import re

#Assign the pdf file
file_name = "C:/Users/Dell/Downloads/MostlyText/Stats-review1.pdf"
doc = PyPDF2.PdfFileReader(file_name)

#Number of pages
pages = doc.getNumPages()

#List of tuples (all occurrences, page number)
list_pages = []

for i in range(pages):
    current_page = doc.getPage(i)
    text = current_page.extractText()
    print(text)
107/14: text
107/15: text.capitalize()
107/16: text
107/17:
for i in text:
    print(i)
107/18:
import PyPDF2
import re

complete=[]

#Assign the pdf file
file_name = "C:/Users/Dell/Downloads/MostlyText/Stats-review1.pdf"
doc = PyPDF2.PdfFileReader(file_name)

#Number of pages
pages = doc.getNumPages()

#List of tuples (all occurrences, page number)
list_pages = []

for i in range(pages):
    current_page = doc.getPage(i)
    text = current_page.extractText()
    print(text)
    complete.append(text)
107/19: commplete
107/20: complete
107/21:
for i in complete:
    f = f + i
107/22:
f = ""
for i in complete:
    f = f + i
107/23: f
107/24:
f = ""
for i in complete:
    f = f + i + '\n'
107/25: f
107/26:
f = ""
for i in complete:
    f = f + i + ' '
107/27: f
107/28: f.replace('\\n','\n')
107/29: print('\\n')
107/30: print('\n')
107/31: f.replace('\\n','\n',-1)
107/32: f.replace('\n','\n',-1)
107/33: f.replace('\\n','\n',-1)
107/34:
a = 'st\nar'
a.replace('\\n','\n',-1)
107/35:
a = 'st\nar'
a. replace('nar','\n',-1)
107/36:
a = 'st\nar'
a.replace('nar','\n',-1)
107/37: complete
107/38:
f = ""
for i in complete:
    f = f + i + ' '
107/39: print(f)
107/40:
f = ""
for i in complete:
    f = f + i + '\n'
107/41: print(f)
107/42: complete.find('1.')
107/43: f.find('1.')
107/44: f[260:265]
107/45: f[262:267]
107/46: f[262:].find('D.')
107/47: f[262+181:262+181+5]
107/48: f.find('2.')
107/49: f[262:473]
107/50: print(f[262:473])
107/51:
count = 0
while True:
    count+=1
    print(count)
    if count == 8:
        break
107/52: a
107/53: a.find('\n')
107/54: print(a)
107/55: a
107/56: a.find('S')
107/57: a
107/58: a.find('S')
107/59: a[-1]
107/60: a.find('T')
107/61: a.find('trt')
107/62: a[-1]
107/63: a
107/64: a[-1]
107/65: a.find('S')
107/66: a.find('s')
107/67:
count = 0
while True:
    count+=1
    if f.find(str(count+1)+'.') != -1:
        print(f[f.find(str(count)+'.'):f.find(str(count+1)+'.')])
        f = f[f.find(str(count)+'.')-1:]
        
    elif f.find('E.') != -1:
        print(f[f.find(str(count)+'.'):f.find('E.')])
    
    elif f.find('D.') != -1:
        print(f[f.find(str(count)+'.'):f.find('D.')])
    
    elif f.find('C.') != -1:
        print(f[f.find(str(count)+'.'):f.find('D.')])
        
    else: 
        break
107/68: f[f.find('E.'):].find('\n')
107/69: f
107/70:
import PyPDF2
import re

complete=[]

#Assign the pdf file
file_name = "C:/Users/Dell/Downloads/MostlyText/Stats-review1.pdf"
doc = PyPDF2.PdfFileReader(file_name)

#Number of pages
pages = doc.getNumPages()

#List of tuples (all occurrences, page number)
list_pages = []

for i in range(pages):
    current_page = doc.getPage(i)
    text = current_page.extractText()
    print(text)
    complete.append(text)
107/71:
f = ""
for i in complete:
    f = f + i + '\n'
107/72: print(f)
107/73: f[f.find('E.'):].find('\n')
107/74: f[:16]
107/75: f.find('E.') + f[f.find('E.'):].find('\n')
107/76: f[450:472]
107/77: f[410:472]
107/78: f.find(str(count)+'.'):f.find(str(count)+'.')
107/79: f[f.find(str(count)+'.'):f.find(str(count)+'.')]
107/80: f[f.find(str(count)+'.'):f.find(str(count)+'.') + f[f.find('E.'):].find('\n')]
107/81: f[f.find(str(count)+'.'):f[f.find('E.'):].find('\n')]
107/82: f[f.find(str(1)+'.'):f.find(str(1)+'.') + f[f.find('E.'):].find('\n')]
107/83: f[f.find(str(1)+'.'):f.find(str(1)+'.') + f[f.find('E.'):]]
107/84: f[f.find(str(1)+'.'):f.find(str(1)+'.') + f.find('E.')]
107/85: print(f[f.find(str(1)+'.'):f.find(str(1)+'.') + f.find('E.')])
107/86: print(f[f.find(str(1)+'.'):f.find(str(1)+'.') + f[f.find('E.')].find('\n')])
107/87: print(f[f.find(str(1)+'.'):f.find(str(1)+'.') + f[f.find('E.'):].find('\n')])
107/88: print(f[f.find(str(1)+'.'):f[f.find('E.'):].find('\n')])
107/89: print(f[f.find(str(1)+'.'):f[f.find(str(1)+'.')+f.find('E.'):].find('\n')])
107/90: f[f.find(str(1)+'.')+f.find('E.'):].find('\n')
107/91: print(f[f.find(str(1)+'.'):f.find('E.'):])
107/92: print(f[f.find(str(1)+'.'):f.find('E.')])
107/93: f.find('E.')
107/94: f[:f.find('E.')]
107/95: print(f[:f.find('E.')])
107/96: print(f[:f.find('E.')+15])
107/97: print(f[:f.find('E.')+20])
107/98: print(f[:f.find('E.')+20.find('\n')])
107/99: print(f[:f[:f.find('E.')+20].find('\n')])
107/100: print(f[:f[f.find('E.')+20]:f.find('E.')+20].find('\n')])
107/101: print(f[f.find('E.')+20]:f[:f.find('E.')+20].find('\n')])
107/102: print(f[f.find('E.')+20]::f.find('E.')+20.find('\n')])
107/103: print(f[f.find('E.')+20]:f.find('E.')+20.find('\n')])
107/104: print(f[f.find('E.')+20:f.find('E.')+20.find('\n')])
107/105: print(f[f.find('E.'):f.find('E.')+20.find('\n')])
107/106: print(f[f.find('E.'):f.find('E.')+20])
108/1:
import PyPDF2
import re

complete=[]

#Assign the pdf file
file_name = "C:/Users/Dell/Downloads/MostlyText/Stats-review1.pdf"
doc = PyPDF2.PdfFileReader(file_name)

#Number of pages
pages = doc.getNumPages()

#List of tuples (all occurrences, page number)
list_pages = []

for i in range(pages):
    current_page = doc.getPage(i)
    text = current_page.extractText()
    print(text)
    complete.append(text)
108/2:
f = ""
for i in complete:
    f = f + i + '\n'
108/3: print(f)
108/4: f.find('E.')
108/5: f[f.find('E.'):].find['\n']
108/6:
abc = f.find('E.')
f[:].find['\n']
108/7:
abc = f.find('E.')
f[abc:].find['\n']
108/8:
abc = f.find('E.')
f[abc:]#.find['\n']
108/9:
abc = f.find('E.')
f[abc:].find('\n')
108/10: f[f.find('E.'):].find('\n')
108/11:
ends = f[f.find('E.'):].find('\n')
f[f.find(str(1)+'.'):ends]
108/12:
ends = f.find('E.') + f[f.find('E.'):].find('\n')
f[f.find(str(1)+'.'):ends]
108/13:
count = 0
while True:
    count+=1
    if f.find(str(count+1)+'.') != -1:
        if f.find('E.') != -1:
            ends = f.find('E.') + f[f.find('E.'):].find('\n')
            
        elif f.find('D.') != -1:
            ends = f.find('D.') + f[f.find('D.'):].find('\n')
            
        else:
            ends = f.find('C.') + f[f.find('C.'):].find('\n')
            
        print(f[f.find(str(count)+'.'):ends])
        f = f[f.find(str(count)+'.')-1:]
        
    elif f.find('E.') != -1:
        print(f[f.find(str(count)+'.'):f.find('E.')])
    
    elif f.find('D.') != -1:
        print(f[f.find(str(count)+'.'):f.find('D.')])
    
    elif f.find('C.') != -1:
        print(f[f.find(str(count)+'.'):f.find('D.')])
        
    else: 
        break
108/14:
f = ""
for i in complete:
    f = f + i + '\n'
108/15: print(f)
108/16:
count = 0
while True:
    count+=1
    if f.find(str(count+1)+'.') != -1:
        if f.find('E.') != -1:
            ends = f.find('E.') + f[f.find('E.'):].find('\n')
            
            print(f[f.find(str(count)+'.'):ends])
            f = f[ends:]
            
        elif f.find('D.') != -1:
            ends = f.find('D.') + f[f.find('D.'):].find('\n')
            
            print(f[f.find(str(count)+'.'):ends])
            f = f[ends:]
            
        else:
            ends = f.find('C.') + f[f.find('C.'):].find('\n')
            
            print(f[f.find(str(count)+'.'):ends])
            f = f[ends:]
            
        
        
    elif f.find('E.') != -1:
        print(f[f.find(str(count)+'.'):f.find('E.')])
    
    elif f.find('D.') != -1:
        print(f[f.find(str(count)+'.'):f.find('D.')])
    
    elif f.find('C.') != -1:
        print(f[f.find(str(count)+'.'):f.find('C.')])
        
    else: 
        break
108/17:
f = ""
for i in complete:
    f = f + i + '\n'
108/18: print(f)
108/19:
count = 0
while True:
    count+=1
    if f.find(str(count+1)+'.') != -1:
        if f.find('E.') != -1:
            ends = f.find('E.') + f[f.find('E.'):].find('\n')
            
            print(f[f.find(str(count)+'.'):ends]+'\n')
            f = f[ends:]
            
        elif f.find('D.') != -1:
            ends = f.find('D.') + f[f.find('D.'):].find('\n')
            
            print(f[f.find(str(count)+'.'):ends]+'\n')
            f = f[ends:]
            
        else:
            ends = f.find('C.') + f[f.find('C.'):].find('\n')
            
            print(f[f.find(str(count)+'.'):ends]+'\n')
            f = f[ends:]
            
        
        
    elif f.find('E.') != -1:
        print(f[f.find(str(count)+'.'):f.find('E.')])
    
    elif f.find('D.') != -1:
        print(f[f.find(str(count)+'.'):f.find('D.')])
    
    elif f.find('C.') != -1:
        print(f[f.find(str(count)+'.'):f.find('C.')])
        
    else: 
        break
108/20:
f = ""
for i in complete:
    f = f + i + '\n'
108/21: print(f)
108/22:
ends = f.find('E.') + f[f.find('E.'):].find('\n')
f[f.find(str(13)+'.'):ends]
108/23:
ends = f[2000:].find('E.') + f[f.find('E.'):].find('\n')
f[f.find(str(13)+'.'):ends]
108/24:
f = ""
for i in complete:
    f = f + i + '\n'
108/25: print(f)
108/26:
count = 0
while True:
    count+=1
    if f.find(str(count+1)+'.') != -1:
        if f.find('E.') != -1:
            ends = f.find('E.') + f[f.find('E.'):].find('\n')
            
            print(f[f.find(str(count)+'.'):ends]+'\n')
            f = f[ends:]
            
        elif f.find('D.') != -1:
            ends = f.find('D.') + f[f.find('D.'):].find('\n')
            
            print(f[f.find(str(count)+'.'):ends]+'\n')
            f = f[ends:]
            
        else:
            ends = f.find('C.') + f[f.find('C.'):].find('\n')
            
            print(f[f.find(str(count)+'.'):ends]+'\n')
            f = f[ends:]
            
        
        
#     elif f.find('E.') != -1:
#         print(f[f.find(str(count)+'.'):f.find('E.')])
    
#     elif f.find('D.') != -1:
#         print(f[f.find(str(count)+'.'):f.find('D.')])
    
#     elif f.find('C.') != -1:
#         print(f[f.find(str(count)+'.'):f.find('C.')])
        
#     else: 
#         break
109/1:
import PyPDF2
import re

complete=[]

#Assign the pdf file
file_name = "C:/Users/Dell/Downloads/MostlyText/Stats-review1.pdf"
doc = PyPDF2.PdfFileReader(file_name)

#Number of pages
pages = doc.getNumPages()

#List of tuples (all occurrences, page number)
list_pages = []

for i in range(pages):
    current_page = doc.getPage(i)
    text = current_page.extractText()
    print(text)
    complete.append(text)
109/2:
f = ""
for i in complete:
    f = f + i + '\n'
109/3: print(f)
109/4:
count = 0
while True:
    count+=1
    if f.find(str(count+1)+'.') != -1:
        if f.find('E.') != -1:
            ends = f.find('E.') + f[f.find('E.'):].find('\n')
            
            print(f[f.find(str(count)+'.'):ends]+'\n')
            f = f[ends:]
            
        elif f.find('D.') != -1:
            ends = f.find('D.') + f[f.find('D.'):].find('\n')
            
            print(f[f.find(str(count)+'.'):ends]+'\n')
            f = f[ends:]
            
        else:
            ends = f.find('C.') + f[f.find('C.'):].find('\n')
            
            print(f[f.find(str(count)+'.'):ends]+'\n')
            f = f[ends:]
            
        
        
#     elif f.find('E.') != -1:
#         print(f[f.find(str(count)+'.'):f.find('E.')])
    
#     elif f.find('D.') != -1:
#         print(f[f.find(str(count)+'.'):f.find('D.')])
    
#     elif f.find('C.') != -1:
#         print(f[f.find(str(count)+'.'):f.find('C.')])
        
    else: 
        break
109/5:
count = 0
while True:
    count+=1
    
    if f.find('E.') != -1:
        ends = f.find('E.') + f[f.find('E.'):].find('\n')

        print(f[f.find(str(count)+'.'):ends]+'\n')
        f = f[ends:]

    elif f.find('D.') != -1:
        ends = f.find('D.') + f[f.find('D.'):].find('\n')

        print(f[f.find(str(count)+'.'):ends]+'\n')
        f = f[ends:]

    else:
        ends = f.find('C.') + f[f.find('C.'):].find('\n')

        print(f[f.find(str(count)+'.'):ends]+'\n')
        f = f[ends:]



# elif f.find('E.') != -1:
#     print(f[f.find(str(count)+'.'):f.find('E.')])

# elif f.find('D.') != -1:
#     print(f[f.find(str(count)+'.'):f.find('D.')])

# elif f.find('C.') != -1:
#     print(f[f.find(str(count)+'.'):f.find('C.')])

    else: 
        break
109/6:
count = 0
while True:
    count+=1
    
    if f.find('E.') != -1:
        ends = f.find('E.') + f[f.find('E.'):].find('\n')

        print(f[f.find(str(count)+'.'):ends]+'\n')
        f = f[ends:]

    elif f.find('D.') != -1:
        ends = f.find('D.') + f[f.find('D.'):].find('\n')

        print(f[f.find(str(count)+'.'):ends]+'\n')
        f = f[ends:]

    elif f.find('C.') != -1::
        ends = f.find('C.') + f[f.find('C.'):].find('\n')

        print(f[f.find(str(count)+'.'):ends]+'\n')
        f = f[ends:]
        
    else: 
        break
    



# elif f.find('E.') != -1:
#     print(f[f.find(str(count)+'.'):f.find('E.')])

# elif f.find('D.') != -1:
#     print(f[f.find(str(count)+'.'):f.find('D.')])

# elif f.find('C.') != -1:
#     print(f[f.find(str(count)+'.'):f.find('C.')])
109/7:
count = 0
while True:
    count+=1
    
    if f.find('E.') != -1:
        ends = f.find('E.') + f[f.find('E.'):].find('\n')

        print(f[f.find(str(count)+'.'):ends]+'\n')
        f = f[ends:]

    elif f.find('D.') != -1:
        ends = f.find('D.') + f[f.find('D.'):].find('\n')

        print(f[f.find(str(count)+'.'):ends]+'\n')
        f = f[ends:]

    elif f.find('C.') != -1:
        ends = f.find('C.') + f[f.find('C.'):].find('\n')

        print(f[f.find(str(count)+'.'):ends]+'\n')
        f = f[ends:]
        
    else: 
        break
    



# elif f.find('E.') != -1:
#     print(f[f.find(str(count)+'.'):f.find('E.')])

# elif f.find('D.') != -1:
#     print(f[f.find(str(count)+'.'):f.find('D.')])

# elif f.find('C.') != -1:
#     print(f[f.find(str(count)+'.'):f.find('C.')])
109/8:
f = ""
for i in complete:
    f = f + i + '\n'
109/9: print(f)
109/10:
count = 0
while True:
    count+=1
    
    if f.find('E.') != -1:
        ends = f.find('E.') + f[f.find('E.'):].find('\n')

        print(f[f.find(str(count)+'.'):ends]+'\n')
        f = f[ends:]

    elif f.find('D.') != -1:
        ends = f.find('D.') + f[f.find('D.'):].find('\n')

        print(f[f.find(str(count)+'.'):ends]+'\n')
        f = f[ends:]

    elif f.find('C.') != -1:
        ends = f.find('C.') + f[f.find('C.'):].find('\n')

        print(f[f.find(str(count)+'.'):ends]+'\n')
        f = f[ends:]
        
    else: 
        break
    



# elif f.find('E.') != -1:
#     print(f[f.find(str(count)+'.'):f.find('E.')])

# elif f.find('D.') != -1:
#     print(f[f.find(str(count)+'.'):f.find('D.')])

# elif f.find('C.') != -1:
#     print(f[f.find(str(count)+'.'):f.find('C.')])
109/11: abc
109/12:
abc = ['A','B','C']
print(abc[*])
109/13:
a,b,c = [],[],[]

if f.find('A.') in f:
    a.append(True)

if f.find('(A)') in f:
    b.append(True)

if f.find'A)' in f:
    c.append(True)

max(len(a),len(b),len(c))
109/14:
a,b,c = [],[],[]

if f.find('A.') in f:
    a.append(True)

if f.find('(A)') in f:
    b.append(True)

if f.find('A)') in f:
    c.append(True)

max(len(a),len(b),len(c))
109/15:
import PyPDF2
import re

complete=[]

#Assign the pdf file
file_name = "C:/Users/Dell/Downloads/MostlyText/Stats-review1.pdf"
doc = PyPDF2.PdfFileReader(file_name)

#Number of pages
pages = doc.getNumPages()

#List of tuples (all occurrences, page number)
list_pages = []

for i in range(pages):
    current_page = doc.getPage(i)
    text = current_page.extractText()
    print(text)
    complete.append(text)
109/16:
f = ""
for i in complete:
    f = f + i + '\n'
109/17: print(f)
109/18:
count = 0
output = ""
while True:
    count+=1
    
    if f.find('E.') != -1:
        ends = f.find('E.') + f[f.find('E.'):].find('\n')
        
        output = output + f[f.find(str(count)+'.'):ends]+'\n'
        f = f[ends:]

    elif f.find('D.') != -1:
        ends = f.find('D.') + f[f.find('D.'):].find('\n')

        output = output + f[f.find(str(count)+'.'):ends]+'\n'
        f = f[ends:]

    elif f.find('C.') != -1:
        ends = f.find('C.') + f[f.find('C.'):].find('\n')

        output = output + f[f.find(str(count)+'.'):ends]+'\n'
        f = f[ends:]
        
    else: 
        break

print(output.strip())
109/19:
import PyPDF2
import re

complete=[]

#Assign the pdf file
file_name = "C:/Users/Dell/Downloads/MostlyText/Stats-review1.pdf"
doc = PyPDF2.PdfFileReader(file_name)

#Number of pages
pages = doc.getNumPages()

#List of tuples (all occurrences, page number)
list_pages = []

for i in range(pages):
    current_page = doc.getPage(i)
    text = current_page.extractText()
    print(text)
    complete.append(text)
109/20:
f = ""
for i in complete:
    f = f + i + '\n'
109/21: print(f)
109/22:
count = 0
output = ""
while True:
    count+=1
    
    if f.find('E.') != -1:
        ends = f.find('E.') + f[f.find('E.'):].find('\n')
        
        output = output + f[f.find(str(count)+'.'):ends]+'\n'
        f = f[ends:]

    elif f.find('D.') != -1:
        ends = f.find('D.') + f[f.find('D.'):].find('\n')

        output = output + f[f.find(str(count)+'.'):ends]+'\n'
        f = f[ends:]

    elif f.find('C.') != -1:
        ends = f.find('C.') + f[f.find('C.'):].find('\n')

        output = output + f[f.find(str(count)+'.'):ends]+'\n'
        f = f[ends:]
        
    else: 
        break

print(output.rstrip())
109/23:
import PyPDF2
import re

complete=[]

#Assign the pdf file
file_name = "C:/Users/Dell/Downloads/MostlyText/Stats-review1.pdf"
doc = PyPDF2.PdfFileReader(file_name)

#Number of pages
pages = doc.getNumPages()

#List of tuples (all occurrences, page number)
list_pages = []

for i in range(pages):
    current_page = doc.getPage(i)
    text = current_page.extractText()
    print(text)
    complete.append(text)
109/24:
f = ""
for i in complete:
    f = f + i + '\n'
109/25: print(f)
109/26:
count = 0
output = ""
while True:
    count+=1
    
    if f.find('E.') != -1:
        ends = f.find('E.') + f[f.find('E.'):].find('\n')
        
        output = output + f[f.find(str(count)+'.'):ends]+'\n\n'
        f = f[ends:]

    elif f.find('D.') != -1:
        ends = f.find('D.') + f[f.find('D.'):].find('\n')

        output = output + f[f.find(str(count)+'.'):ends]+'\n\n'
        f = f[ends:]

    elif f.find('C.') != -1:
        ends = f.find('C.') + f[f.find('C.'):].find('\n')

        output = output + f[f.find(str(count)+'.'):ends]+'\n\n'
        f = f[ends:]
        
    else: 
        break

print(output)
109/27:
import PyPDF2
import re

complete=[]

#Assign the pdf file
file_name = "C:/Users/Dell/Downloads/MostlyText/Stats-review1.pdf"
doc = PyPDF2.PdfFileReader(file_name)

#Number of pages
pages = doc.getNumPages()

#List of tuples (all occurrences, page number)
list_pages = []

for i in range(pages):
    current_page = doc.getPage(i)
    text = current_page.extractText()
    print(text)
    complete.append(text)
109/28:
f = ""
for i in complete:
    f = f + i + '\n'
109/29: print(f)
109/30:
count = 0
output = ""
while True:
    count+=1
    
    if f.find('E.') != -1:
        ends = f.find('E.') + f[f.find('E.'):].find('\n')
        
        output = output + f[f.find(str(count)+'.'):ends]+'\n\n'
        f = f[ends:]

    elif f.find('D.') != -1:
        ends = f.find('D.') + f[f.find('D.'):].find('\n')

        output = output + f[f.find(str(count)+'.'):ends]+'\n\n'
        f = f[ends:]

    elif f.find('C.') != -1:
        ends = f.find('C.') + f[f.find('C.'):].find('\n')

        output = output + f[f.find(str(count)+'.'):ends]+'\n\n'
        f = f[ends:]
        
    else: 
        break

print(output.rstrip())
110/1:
data=pd.read_csv('cause_failure.csv')
data.head(3)
110/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
110/3:
data=pd.read_csv('cause_failure.csv')
data.head(3)
110/4:
plt.scatter(data['Label'],df['Throughput'],color='red')
plt.title("Throughput Vs P1")
plt.xlabel("Throughput")
plt.xlabel("P1")
plt.grid()
plt.show()
110/5:
plt.scatter(data['Label'],data['Throughput'],color='red')
plt.title("Throughput Vs P1")
plt.xlabel("Throughput")
plt.xlabel("P1")
plt.grid()
plt.show()
110/6: data.shape
110/7:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# df = np.array(data).reshape(-1, 1)
# df
Y = data['Label'] 
X = data['Throughput']
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
reg=linear_model.LinearRegression()
reg.fit(x_train,y_train)
110/8:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

df = np.array(data).reshape(-1, 1)
df
# Y = data['Label'] 
# X = data['Throughput']
 
# x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
# reg=linear_model.LinearRegression()
# reg.fit(x_train,y_train)
110/9: df[::2]
110/10: df[::2].shape
110/11: df[1::2].shape
110/12:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

df = np.array(data).reshape(-1, 1)
df
Y = df[::2]
X = df[1::2]
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
reg=linear_model.LinearRegression()
reg.fit(x_train,y_train)
110/13:
data=pd.read_csv('cause_failure2.csv')
data.head(3)
110/14:
plt.scatter(data['Label'],data['Throughput'],color='red')
plt.title("Throughput Vs P1")
plt.xlabel("Throughput")
plt.xlabel("P1")
plt.grid()
plt.show()
110/15: data.shape
110/16:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

df = np.array(data).reshape(-1, 1)
df
Y = df[::2]
X = df[1::2]
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
reg=linear_model.LinearRegression()
reg.fit(x_train,y_train)
110/17: x_test = np.arange(95,1200)
110/18:
x_test = np.arange(95,1200)
x_test[:5]
110/19:
import random.random
x = random.randrange(5)
x
# x_test = np.arange(95,1200)
# x_test[:5]
110/20:
import random
x = random.randrange(5)
x
# x_test = np.arange(95,1200)
# x_test[:5]
110/21:
import random
x = random.randrange(95,1200)
x
# x_test = np.arange(95,1200)
# x_test[:5]
110/22:
import random
x = random.randrange(95.0453,1200.316)
x
# x_test = np.arange(95,1200)
# x_test[:5]
110/23:
import random
x = random.randrange(95.0453,1200)
x
# x_test = np.arange(95,1200)
# x_test[:5]
110/24:
import random
# x = random.randrange(95.0453,1200.316)
# x
deck = list(range(1, 53))

hand = random.sample(deck, k=5)
print(hand)
# x_test = np.arange(95,1200)
# x_test[:5]
110/25:
import random
# x = random.randrange(95.0453,1200.316)
# x
deck = list(range(95.0453,1200.316))

hand = random.sample(deck, k=5)
print(hand)
# x_test = np.arange(95,1200)
# x_test[:5]
110/26:
import random
# x = random.randrange(95.0453,1200.316)
# x

# deck = list(range(95.0453,1200.316))

# hand = random.sample(deck, k=5)
# print(hand)

sampl = np.random.uniform(low=0.5, high=13.3, size=(50,))
sampl

# x_test = np.arange(95,1200)
# x_test[:5]
110/27:
import random
# x = random.randrange(95.0453,1200.316)
# x

# deck = list(range(95.0453,1200.316))

# hand = random.sample(deck, k=5)
# print(hand)

sampl = np.random.uniform(95.0453,1200.316, size=(50,))
sampl

# x_test = np.arange(95,1200)
# x_test[:5]
110/28:
import random
# x = random.randrange(95.0453,1200.316)
# x

# deck = list(range(95.0453,1200.316))

# hand = random.sample(deck, k=5)
# print(hand)

sampl = np.random.uniform(95.0453,1200.316, size=(50,))
sampl


# x_test = np.arange(95,1200)
# x_test[:5]
110/29:
x_test = [ 247.69970629,  648.71435716,  328.29026744,  945.62146567,
       1148.16948737, 1141.35179249,  586.72920291,  897.50484664,
        733.41407988,  455.82009653,  549.18418089,  871.84601907,
        737.19681844,  149.11665999,  746.58963087,  688.61182386,
       1189.84300368,  846.81465216, 1040.33193473, 1087.01460512,
        504.15180304,  826.76220948, 1121.0965317 ,  265.90214761,
        375.9336458 ,  764.46045202,  489.71756458,  284.37665675,
       1073.65635896,  165.11835294,  958.23747072,  399.91181708,
        228.31997667,  850.17914004,  379.92248405, 1169.61623374,
         98.40117972,  159.8788348 ,  461.4870474 ,  959.29319116,
       1022.04269564,  588.62671648,  935.73412816,  151.83013884,
       1184.67222982,  129.32366801,  434.40619415,  577.87760656,
        869.6373627 ,  507.93049207]
x_test
110/30:
x_test = [ 247.69970629,  648.71435716,  328.29026744,  945.62146567,
       1148.16948737, 1141.35179249,  586.72920291,  897.50484664,
        733.41407988,  455.82009653,  549.18418089,  871.84601907,
        737.19681844,  149.11665999,  746.58963087,  688.61182386,
       1189.84300368,  846.81465216, 1040.33193473, 1087.01460512,
        504.15180304,  826.76220948, 1121.0965317 ,  265.90214761,
        375.9336458 ,  764.46045202,  489.71756458,  284.37665675,
       1073.65635896,  165.11835294,  958.23747072,  399.91181708,
        228.31997667,  850.17914004,  379.92248405, 1169.61623374,
         98.40117972,  159.8788348 ,  461.4870474 ,  959.29319116,
       1022.04269564,  588.62671648,  935.73412816,  151.83013884,
       1184.67222982,  129.32366801,  434.40619415,  577.87760656,
        869.6373627 ,  507.93049207]
110/31:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
df
110/32:
x_test = [ 247.69970629,  648.71435716,  328.29026744,  945.62146567,
       1148.16948737, 1141.35179249,  586.72920291,  897.50484664,
        733.41407988,  455.82009653,  549.18418089,  871.84601907,
        737.19681844,  149.11665999,  746.58963087,  688.61182386,
       1189.84300368,  846.81465216, 1040.33193473, 1087.01460512,
        504.15180304,  826.76220948, 1121.0965317 ,  265.90214761,
        375.9336458 ,  764.46045202,  489.71756458,  284.37665675,
       1073.65635896,  165.11835294,  958.23747072,  399.91181708,
        228.31997667,  850.17914004,  379.92248405, 1169.61623374,
         98.40117972,  159.8788348 ,  461.4870474 ,  959.29319116,
       1022.04269564,  588.62671648,  935.73412816,  151.83013884,
       1184.67222982,  129.32366801,  434.40619415,  577.87760656,
        869.6373627 ,  507.93049207]
x_test = np.array(x_test).reshape(-1, 1)
110/33:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
df
110/34:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
df[0]*6
110/35:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
df[0]
110/36:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
df[0:1]
110/37:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
df['Predicted'][0]
110/38:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
df['Predicted'][0]/1000000
110/39:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
df['Predicted'][0]/10000000
110/40:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
df['Predicted'][1]/10000000
110/41:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
df['Predicted'][0]/10000000
110/42:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
df['Predicted'][8]/10000000
110/43:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
df['Predicted'][2]/10000000
110/44:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})
df['Predicted']/10000000
110/45:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})
round(df['Predicted']/10000000,1)
110/46:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})
df['outputs'] = round(df['Predicted']/10000000,1)
110/47: df
110/48: df.drop('outputs')
110/49: df.drop(['outputs'])
110/50: df.drop(['outputs'],axis=1)
110/51: df.drop(['outputs'],axis=1,inplace=True)
110/52:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})
df['random'] = x_test
df['outputs'] = round(df['Predicted']/10000000,1)
110/53: df
110/54:
data=pd.read_csv('cause_failure2.csv')
data.head(3)
110/55:
plt.scatter(data['Label'],data['Throughput'],color='red')
plt.title("Throughput Vs P1")
plt.xlabel("Throughput")
plt.xlabel("P1")
plt.grid()
plt.show()
110/56: data.shape
110/57:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

df = np.array(data).reshape(-1, 1)
df
Y = df[::2]
X = df[1::2]
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
reg=linear_model.LinearRegression()
reg.fit(x_train,y_train)
110/58:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
# df['random'] = x_test
# df['outputs'] = round(df['Predicted']/10000000,1)
110/59: df
110/60:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
110/61:
data=pd.read_csv('cause_failure2.csv')
data.head(3)
110/62:
data=pd.read_csv('cause_failure2.csv')
data.head(3)
110/63:
plt.scatter(data['Label'],data['Throughput'],color='red')
plt.title("Throughput Vs P1")
plt.xlabel("Throughput")
plt.xlabel("P1")
plt.grid()
plt.show()
110/64: data.shape
110/65:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

df = np.array(data).reshape(-1, 1)
df
Y = df[::2]
X = df[1::2]
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
reg=linear_model.LinearRegression()
reg.fit(x_train,y_train)
110/66:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
# df['random'] = x_test
# df['outputs'] = round(df['Predicted']/10000000,1)
110/67: df
110/68:
data=pd.read_csv('cause_failure2.csv')
data.tail(3)
110/69:
data=pd.read_csv('cause_failure2.csv')
data.tail(10)
110/70:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

df = np.array(data).reshape(-1, 1)
df
Y = df[1]
X = df[:-1]
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
reg=linear_model.LinearRegression()
reg.fit(x_train,y_train)
110/71:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

df = np.array(data).reshape(-1, 1)
df
Y = df[1]
X = df[0]
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
reg=linear_model.LinearRegression()
reg.fit(x_train,y_train)
110/72:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# df = np.array(data).reshape(-1, 1)
# df
Y = df['Label']
X = df['Throughput']
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
reg=linear_model.LinearRegression()
reg.fit(x_train,y_train)
110/73:
df=pd.read_csv('cause_failure2.csv')
df.tail(10)
110/74:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# df = np.array(data).reshape(-1, 1)
# df
Y = df['Label']
X = df['Throughput']
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
reg=linear_model.LinearRegression()
reg.fit(x_train,y_train)
110/75:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# df = np.array(data).reshape(-1, 1)
# df
Y = df['Label']
X = df['Throughput']
110/76: x_train,x_test,y_train,y_test=train_test_split(X,Y, train_size=0.8,  shuffle=False)
110/77:
reg=linear_model.LinearRegression()
reg.fit(x_train,y_train)
110/78: data.shape
110/79: data.head()
110/80:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

X = data.iloc[:, 0].values
Y = data.iloc[:, 1].values
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, test_size=0.832,  shuffle=False)
reg=linear_model.LinearRegression()
x_train= x_train.reshape(-1, 1)
x_test = x_test.reshape(-1, 1)
reg.fit(x_train,y_train)
110/81:
  

# prediction
y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
df
110/82:
  

# prediction
# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})
# df
x_test.head()
110/83:
x_test = [ 247.69970629,  648.71435716,  328.29026744,  945.62146567,
       1148.16948737, 1141.35179249,  586.72920291,  897.50484664,
        733.41407988,  455.82009653,  549.18418089,  871.84601907,
        737.19681844,  149.11665999,  746.58963087,  688.61182386,
       1189.84300368,  846.81465216, 1040.33193473, 1087.01460512,
        504.15180304,  826.76220948, 1121.0965317 ,  265.90214761,
        375.9336458 ,  764.46045202,  489.71756458,  284.37665675,
       1073.65635896,  165.11835294,  958.23747072,  399.91181708,
        228.31997667,  850.17914004,  379.92248405, 1169.61623374,
         98.40117972,  159.8788348 ,  461.4870474 ,  959.29319116,
       1022.04269564,  588.62671648,  935.73412816,  151.83013884,
       1184.67222982,  129.32366801,  434.40619415,  577.87760656,
        869.6373627 ,  507.93049207]
x_test = np.array(x_test).reshape(-1, 1)
110/84:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
df
# df['random'] = x_test
# df['outputs'] = round(df['Predicted']/10000000,1)
110/85:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})

df['random'] = x_test
df
# df['outputs'] = round(df['Predicted']/10000000,1)
110/86:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
110/87:
df=pd.read_csv('cause_failure3.csv')
df.tail(10)
110/88:
plt.scatter(data['Label'],data['Throughput'],color='red')
plt.title("Throughput Vs P1")
plt.xlabel("Throughput")
plt.xlabel("P1")
plt.grid()
plt.show()
110/89:
data=pd.read_csv('cause_failure3.csv')
data.tail(10)
110/90:
plt.scatter(data['Label'],data['Throughput'],color='red')
plt.title("Throughput Vs P1")
plt.xlabel("Throughput")
plt.xlabel("P1")
plt.grid()
plt.show()
110/91:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

X = data.iloc[:, 0].values
Y = data.iloc[:, 1].values
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, test_size=0.832,  shuffle=False)
reg=linear_model.LinearRegression()
x_train= x_train.reshape(-1, 1)
x_test = x_test.reshape(-1, 1)
reg.fit(x_train,y_train)
110/92:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})

# df['random'] = x_test
df
# df['outputs'] = round(df['Predicted']/10000000,1)
110/93:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

df['random'] = x_test
df
# df['outputs'] = round(df['Predicted']/10000000,1)
110/94:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

# df['random'] = x_test
df[df['random']<0.9]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/95:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

# df['random'] = x_test
df[df['random']<0.9]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/96:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

# df['random'] = x_test
df[df['random']<0.4]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/97:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

# df['random'] = x_test
df[df['random']<1.2]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/98:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

# df['random'] = x_test
df[df['random']]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/99:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

# df['random'] = x_test
# df[df['random']]
df
# df['outputs'] = round(df['Predicted']/10000000,1)
110/100:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

# df['random'] = x_test
df[df['random']<1200]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/101:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

# df['random'] = x_test
df[df['random']<1199]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/102:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

# df['random'] = x_test
df[df['random']>1199]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/103:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

# df['random'] = x_test
df[df['random']<1200]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/104:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

# df['random'] = x_test
df[df['Predicted']>0.5]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/105:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

# df['random'] = x_test
df[df['Predicted']<1200]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/106:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

# df['random'] = x_test
df[df['Predicted']>1200]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/107:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

# df['random'] = x_test
df[df['random']>1200]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/108:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

# df['random'] = x_test
df[df['random']<1200]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/109: df.drop(['random'],axis=1)
110/110: df.drop(['random'],axis=1,inplace=True)
110/111:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

df['test values'] = x_test
df[df['random']<1200]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/112:


# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.flatten()})

df['test values'] = x_test
df[df['test values']<1200]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/113:
#Round

y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})

df['test values'] = x_test
df
# df['outputs'] = round(df['Predicted']/10000000,1)
110/114:
#Round

y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.round()})

df['test values'] = x_test
df
# df['outputs'] = round(df['Predicted']/10000000,1)
110/115:
#Round

# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.round()})

# df['test values'] = x_test
df[df['Predicted']<1.0]
110/116:
#Round

# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.round()})

# df['test values'] = x_test
df[df['Predicted']<2.0]
110/117:
#Round

# y_pred=reg.predict(x_test)
# df = pd.DataFrame({'Predicted': y_pred.round()})

# df['test values'] = x_test
df[df['Predicted']<1.0]
110/118:
data=pd.read_csv('cause_failure3.csv')
data.tail(10)
110/119:
data=pd.read_csv('cause_failure2.csv')
data.tail(10)
110/120:
plt.scatter(data['Label'],data['Throughput'],color='red')
plt.title("Throughput Vs P1")
plt.xlabel("Throughput")
plt.xlabel("P1")
plt.grid()
plt.show()
110/121:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

X = data.iloc[:, 0].values
Y = data.iloc[:, 1].values
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, test_size=0.1)
reg=linear_model.LinearRegression()
x_train= x_train.reshape(-1, 1)
x_test = x_test.reshape(-1, 1)
reg.fit(x_train,y_train)
110/122:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})

df['test values'] = x_test
# df[df['test values']<1200]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/123:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})

# df['test values'] = x_test
# df[df['test values']<1200]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/124:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
df
# df['test values'] = x_test
# df[df['test values']<1200]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/125:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
df
# df['test values'] = x_test
# df[df['test values']<1200]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/126:


y_pred=reg.predict(x_test)
df = pd.DataFrame({'Predicted': y_pred.flatten()})
df['test values'] = x_test
df
# 
# df[df['test values']<1200]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/127:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
110/128:
data=pd.read_csv('cause_failure2.csv')
data.tail(10)
110/129:
plt.scatter(data['Label'],data['Throughput'],color='red')
plt.title("Throughput Vs Label")
plt.xlabel("Throughput")
plt.xlabel("P1")
plt.grid()
plt.show()
110/130:
plt.scatter(data['Label'],data['Throughput'],color='red')
plt.title("Throughput Vs Label")
plt.xlabel("Throughput")
plt.xlabel("Label")
plt.grid()
plt.show()
110/131:
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

X = data.iloc[:, 0].values
Y = data.iloc[:, 1].values
 
x_train,x_test,y_train,y_test=train_test_split(X,Y, test_size=0.5, shuffle=False)
reg=linear_model.LinearRegression()
x_train= x_train.reshape(-1, 1)
x_test = x_test.reshape(-1, 1)
reg.fit(x_test,y_test)
110/132:


y_pred=reg.predict(x_ttrain)
df = pd.DataFrame({'Predicted': y_pred.round()})
df['test values'] = x_train
df
# 
# df[df['test values']<1200]
# df['outputs'] = round(df['Predicted']/10000000,1)
110/133:


y_pred=reg.predict(x_train)
df = pd.DataFrame({'Predicted': y_pred.round()})
df['test values'] = x_train
df
# 
# df[df['test values']<1200]
# df['outputs'] = round(df['Predicted']/10000000,1)
113/1:
import pandas as pd          
import numpy as np          # For mathematical calculations
import matplotlib.pyplot as plt  # For plotting graphs
from datetime import datetime    # To access datetime
from pandas import Series        # To work on series
%matplotlib inline
import warnings                   # To ignore the warnings
warnings.filterwarnings("ignore")
113/2:
df = pd.read_csv('time-series.csv')
df1=df.reset_index()['Throughput']
113/3: df1
113/4:
### LSTM are sensitive to the scale of the data. so we apply MinMax scaler 
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))
df1=scaler.fit_transform(np.array(df1).reshape(-1,1))
113/5: print(df1)
113/6:
df = pd.read_csv('time-series.csv')
df1=df.reset_index()['Throughput']
113/7: df1
113/8:
### LSTM are sensitive to the scale of the data. so we apply MinMax scaler 
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))
df1=scaler.fit_transform(np.array(df1).reshape(-1,1))
113/9: print(df1)
113/10:
##splitting dataset into train and test split
training_size=int(len(df1)*0.80)
test_size=len(df1)-training_size
train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]
113/11: training_size,test_size
113/12:
import numpy
# convert an array of values into a dataset matrix
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return numpy.array(dataX), numpy.array(dataY)
113/13:
# reshape into X=t,t+1,t+2,t+3 and Y=t+4
time_step = 5
X_train, y_train = create_dataset(train_data, time_step)
X_test, ytest = create_dataset(test_data, time_step)
113/14: print(X_train.shape), print(y_train.shape)
113/15: print(X_test.shape), print(ytest.shape)
113/16:
# reshape input to be [samples, time steps, features] which is required for LSTM
X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)
113/17:
### Create the Stacked LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
113/18:
model=Sequential()
model.add(LSTM(50,return_sequences=True,input_shape=(10,1)))
model.add(LSTM(50,return_sequences=True))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
113/19: model.summary()
113/20: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=400,batch_size=16,verbose=1)
113/21:
model=Sequential()
model.add(LSTM(50,return_sequences=True,input_shape=(5,1)))
model.add(LSTM(50,return_sequences=True))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
113/22: model.summary()
113/23: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=400,batch_size=16,verbose=1)
113/24:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
113/25:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
113/26:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
113/27:

### Test Data RMSE
math.sqrt(mean_squared_error(ytest,test_predict))
113/28:
### Plotting 
# shift train predictions for plotting
look_back=5
trainPredictPlot = numpy.empty_like(df1)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
# shift test predictions for plotting
testPredictPlot = numpy.empty_like(df1)
testPredictPlot[:, :] = numpy.nan
testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict
# plot baseline and predictions
plt.figure(figsize=(18,8))
plt.plot(scaler.inverse_transform(df1))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
113/29:
df = pd.read_csv('time-series.csv')
df1=df.reset_index()['Throughput']
113/30: df1
113/31:
### LSTM are sensitive to the scale of the data. so we apply MinMax scaler 
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))
df1=scaler.fit_transform(np.array(df1).reshape(-1,1))
113/32: print(df1)
113/33:
##splitting dataset into train and test split
training_size=int(len(df1)*0.80)
test_size=len(df1)-training_size
train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]
113/34: training_size,test_size
113/35:
import numpy
# convert an array of values into a dataset matrix
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return numpy.array(dataX), numpy.array(dataY)
113/36:
# reshape into X=t,t+1,t+2,t+3 and Y=t+4
time_step = 7
X_train, y_train = create_dataset(train_data, time_step)
X_test, ytest = create_dataset(test_data, time_step)
113/37: print(X_train.shape), print(y_train.shape)
113/38: print(X_test.shape), print(ytest.shape)
113/39:
# reshape input to be [samples, time steps, features] which is required for LSTM
X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)
113/40:
### Create the Stacked LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
113/41:
model=Sequential()
model.add(LSTM(50,return_sequences=True,input_shape=(7,1)))
model.add(LSTM(50,return_sequences=True))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
113/42: model.summary()
113/43: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=400,batch_size=16,verbose=1)
113/44:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
113/45:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
113/46:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
113/47:

### Test Data RMSE
math.sqrt(mean_squared_error(ytest,test_predict))
113/48:
### Plotting 
# shift train predictions for plotting
look_back=5
trainPredictPlot = numpy.empty_like(df1)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
# shift test predictions for plotting
testPredictPlot = numpy.empty_like(df1)
testPredictPlot[:, :] = numpy.nan
testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict
# plot baseline and predictions
plt.figure(figsize=(18,8))
plt.plot(scaler.inverse_transform(df1))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
113/49:
### Plotting 
# shift train predictions for plotting
look_back=7
trainPredictPlot = numpy.empty_like(df1)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
# shift test predictions for plotting
testPredictPlot = numpy.empty_like(df1)
testPredictPlot[:, :] = numpy.nan
testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict
# plot baseline and predictions
plt.figure(figsize=(18,8))
plt.plot(scaler.inverse_transform(df1))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
113/50:
df = pd.read_csv('time-series.csv')
df1=df.reset_index()['Throughput']
113/51: df1
113/52:
### LSTM are sensitive to the scale of the data. so we apply MinMax scaler 
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))
df1=scaler.fit_transform(np.array(df1).reshape(-1,1))
113/53: print(df1)
113/54:
##splitting dataset into train and test split
training_size=int(len(df1)*0.80)
test_size=len(df1)-training_size
train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]
113/55: training_size,test_size
113/56:
import numpy
# convert an array of values into a dataset matrix
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return numpy.array(dataX), numpy.array(dataY)
113/57:
# reshape into X=t,t+1,t+2,t+3 and Y=t+4
time_step = 3
X_train, y_train = create_dataset(train_data, time_step)
X_test, ytest = create_dataset(test_data, time_step)
113/58: print(X_train.shape), print(y_train.shape)
113/59: print(X_test.shape), print(ytest.shape)
113/60:
# reshape input to be [samples, time steps, features] which is required for LSTM
X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)
113/61:
### Create the Stacked LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
113/62:
model=Sequential()
model.add(LSTM(50,return_sequences=True,input_shape=(7,1)))
model.add(LSTM(50,return_sequences=True))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
113/63: model.summary()
113/64: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=3000,batch_size=16,verbose=1)
113/65:
model=Sequential()
model.add(LSTM(50,return_sequences=True,input_shape=(3,1)))
model.add(LSTM(50,return_sequences=True))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
113/66: model.summary()
113/67: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=3000,batch_size=16,verbose=1)
113/68:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
113/69:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
113/70:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
113/71:

### Test Data RMSE
math.sqrt(mean_squared_error(ytest,test_predict))
113/72:
### Plotting 
# shift train predictions for plotting
look_back=3
trainPredictPlot = numpy.empty_like(df1)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
# shift test predictions for plotting
testPredictPlot = numpy.empty_like(df1)
testPredictPlot[:, :] = numpy.nan
testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict
# plot baseline and predictions
plt.figure(figsize=(18,8))
plt.plot(scaler.inverse_transform(df1))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
113/73:
df = pd.read_csv('time-series.csv')
df1=df.reset_index()['Throughput']
113/74: df1
113/75:
### LSTM are sensitive to the scale of the data. so we apply MinMax scaler 
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))
df1=scaler.fit_transform(np.array(df1).reshape(-1,1))
113/76: print(df1)
113/77:
##splitting dataset into train and test split
training_size=int(len(df1)*0.80)
test_size=len(df1)-training_size
train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]
113/78: training_size,test_size
113/79:
import numpy
# convert an array of values into a dataset matrix
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return numpy.array(dataX), numpy.array(dataY)
113/80:
# reshape into X=t,t+1,t+2,t+3 and Y=t+4
time_step = 5
X_train, y_train = create_dataset(train_data, time_step)
X_test, ytest = create_dataset(test_data, time_step)
113/81: print(X_train.shape), print(y_train.shape)
113/82: print(X_test.shape), print(ytest.shape)
113/83:
# reshape input to be [samples, time steps, features] which is required for LSTM
X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)
113/84:
### Create the Stacked LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
113/85:
model=Sequential()
model.add(LSTM(50,return_sequences=True,input_shape=(5,1)))
model.add(LSTM(50,return_sequences=True))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
113/86: model.summary()
113/87: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=600,batch_size=8,verbose=1)
113/88:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
113/89:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
113/90:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
113/91:

### Test Data RMSE
math.sqrt(mean_squared_error(ytest,test_predict))
113/92:
### Plotting 
# shift train predictions for plotting
look_back=5
trainPredictPlot = numpy.empty_like(df1)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
# shift test predictions for plotting
testPredictPlot = numpy.empty_like(df1)
testPredictPlot[:, :] = numpy.nan
testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict
# plot baseline and predictions
plt.figure(figsize=(18,8))
plt.plot(scaler.inverse_transform(df1))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
114/1:
import pandas as pd          
import numpy as np          # For mathematical calculations
import matplotlib.pyplot as plt  # For plotting graphs
from datetime import datetime    # To access datetime
from pandas import Series        # To work on series
%matplotlib inline
import warnings                   # To ignore the warnings
warnings.filterwarnings("ignore")
114/2:
df = pd.read_csv('time-series.csv')
df1=df.reset_index()['Throughput']
114/3: df1
114/4:
### LSTM are sensitive to the scale of the data. so we apply MinMax scaler 
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))
df1=scaler.fit_transform(np.array(df1).reshape(-1,1))
114/5: print(df1)
114/6:
##splitting dataset into train and test split
training_size=int(len(df1)*0.80)
test_size=len(df1)-training_size
train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]
114/7: training_size,test_size
114/8:
import numpy
# convert an array of values into a dataset matrix
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return numpy.array(dataX), numpy.array(dataY)
114/9:
# reshape into X=t,t+1,t+2,t+3 and Y=t+4
time_step = 5
X_train, y_train = create_dataset(train_data, time_step)
X_test, ytest = create_dataset(test_data, time_step)
114/10: print(X_train.shape), print(y_train.shape)
114/11: print(X_test.shape), print(ytest.shape)
114/12:
# reshape input to be [samples, time steps, features] which is required for LSTM
X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)
114/13:
### Create the Stacked LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
114/14:
model=Sequential()
model.add(LSTM(50,return_sequences=True,input_shape=(5,1)))
model.add(LSTM(50,return_sequences=True))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
114/15: model.summary()
114/16: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=400,batch_size=16,verbose=1)
114/17:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
114/18:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
114/19:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
114/20:

### Test Data RMSE
math.sqrt(mean_squared_error(ytest,test_predict))
114/21:
### Create the Stacked LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
114/22:
model=Sequential()
model.add(LSTM(64,return_sequences=True,input_shape=(5,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
114/23: model.summary()
114/24: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=200,batch_size=32,verbose=1)
114/25:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
114/26:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
114/27:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
114/28:

### Test Data RMSE
math.sqrt(mean_squared_error(ytest,test_predict))
114/29:
### Plotting 
# shift train predictions for plotting
look_back=5
trainPredictPlot = numpy.empty_like(df1)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
# shift test predictions for plotting
testPredictPlot = numpy.empty_like(df1)
testPredictPlot[:, :] = numpy.nan
testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict
# plot baseline and predictions
plt.figure(figsize=(18,8))
plt.plot(scaler.inverse_transform(df1))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
114/30:
import pandas as pd          
import numpy as np          # For mathematical calculations
import matplotlib.pyplot as plt  # For plotting graphs
from datetime import datetime    # To access datetime
from pandas import Series        # To work on series
%matplotlib inline
import warnings                   # To ignore the warnings
warnings.filterwarnings("ignore")
114/31:
df = pd.read_csv('time-series.csv')
df1=df.reset_index()['Throughput']
114/32: df1
114/33:
### LSTM are sensitive to the scale of the data. so we apply MinMax scaler 
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))
df1=scaler.fit_transform(np.array(df1).reshape(-1,1))
114/34: print(df1)
114/35:
##splitting dataset into train and test split
training_size=int(len(df1)*0.80)
test_size=len(df1)-training_size
train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]
114/36: training_size,test_size
114/37:
import numpy
# convert an array of values into a dataset matrix
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return numpy.array(dataX), numpy.array(dataY)
114/38:
# reshape into X=t,t+1,t+2,t+3 and Y=t+4
time_step = 4
X_train, y_train = create_dataset(train_data, time_step)
X_test, ytest = create_dataset(test_data, time_step)
114/39: print(X_train.shape), print(y_train.shape)
114/40: print(X_test.shape), print(ytest.shape)
114/41:
# reshape input to be [samples, time steps, features] which is required for LSTM
X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)
114/42:
### Create the Stacked LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
114/43:
model=Sequential()
model.add(LSTM(64,return_sequences=True,input_shape=(4,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
114/44: model.summary()
114/45: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=200,batch_size=32,verbose=1)
114/46:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
114/47:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
114/48:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
114/49:

### Test Data RMSE
math.sqrt(mean_squared_error(ytest,test_predict))
114/50:
### Plotting 
# shift train predictions for plotting
look_back=4
trainPredictPlot = numpy.empty_like(df1)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
# shift test predictions for plotting
testPredictPlot = numpy.empty_like(df1)
testPredictPlot[:, :] = numpy.nan
testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict
# plot baseline and predictions
plt.figure(figsize=(18,8))
plt.plot(scaler.inverse_transform(df1))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
114/51:
df = pd.read_csv('time-series.csv')
df1=df.reset_index()['Throughput']
114/52: df1
114/53:
### LSTM are sensitive to the scale of the data. so we apply MinMax scaler 
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))
df1=scaler.fit_transform(np.array(df1).reshape(-1,1))
114/54: print(df1)
114/55:
##splitting dataset into train and test split
training_size=int(len(df1)*0.80)
test_size=len(df1)-training_size
train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]
114/56: training_size,test_size
114/57:
import numpy
# convert an array of values into a dataset matrix
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return numpy.array(dataX), numpy.array(dataY)
114/58:
# reshape into X=t,t+1,t+2,t+3 and Y=t+4
time_step = 1
X_train, y_train = create_dataset(train_data, time_step)
X_test, ytest = create_dataset(test_data, time_step)
114/59: print(X_train.shape), print(y_train.shape)
114/60: print(X_test.shape), print(ytest.shape)
114/61:
# reshape input to be [samples, time steps, features] which is required for LSTM
X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)
114/62:
### Create the Stacked LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
114/63:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(4,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
114/64:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(4,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
114/65: model.summary()
114/66: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=150,batch_size=16,verbose=1)
114/67: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=160,batch_size=16,verbose=1)
114/68: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=150,batch_size=32,verbose=1)
114/69:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
114/70: model.summary()
114/71: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=150,batch_size=16,verbose=1)
114/72:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
114/73:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
114/74:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
114/75:
### Plotting 
# shift train predictions for plotting
look_back=1
trainPredictPlot = numpy.empty_like(df1)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
# shift test predictions for plotting
testPredictPlot = numpy.empty_like(df1)
testPredictPlot[:, :] = numpy.nan
testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict
# plot baseline and predictions
plt.figure(figsize=(18,8))
plt.plot(scaler.inverse_transform(df1))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
114/76:
df = pd.read_csv('time-series.csv')
df1=df.reset_index()['Throughput']
114/77: df1
114/78:
### LSTM are sensitive to the scale of the data. so we apply MinMax scaler 
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))
df1=scaler.fit_transform(np.array(df1).reshape(-1,1))
114/79: print(df1)
114/80:
##splitting dataset into train and test split
training_size=int(len(df1)*0.80)
test_size=len(df1)-training_size
train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]
114/81: training_size,test_size
114/82:
import numpy
# convert an array of values into a dataset matrix
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return numpy.array(dataX), numpy.array(dataY)
114/83:
# reshape into X=t,t+1,t+2,t+3 and Y=t+4
time_step = 1
X_train, y_train = create_dataset(train_data, time_step)
X_test, ytest = create_dataset(test_data, time_step)
114/84: print(X_train.shape), print(y_train.shape)
114/85: print(X_test.shape), print(ytest.shape)
114/86:
# reshape input to be [samples, time steps, features] which is required for LSTM
X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)
114/87:
### Create the Stacked LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
114/88:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
114/89: model.summary()
114/90: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=300,batch_size=16,verbose=1)
114/91:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
114/92:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
114/93:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
114/94:
import pandas as pd          
import numpy as np          # For mathematical calculations
import matplotlib.pyplot as plt  # For plotting graphs
from datetime import datetime    # To access datetime
from pandas import Series        # To work on series
%matplotlib inline
import warnings                   # To ignore the warnings
warnings.filterwarnings("ignore")
114/95:
df = pd.read_csv('time-series.csv')
df1=df.reset_index()['Throughput']
114/96: df1
114/97:
### LSTM are sensitive to the scale of the data. so we apply MinMax scaler 
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))
df1=scaler.fit_transform(np.array(df1).reshape(-1,1))
114/98: print(df1)
114/99:
##splitting dataset into train and test split
training_size=int(len(df1)*0.80)
test_size=len(df1)-training_size
train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]
114/100: training_size,test_size
114/101:
import numpy
# convert an array of values into a dataset matrix
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return numpy.array(dataX), numpy.array(dataY)
114/102:
# reshape into X=t,t+1,t+2,t+3 and Y=t+4
time_step = 1
X_train, y_train = create_dataset(train_data, time_step)
X_test, ytest = create_dataset(test_data, time_step)
114/103: print(X_train.shape), print(y_train.shape)
114/104: print(X_test.shape), print(ytest.shape)
114/105:
# reshape input to be [samples, time steps, features] which is required for LSTM
X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)
114/106:
### Create the Stacked LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
114/107:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
114/108: model.summary()
114/109: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=150,batch_size=8,verbose=1)
114/110:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
114/111:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
114/112:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
114/113:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='relu')
114/114:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='RMSprop')
114/115: model.summary()
114/116: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=150,batch_size=16,verbose=1)
114/117:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
114/118:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
114/119:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
114/120:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='GAD')
114/121:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='SGD')
116/1:
import pandas as pd          
import numpy as np          # For mathematical calculations
import matplotlib.pyplot as plt  # For plotting graphs
from datetime import datetime    # To access datetime
from pandas import Series        # To work on series
%matplotlib inline
import warnings                   # To ignore the warnings
warnings.filterwarnings("ignore")
116/2:
df = pd.read_csv('time-series.csv')
df1=df.reset_index()['Throughput']
116/3: df1
116/4:
### LSTM are sensitive to the scale of the data. so we apply MinMax scaler 
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))
df1=scaler.fit_transform(np.array(df1).reshape(-1,1))
116/5: print(df1)
116/6:
##splitting dataset into train and test split
training_size=int(len(df1)*0.80)
test_size=len(df1)-training_size
train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]
116/7: training_size,test_size
116/8:
import numpy
# convert an array of values into a dataset matrix
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return numpy.array(dataX), numpy.array(dataY)
116/9:
# reshape into X=t,t+1,t+2,t+3 and Y=t+4
time_step = 1
X_train, y_train = create_dataset(train_data, time_step)
X_test, ytest = create_dataset(test_data, time_step)
116/10: print(X_train.shape), print(y_train.shape)
116/11: print(X_test.shape), print(ytest.shape)
116/12:
# reshape input to be [samples, time steps, features] which is required for LSTM
X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)
116/13:
### Create the Stacked LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
116/14:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='SGD')
116/15: model.summary()
116/16: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=150,batch_size=16,verbose=1)
116/17:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
116/18:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
116/19:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
116/20:

### Test Data RMSE
math.sqrt(mean_squared_error(ytest,test_predict))
116/21:
### Plotting 
# shift train predictions for plotting
look_back=1
trainPredictPlot = numpy.empty_like(df1)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
# shift test predictions for plotting
testPredictPlot = numpy.empty_like(df1)
testPredictPlot[:, :] = numpy.nan
testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict
# plot baseline and predictions
plt.figure(figsize=(18,8))
plt.plot(scaler.inverse_transform(df1))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
116/22:
df = pd.read_csv('time-series.csv')
df1=df.reset_index()['Throughput']
116/23: df1
116/24:
### LSTM are sensitive to the scale of the data. so we apply MinMax scaler 
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))
df1=scaler.fit_transform(np.array(df1).reshape(-1,1))
116/25: print(df1)
116/26:
##splitting dataset into train and test split
training_size=int(len(df1)*0.80)
test_size=len(df1)-training_size
train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]
116/27: training_size,test_size
116/28:
import numpy
# convert an array of values into a dataset matrix
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return numpy.array(dataX), numpy.array(dataY)
116/29:
# reshape into X=t,t+1,t+2,t+3 and Y=t+4
time_step = 1
X_train, y_train = create_dataset(train_data, time_step)
X_test, ytest = create_dataset(test_data, time_step)
116/30: print(X_train.shape), print(y_train.shape)
116/31: print(X_test.shape), print(ytest.shape)
116/32:
# reshape input to be [samples, time steps, features] which is required for LSTM
X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)
116/33:
### Create the Stacked LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
116/34:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='Momentum')
116/35:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='momentum')
116/36:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='AdaMax')
116/37: model.summary()
116/38: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=150,batch_size=16,verbose=1)
116/39:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
116/40:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
116/41:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
116/42:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='AdaDelta')
116/43:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='AdaDelta')
116/44: model.summary()
116/45: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=150,batch_size=16,verbose=1)
116/46:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
116/47:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
116/48:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
116/49:

### Test Data RMSE
math.sqrt(mean_squared_error(ytest,test_predict))
116/50:
### Plotting 
# shift train predictions for plotting
look_back=1
trainPredictPlot = numpy.empty_like(df1)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
# shift test predictions for plotting
testPredictPlot = numpy.empty_like(df1)
testPredictPlot[:, :] = numpy.nan
testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict
# plot baseline and predictions
plt.figure(figsize=(18,8))
plt.plot(scaler.inverse_transform(df1))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
116/51: len(test_data)
116/52:
x_input=test_data[40:].reshape(1,-1)
x_input.shape
116/53:
temp_input=list(x_input)
temp_input=temp_input[0].tolist()
116/54: temp_input
116/55:
# demonstrate prediction for next 10 days
from numpy import array

lst_output=[]
n_steps=10
i=0
while(i<90):
    
    if(len(temp_input)>10):
        #print(temp_input)
        x_input=np.array(temp_input[1:])
        print("{} day input {}".format(i,x_input))
        x_input=x_input.reshape(1,-1)
        x_input = x_input.reshape((1, n_steps, 1))
        #print(x_input)
        yhat = model.predict(x_input, verbose=0)
        print("{} day output {}".format(i,yhat))
        temp_input.extend(yhat[0].tolist())
        temp_input=temp_input[1:]
        #print(temp_input)
        lst_output.extend(yhat.tolist())
        i=i+1
    else:
        x_input = x_input.reshape((1, n_steps,1))
        yhat = model.predict(x_input, verbose=0)
        print(yhat[0])
        temp_input.extend(yhat[0].tolist())
        print(len(temp_input))
        lst_output.extend(yhat.tolist())
        i=i+1
    

print(lst_output)
116/56:
day_new=np.arange(1,11)
day_pred=np.arange(11,101)
116/57: len(df1)
116/58:
plt.figure(figsize=(18,8))
plt.plot(day_new,scaler.inverse_transform(df1[240:]))
plt.plot(day_pred,scaler.inverse_transform(lst_output))
116/59:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='MOMENTUM')
116/60:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='Nadam')
116/61: model.summary()
116/62: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=150,batch_size=16,verbose=1)
116/63:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
116/64:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
116/65:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
116/66:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
116/67:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
116/68:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
116/69:

### Test Data RMSE
math.sqrt(mean_squared_error(ytest,test_predict))
116/70:
df = pd.read_csv('time-series.csv')
df1=df.reset_index()['Throughput']
116/71: df1
116/72:
### LSTM are sensitive to the scale of the data. so we apply MinMax scaler 
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))
df1=scaler.fit_transform(np.array(df1).reshape(-1,1))
116/73: print(df1)
116/74:
##splitting dataset into train and test split
training_size=int(len(df1)*0.80)
test_size=len(df1)-training_size
train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]
116/75: training_size,test_size
116/76:
import numpy
# convert an array of values into a dataset matrix
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return numpy.array(dataX), numpy.array(dataY)
116/77:
# reshape into X=t,t+1,t+2,t+3 and Y=t+4
time_step = 1
X_train, y_train = create_dataset(train_data, time_step)
X_test, ytest = create_dataset(test_data, time_step)
116/78: print(X_train.shape), print(y_train.shape)
116/79: print(X_test.shape), print(ytest.shape)
116/80:
# reshape input to be [samples, time steps, features] which is required for LSTM
X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)
116/81:
### Create the Stacked LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
116/82:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32,return_sequences=True))
model.add(LSTM(16))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='SGD')
116/83: model.summary()
116/84: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=250,batch_size=8,verbose=1)
116/85:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
116/86:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
116/87:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
116/88:

### Test Data RMSE
math.sqrt(mean_squared_error(ytest,test_predict))
116/89:
### Plotting 
# shift train predictions for plotting
look_back=1
trainPredictPlot = numpy.empty_like(df1)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
# shift test predictions for plotting
testPredictPlot = numpy.empty_like(df1)
testPredictPlot[:, :] = numpy.nan
testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict
# plot baseline and predictions
plt.figure(figsize=(18,8))
plt.plot(scaler.inverse_transform(df1))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
116/90:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32,return_sequences=True))
model.add(LSTM(16))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adams')
116/91:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32,return_sequences=True))
model.add(LSTM(16))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
116/92:
##splitting dataset into train and test split
training_size=int(len(df1)*0.80)
test_size=len(df1)-training_size
train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]
116/93: training_size,test_size
116/94:
import numpy
# convert an array of values into a dataset matrix
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return numpy.array(dataX), numpy.array(dataY)
116/95:
# reshape into X=t,t+1,t+2,t+3 and Y=t+4
time_step = 1
X_train, y_train = create_dataset(train_data, time_step)
X_test, ytest = create_dataset(test_data, time_step)
116/96: print(X_train.shape), print(y_train.shape)
116/97: print(X_test.shape), print(ytest.shape)
116/98:
# reshape input to be [samples, time steps, features] which is required for LSTM
X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)
116/99:
### Create the Stacked LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
116/100:
model=Sequential()
model.add(LSTM(128,return_sequences=True,input_shape=(1,1)))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(32,return_sequences=True))
model.add(LSTM(16))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
116/101: model.summary()
116/102: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=150,batch_size=16,verbose=1)
116/103:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
116/104:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
116/105:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
116/106: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=150,batch_size=32,verbose=1)
116/107:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
116/108:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
116/109:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
116/110: model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=150,batch_size=16,verbose=1)
116/111:
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
116/112:

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
116/113:
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
116/114:

### Test Data RMSE
math.sqrt(mean_squared_error(ytest,test_predict))
116/115:
### Plotting 
# shift train predictions for plotting
look_back=1
trainPredictPlot = numpy.empty_like(df1)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
# shift test predictions for plotting
testPredictPlot = numpy.empty_like(df1)
testPredictPlot[:, :] = numpy.nan
testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict
# plot baseline and predictions
plt.figure(figsize=(18,8))
plt.plot(scaler.inverse_transform(df1))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
116/116: len(test_data)
116/117:
x_input=test_data[40:].reshape(1,-1)
x_input.shape
116/118:
temp_input=list(x_input)
temp_input=temp_input[0].tolist()
116/119: temp_input
116/120:
# demonstrate prediction for next 10 days
from numpy import array

lst_output=[]
n_steps=10
i=0
while(i<90):
    
    if(len(temp_input)>10):
        #print(temp_input)
        x_input=np.array(temp_input[1:])
        print("{} day input {}".format(i,x_input))
        x_input=x_input.reshape(1,-1)
        x_input = x_input.reshape((1, n_steps, 1))
        #print(x_input)
        yhat = model.predict(x_input, verbose=0)
        print("{} day output {}".format(i,yhat))
        temp_input.extend(yhat[0].tolist())
        temp_input=temp_input[1:]
        #print(temp_input)
        lst_output.extend(yhat.tolist())
        i=i+1
    else:
        x_input = x_input.reshape((1, n_steps,1))
        yhat = model.predict(x_input, verbose=0)
        print(yhat[0])
        temp_input.extend(yhat[0].tolist())
        print(len(temp_input))
        lst_output.extend(yhat.tolist())
        i=i+1
    

print(lst_output)
116/121:
day_new=np.arange(1,11)
day_pred=np.arange(11,101)
116/122: len(df1)
116/123:
plt.figure(figsize=(18,8))
plt.plot(day_new,scaler.inverse_transform(df1[240:]))
plt.plot(day_pred,scaler.inverse_transform(lst_output))
116/124:
plt.figure(figsize=(18,8))
df3=df1.tolist()
df3.extend(lst_output)
plt.plot(df3)
117/1:
from sklearn.datasets import make_classification
from matplotlib import pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import pandas as pd
117/2:
x, y = make_classification(
    n_samples=100,
    n_features=1,
    n_classes=2,
    n_clusters_per_class=1,
    flip_y=0.03,
    n_informative=1,
    n_redundant=0,
    n_repeated=0
)
117/3:
plt.scatter(x, y, c=y, cmap='rainbow')
plt.title('Scatter Plot of Logistic Regression')
plt.show()
117/4:
data=pd.read_csv('cause_failure2.csv')
data.head(3)
117/5: data['Throughput']
117/6: type(data['Throughput'])
117/7: x, y = data['Throughput'], data['Label']
117/8:
plt.scatter(x, y, c=y, cmap='rainbow')
plt.title('Scatter Plot of Logistic Regression')
plt.xlabel("Throughput")
plt.xlabel("Label")
plt.grid()
plt.show()
117/9:
plt.scatter(x, y, cmap='rainbow')
plt.title('Scatter Plot of Logistic Regression')
plt.xlabel("Throughput")
plt.xlabel("Label")
plt.grid()
plt.show()
117/10:
plt.scatter(x, y, cmap='red')
plt.title('Scatter Plot of Logistic Regression')
plt.xlabel("Throughput")
plt.xlabel("Label")
plt.grid()
plt.show()
117/11:
plt.scatter(x, y, c=y, cmap='rainbow')
plt.title('Scatter Plot of Logistic Regression')
plt.xlabel("Throughput")
plt.xlabel("Label")
plt.grid()
plt.show()
117/12:
plt.scatter(x, y, c=y, cmap='rainbow')
plt.title('Scatter Plot of Logistic Regression')
plt.xlabel("Throughput")
plt.xlabel("Label")
plt.grid()
plt.show()
117/13: x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)
117/14:
log_reg = LogisticRegression()
log_reg.fit(x_train, y_train)
117/15: x_train.reshae(-1,1)
117/16: np.array(x_train).reshape(-1,1)
117/17:
import numpy as np
np.array(x_train).reshape(-1,1)
117/18:
from sklearn.datasets import make_classification
from matplotlib import pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import pandas as pd
import numpy as np
117/19:
log_reg = LogisticRegression()
xtrain = np.array(x_train).reshape(-1,1)
y_train = np.array(y_train).reshape(-1,1)
log_reg.fit(x_train, y_train)
117/20:
log_reg = LogisticRegression()
x_train = np.array(x_train).reshape(-1,1)
y_train = np.array(y_train).reshape(-1,1)
log_reg.fit(x_train, y_train)
117/21:
print(log_reg.coef_)
print(log_reg.intercept_)
117/22:
x_test = np.array(x_test).reshape(-1,1)
y_pred = log_reg.predict(x_test)
117/23:
y_test = np.array(y_test).reshape(-1,1)
confusion_matrix(y_test, y_pred)
117/24: y_pred
117/25: y_test
117/26:
from sklearn.datasets import make_classification
from matplotlib import pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import pandas as pd
import numpy as np
117/27:
data=pd.read_csv('cause_failure2.csv')
data.head(3)
117/28: x, y = data['Throughput'], data['Label']
117/29:
plt.scatter(x, y, c=y, cmap='rainbow')
plt.title('Scatter Plot of Logistic Regression')
plt.xlabel("Throughput")
plt.xlabel("Label")
plt.grid()
plt.show()
117/30: x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2)
117/31:
log_reg = LogisticRegression()
x_train = np.array(x_train).reshape(-1,1)
y_train = np.array(y_train).reshape(-1,1)
log_reg.fit(x_train, y_train)
117/32:
print(log_reg.coef_)
print(log_reg.intercept_)
117/33:
x_test = np.array(x_test).reshape(-1,1)
y_pred = log_reg.predict(x_test)
117/34:
y_test = np.array(y_test).reshape(-1,1)
confusion_matrix(y_test, y_pred)
117/35: y_pred
117/36: y_test
117/37:
allx = np.array(x).reshape(-1,1)
y_pred = log_reg.predict(allx)
117/38: y_predict
117/39: y_pred
117/40: type(y_pred)
117/41: predicted = y_pred
117/42: predicted[0]
117/43: predicted.item()
117/44:
for i in y_pred:
    print(I)
117/45:
for i in y_pred:
    print(i)
117/46:
predicted = []
for i in y_pred:
    if i == 0:
        predicted.append('Low Pressure')
    else:
        predicted.append('High Temprature')
predicted
117/47: allx
117/48: result = pd.DataFrame([allx,predicted], name=['Throughput','Prediction'])
117/49: result = pd.DataFrame([allx,predicted], ['Throughput','Prediction'])
117/50: result = pd.DataFrame([allx,predicted], columns=['Throughput','Prediction'])
117/51: allx.shape, predicted.shape
117/52: allx.size, predicted.shape
117/53: allx.view, predicted.shape
117/54: np.array(allx).shape, predicted.shape
117/55: np.array(allx).shape, np.array(predicted).shape
117/56:
allx = np.array(allx).reshape(35,)
result = pd.DataFrame([allx,predicted], columns=['Throughput','Prediction'])
117/57:
allx = np.array(x).reshape(-1,1)
predicted = np.array(predicted).reshape(-1,1)
result = pd.DataFrame([allx,predicted], columns=['Throughput','Prediction'])
117/58:
allx = np.array(x).reshape(-1,1)
predicted = np.array(predicted).reshape(-1,1)
result = pd.DataFrame([allx,predicted])
117/59:
allx = np.array(x).reshape(-1,1)
predicted = np.array(predicted).reshape(-1,1)
result = pd.DataFrame(allx,predicted)
117/60: result
117/61: predicted = np.array(predicted).reshape(1,-1)
117/62: predicted
117/63: predicted[0]
117/64: predicted[0].shape
117/65: allx.shape
117/66: allx
117/67: allx.shape
117/68: allx = np.array().reshape(-1,1)
117/69: allx = np.array(x).reshape(-1,1)
117/70: allx.shape
117/71: allx = np.array(x).reshape(35,0)
117/72: allx[0]
117/73: allx.shape
117/74: predict.shape
117/75: predicted.shape
117/76: predicted[0.shape
117/77: predicted[0].shape
117/78: predicted[0]
117/79:
predicted = predicted[0]
predicted
117/80:
throughput = allx
throughput
117/81:
throughput = allx[0]
throughput
117/82:
throughput = np.array(x).reshape(-1,1)
throughput
117/83:
throughput = np.array(x).reshape(1,-1)
throughput
117/84:
throughput = np.array(x).reshape(1,-1)[0]
throughput
117/85:
result = pd.DataFrame(data=[throughput,predicted], columns=['Throughput','Prediction'])
result.tail()
117/86:
result = pd.DataFrame(data=[throughput,predicted[0]], columns=['Throughput','Prediction'])
result.tail()
117/87:
result = pd.DataFrame(data=[throughput,predicted], columns=['Throughput','Prediction'])
result.tail()
117/88:
predicted = predicted[0]
predicted.shape
117/89:
predicted = predicted[0]
predicted
117/90:
predicted = predicted[0]
predicted
117/91:
allx = np.array(x).reshape(-1,1)
predicted = np.array(predicted).reshape(-1,1)
result = pd.DataFrame(allx,predicted)
117/92: predicted[0].shape
117/93:
predicted = predicted[0]
predicted
117/94:
predicted = []
for i in y_pred:
    if i == 0:
        predicted.append('Low Pressure')
    else:
        predicted.append('High Temprature')
predicted
117/95:
# predicted = predicted[0]
predicted
117/96:
result = pd.DataFrame(data=[throughput,predicted], columns=['Throughput','Prediction'])
result.tail()
117/97: predicted.shape
117/98: np.array(predicted).shape
117/99: result.shape
117/100: result.reshape(1,35)
117/101: result.reshape
117/102: result
117/103:
predicted = []
for i in y_pred:
    if i == 0:
        predicted.append('Low Pressure')
    else:
        predicted.append('High Temprature')
predicted
117/104: allx
117/105: allx[0]
117/106:
da = []
for i in da:
    da.append(i)
da
117/107:
da = []
for i in allx:
    da.append(i)
da
117/108:
# da = []
# for i in allx:
#     da.append(float(i))
da.clear()
117/109:
# # da = []
# # for i in allx:
# #     da.append(float(i))
# da
117/110:
da = []
for i in allx:
    da.append(float(i))
da
117/111:
result = pd.DataFrame([da,predicted],['Throughput','Prediction'])
result.tail()
117/112:
result = pd.DataFrame([da,predicted],['Throughput','Prediction'])
result.T
117/113:
result = pd.DataFrame([da,predicted],['Throughput','Prediction'])
result.T(inplace=True)
117/114:
result = pd.DataFrame([da,predicted],['Throughput','Prediction'])
result = result.T
117/115: result.to_csv('result.csv')
117/116: result.to_csv('result.csv', index=None)
119/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
119/2:
df = pd.read_csv("time-series.csv")
df.head()
119/3: df.isnull().sum()
119/4:
plt.hist(df["Throughput"])
plt.xlabel("Time")
plt.ylabel("Throughput")
plt.show()
119/5:
plt.hist(df["DAte"])
plt.xlabel("Time")
plt.ylabel("Throughput")
plt.show()
119/6:
plt.hist(df["Date"])
plt.xlabel("Time")
plt.ylabel("Throughput")
plt.show()
119/7:
plt.hist(df["Throughput"])
plt.xlabel("Time")
plt.ylabel("Throughput")
plt.show()
119/8:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
119/9:
x = df['Date']
y = df['Throughput']
plt.scatter(x, y, c=y, cmap='rainbow')
plt.title('Scatter Plot of Logistic Regression')
plt.xlabel("Throughput")
plt.xlabel("Label")
plt.grid()
plt.show()
119/10:
x = df['Date']
y = df['Throughput']
plt.scatter(x, y, c=y, cmap='rainbow')
plt.title('Scatter Plot of Logistic Regression')
plt.xlabel("Throughput")
plt.xlabel("Label")
plt.show()
119/11: x_train, x_test, y_train, y_test = train_test_split(x, y, shuffle=False)
119/12: x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=False)
119/13:
log_reg = LogisticRegression()
x_train = np.array(x_train).reshape(-1,1)
y_train = np.array(y_train).reshape(-1,1)
log_reg.fit(x_train, y_train)
119/14: from sklearn.ensemble import RandomForestRegressor
119/15:
rf = RandomForestRegressor()
rf.fit(x_train,y_train)
119/16: df['Data'].shape
119/17: df['Data'].head()
119/18: df['Data'][0]
119/19:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
119/20:
df = pd.read_csv("time-series.csv")
df.head()
119/21: df[df['Data']]
119/22: df[df['Data']].shape
119/23: df[df['Data']]
119/24: df[df['Date']].shape
119/25: df['Date'].shape
119/26:
num = np.arange(1,250)
num
119/27:
num = np.arange(1,251)
num
119/28: df.tail(2)
119/29:
num = np.arange(1,250)
num
119/30: x_train, x_test, y_train, y_test = train_test_split(num, y, train_size=0.8, shuffle=False)
119/31:
num = np.arange(1,251)
num
119/32: x_train, x_test, y_train, y_test = train_test_split(num, y, train_size=0.8, shuffle=False)
119/33: from sklearn.ensemble import RandomForestRegressor
119/34:
rf = RandomForestRegressor()
rf.fit(x_train,y_train)
119/35:
rf = RandomForestRegressor()
num = np.array(num).reshape(-1,1)
rf.fit(x_train,y_train)
119/36:
rf = RandomForestRegressor()
x_train = np.array(x_train).reshape(-1,1)
y_train = np.array(y_train).reshape(-1,1)
rf.fit(x_train,y_train)
119/37:
x_test = np.array(x_test).reshape(-1,1)
y_pred = rf.predict(x_test)
119/38: y_pred
119/39: y_pred.shape
119/40: y_pred
119/41: x_test
119/42: type(x_test)
119/43: x_test.reshape(1,-1)
119/44: x_test.reshape(1,-1)[0]
119/45: x_test.reshape(1,-1)[0]
117/117:
result = pd.DataFrame([da,predicted],['Throughput','Prediction'])
result = result.T
117/118:
from sklearn.metrics import mean_squared_error

y_test = np.array(y_test).reshape(-1,1)

mean_squared_error(y_test, y_predict)
117/119:
from sklearn.metrics import mean_squared_error

y_test = np.array(y_test).reshape(-1,1)

mean_squared_error(y_test, y_pred)
117/120: y_test.shape, y_pred
117/121: y_test.shape, y_pred.shape
117/122:
from sklearn.datasets import make_classification
from matplotlib import pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import pandas as pd
import numpy as np
117/123:
data=pd.read_csv('cause_failure2.csv')
data.head(3)
117/124: x, y = data['Throughput'], data['Label']
117/125:
plt.scatter(x, y, c=y, cmap='rainbow')
plt.title('Scatter Plot of Logistic Regression')
plt.xlabel("Throughput")
plt.xlabel("Label")
plt.grid()
plt.show()
117/126: x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2)
117/127:
log_reg = LogisticRegression()
x_train = np.array(x_train).reshape(-1,1)
y_train = np.array(y_train).reshape(-1,1)
log_reg.fit(x_train, y_train)
117/128:
print(log_reg.coef_)
print(log_reg.intercept_)
117/129:
x_test = np.array(x_test).reshape(-1,1)
y_pred = log_reg.predict(x_test)
117/130: y_test.shape, y_pred.shape
117/131:
from sklearn.metrics import mean_squared_error

y_test = np.array(y_test).reshape(-1,1)

mean_squared_error(y_test, y_pred)
120/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import statsmodels.api as sm

from sklearn.ensemble.forest import RandomForestRegressor

from sklearn.metrics import mean_squared_error, r2_score

import math
from dateutil.relativedelta import relativedelta
from datetime import datetime, date
%matplotlib inline
plt.rcParams['figure.figsize']=(20,10)
plt.style.use('ggplot')
120/2:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import statsmodels.api as sm

from sklearn.ensemble import RandomForestRegressor

from sklearn.metrics import mean_squared_error, r2_score

import math
from dateutil.relativedelta import relativedelta
from datetime import datetime, date
%matplotlib inline
plt.rcParams['figure.figsize']=(20,10)
plt.style.use('ggplot')
120/3:
df = pd.read_csv("time-series.csv")
df.head()
120/4:
x = df['Date']
y = df['Throughput']
plt.scatter(x, y, c=y, cmap='rainbow')
plt.title('Scatter Plot of Logistic Regression')
plt.xlabel("Throughput")
plt.xlabel("Label")
plt.show()
120/5:
x = np.arange(1,251)
y = df["Label"]
x.shape, y.shape
120/6:
x = np.arange(1,251)
y = df["Throughput"]
x.shape, y.shape
120/7:
x = np.arange(1,251)
y = df["Throughput"]
120/8:
def prepareDataForClassificationRF(X, y):
    """
    generates categorical output column, which is then used
    to create the train and test data
    """
    
    X_train = X[X.index < 200]
    y_train = y[y.index < 200]              
    
    X_test = X[X.index >= 200]    
    y_test = y[y.index >= 200]
    
    return X_train, y_train, X_test, y_test, datase2
120/9: df.plot(subplots=True)
120/10: X_train, y_train, X_test, y_test, dataset  = prepareDataForClassificationRF(df)
120/11: X_train, y_train, X_test, y_test, dataset  = prepareDataForClassificationRF(x,y)
120/12:
def prepareDataForClassificationRF(X, y):
    """
    generates categorical output column, which is then used
    to create the train and test data
    """
    
    X_train = X[:200]
    y_train = y[:200]              
    
    X_test = X[200:]    
    y_test = y[200:]
    
    return X_train, y_train, X_test, y_test
120/13: X_train, y_train, X_test, y_test, dataset  = prepareDataForClassificationRF(x,y)
120/14: X_train, y_train, X_test, y_test  = prepareDataForClassificationRF(x,y)
120/15:
RF_Model = RandomForestRegressor(n_estimators=100,
                                 max_features=1, oob_score=True)
labels = y_train#[:, None]
features = X_train[:, None]
rgr=RF_Model.fit(features, labels)
X_test_predict=pd.DataFrame(
    rgr.predict(X_test[:, None])).rename(
    columns={0:'predicted_price'}).set_index('predicted_price')
X_train_predict=pd.DataFrame(
    rgr.predict(X_train[:, None])).rename(
    columns={0:'predicted_price'}).set_index('predicted_price')
RF_predict = X_train_predict.append(X_test_predict)
120/16: df[['price', 'predicted_price']].plot()
120/17: df = df.join((RF_predict.reset_index()))
120/18: df.head()
120/19: df[['Date', 'predicted_price']].plot()
120/20: df[['Throughput', 'predicted_price']].plot()
120/21:
RF_Model = RandomForestRegressor(n_estimators=100,
                                 max_features=1, oob_score=True)
labels = y_train#[:, None]
features = X_train[:, None]
rgr=RF_Model.fit(features, labels)
X_test_predict=pd.DataFrame(
    rgr.predict(X_test[:, None])).rename(
    columns={0:'predicted_throughput'}).set_index('predicted_throughput')
X_train_predict=pd.DataFrame(
    rgr.predict(X_train[:, None])).rename(
    columns={0:'predicted_throughput'}).set_index('predicted_throughput')
RF_predict = X_train_predict.append(X_test_predict)
120/22: df.drop(["predicted_price"],axis=1,inplace=True)
120/23: df = df.join((RF_predict.reset_index()))
120/24: df.head()
120/25: df[['Throughput', 'predicted_price']].plot()
120/26: df[['Throughput', 'predicted_throughput']].plot()
120/27: df['diff']=df.predicted_throughput - df.Throughput
120/28: df.tail()
120/29: df['diff'].plot(kind='bar')
120/30:
#### check R2 ###\n",
r2 = r2_score(y_train[:, None], X_train_predict.reset_index().values)
120/31: r2
121/1:
import pandas as pd
from pandas import read_csv
import numpy as np
from datetime import datetime
import matplotlib.pylab as plt
from statsmodels.tsa.stattools import adfuller
from pandas.plotting import autocorrelation_plot
from statsmodels.tsa.stattools import acf, pacf
121/2:
dateparse = lambda dates: pd.datetime.strptime(dates, '%Y-%m')
data = pd.read_csv('time-series.csv', parse_dates=['Month'], index_col='Month',date_parser=dateparse)
data.head()
121/3:
data = pd.read_csv('time-series.csv')
data.head()
121/4:
plt.plot(data)
plt.show()
121/5:
plt.plot(list(data['Date']),list(data['Throughput']))
plt.show()
121/6:
def test_stationarity(timeseries):
    #Determing rolling statistics
    rolmean = timeseries.rolling(window=12).mean()
    rolstd = timeseries.rolling(window=12).std()

    #Plot rolling statistics:
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)
121/7:
def test_stationarity(timeseries):
    #Determing rolling statistics
    rolmean = timeseries.rolling(window=12).mean()
    rolstd = timeseries.rolling(window=12).std()

    #Plot rolling statistics:
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)

test_stationarity(data)
121/8:
def test_stationarity(timeseries):
    #Determing rolling statistics
    rolmean = timeseries.rolling(window=12).mean()
    rolstd = timeseries.rolling(window=12).std()

    #Plot rolling statistics:
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)

test_stationarity(list(data['Date']),list(data['Throughput']))
121/9:
def test_stationarity(timeseries):
    #Determing rolling statistics
    rolmean = timeseries.rolling(window=12).mean()
    rolstd = timeseries.rolling(window=12).std()

    #Plot rolling statistics:
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)

test_stationarity([list(data['Date']),list(data['Throughput'])])
121/10:
def test_stationarity(timeseries):
    #Determing rolling statistics
    rolmean = timeseries.rolling(window=12).mean()
    rolstd = timeseries.rolling(window=12).std()

    #Plot rolling statistics:
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)

test_stationarity((list(data['Date']),list(data['Throughput'])))
121/11:
def test_stationarity(timeseries):
    #Determing rolling statistics
    rolmean = timeseries.rolling(window=12).mean()
    rolstd = timeseries.rolling(window=12).std()

    #Plot rolling statistics:
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)

test_stationarity({list(data['Date']),list(data['Throughput'])})
121/12: list(data['Date']),list(data['Throughput'])
121/13:
data = pd.read_csv('C:/Users/Dell/Downloads/time-series_check.csv')
data['Date']=pd.to_datetime(data['Date'])
data.head()
121/14:
data = pd.read_csv('C:/Users/Dell/Downloads/time-series_check.csv')
data['Date']=pd.to_datetime(data['Date'])
data.tail()
121/15:
data = pd.read_csv('C:/Users/Dell/Downloads/time-series_check.csv')
data['Date']=pd.to_datetime(data['Date'])
data.head()
121/16:
data = pd.read_csv('C:/Users/Dell/Downloads/time-series_check.csv')
data['Date']=pd.to_datetime(data['Date'])
data.size
121/17:
data = pd.read_csv('C:/Users/Dell/Downloads/time-series_check.csv')
data['Date']=pd.to_datetime(data['Date'])
data.shape
121/18:
data = pd.read_csv('C:/Users/Dell/Downloads/time-series_check.csv')
data['Date']=pd.to_datetime(data['Date'])
data.head()
121/19:
plt.plot(list(data['Date']),list(data['Throughput']))
plt.show()
121/20:
def test_stationarity(timeseries):
    #Determing rolling statistics
    rolmean = timeseries.rolling(window=12).mean()
    rolstd = timeseries.rolling(window=12).std()

    #Plot rolling statistics:
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)

test_stationarity(list(data['Date']),list(data['Throughput']))
121/21:
def test_stationarity(timeseries):
    #Determing rolling statistics
    rolmean = timeseries.rolling(window=12).mean()
    rolstd = timeseries.rolling(window=12).std()

    #Plot rolling statistics:
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)

test_stationarity(data)
121/22:
def difference(dataset, interval=1):
    index = list(dataset.index)
    diff = list()
    for i in range(interval, len(dataset)):
        value = dataset["#Passengers"][i] - dataset["#Passengers"][i - interval]
        diff.append(value)
    return (diff)
121/23:
diff = difference(data)
plt.plot(diff)
plt.show()
121/24:
def difference(dataset, interval=1):
    index = list(dataset.index)
    diff = list()
    for i in range(interval, len(dataset)):
        value = dataset["Throughput"][i] - dataset["Throughput"][i - interval]
        diff.append(value)
    return (diff)
121/25:
diff = difference(data)
plt.plot(diff)
plt.show()
121/26:
ts_log = np.log(data)
plt.title('Log of the data')
plt.plot(ts_log)
plt.show()
121/27: ts_log
121/28:
ts_log = np.log(data)
plt.title('Log of the data')
plt.plot(ts_log)
plt.show()
121/29:
moving_avg = ts_log.rolling(12).mean()
plt.plot(ts_log)
plt.title('12 years of Moving average')
plt.plot(moving_avg, color='blue')
plt.show()
121/30:
def test_stationarity(timeseries):
    #Determing rolling statistics
    rolmean = timeseries.rolling(window=12).mean()
    rolstd = timeseries.rolling(window=12).std()

    #Plot rolling statistics:
#     orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)

test_stationarity(data)
121/31:
def test_stationarity(timeseries):
    #Determing rolling statistics
    rolmean = timeseries.rolling(window=12).mean()
    rolstd = timeseries.rolling(window=12).std()

    #Plot rolling statistics:
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)

test_stationarity(data)
121/32:
data = pd.read_csv('C:/Users/Dell/Downloads/time-series_check.csv')
data['Date']=pd.to_datetime(data['Date'],, index_col='Date')
data.head()
121/33:
data = pd.read_csv('C:/Users/Dell/Downloads/time-series_check.csv')
data['Date']=pd.to_datetime(data['Date'], index_col='Date')
data.head()
121/34:
data = pd.read_csv('C:/Users/Dell/Downloads/time-series_check.csv', index_col='Date')
data['Date']=pd.to_datetime(data['Date'])
data.head()
121/35:
data = pd.read_csv('C:/Users/Dell/Downloads/time-series_check.csv', index_col='Date')
data['Date']=pd.to_datetime(data.indexs)
data.head()
121/36:
data = pd.read_csv('C:/Users/Dell/Downloads/time-series_check.csv', index_col='Date')
data['Date']=pd.to_datetime(data.index)
data.head()
121/37:
data = pd.read_csv('C:/Users/Dell/Downloads/time-series_check.csv', index_col='Date')
data.index=pd.to_datetime(data.index)
data.head()
121/38:
plt.plot(list(data['Date']),list(data['Throughput']))
plt.show()
121/39:
plt.plot(list(dat.index),list(data['Throughput']))
plt.show()
121/40:
plt.plot(list(date.index),list(data['Throughput']))
plt.show()
121/41:
plt.plot(list(data.index),list(data['Throughput']))
plt.show()
121/42:
def test_stationarity(timeseries):
    #Determing rolling statistics
    rolmean = timeseries.rolling(window=12).mean()
    rolstd = timeseries.rolling(window=12).std()

    #Plot rolling statistics:
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.
    plt.show(block=False)

test_stationarity(data)
121/43:
def test_stationarity(timeseries):
    #Determing rolling statistics
    rolmean = timeseries.rolling(window=12).mean()
    rolstd = timeseries.rolling(window=12).std()

    #Plot rolling statistics:
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)

test_stationarity(data)
121/44:
def difference(dataset, interval=1):
    index = list(dataset.index)
    diff = list()
    for i in range(interval, len(dataset)):
        value = dataset["Throughput"][i] - dataset["Throughput"][i - interval]
        diff.append(value)
    return (diff)
121/45:
diff = difference(data)
plt.plot(diff)
plt.show()
121/46:
ts_log = np.log(data)
plt.title('Log of the data')
plt.plot(ts_log)
plt.show()
121/47:
moving_avg = ts_log.rolling(12).mean()
plt.plot(ts_log)
plt.title('12 years of Moving average')
plt.plot(moving_avg, color='blue')
plt.show()
121/48:
moving_avg = ts_log.rolling(12).mean()
plt.plot(ts_log)
plt.title('12 years of Moving average')
plt.plot(moving_avg, color='lemon')
plt.show()
121/49:
moving_avg = ts_log.rolling(12).mean()
plt.plot(ts_log)
plt.title('12 years of Moving average')
plt.plot(moving_avg, color='cyan')
plt.show()
121/50:
moving_avg = ts_log.rolling(12).mean()
plt.plot(ts_log)
plt.title('12 years of Moving average')
plt.plot(moving_avg, color='orange')
plt.show()
121/51:
moving_avg = ts_log.rolling(12).mean()
plt.plot(ts_log)
plt.title('12 years of Moving average')
plt.plot(moving_avg, color='yellow')
plt.show()
121/52:
moving_avg = ts_log.rolling(12).mean()
plt.plot(ts_log)
plt.title('12 years of Moving average')
plt.plot(moving_avg, color='#432D80')
plt.show()
121/53:
moving_avg = ts_log.rolling(12).mean()
plt.plot(ts_log)
plt.title('12 years of Moving average')
plt.plot(moving_avg, color='#422D80')
plt.show()
121/54:
moving_avg = ts_log.rolling(12).mean()
plt.plot(ts_log)
plt.title('12 years of Moving average')
plt.plot(moving_avg, color='#822D80')
plt.show()
121/55:
moving_avg = ts_log.rolling(12).mean()
plt.plot(ts_log)
plt.title('12 years of Moving average')
plt.plot(moving_avg, color='#802D80')
plt.show()
121/56:
moving_avg = ts_log.rolling(12).mean()
plt.plot(ts_log)
plt.title('12 years of Moving average')
plt.plot(moving_avg, color='#802780')
plt.show()
121/57:
moving_avg = ts_log.rolling(12).mean()
plt.plot(ts_log)
plt.title('12 years of Moving average')
plt.plot(moving_avg, color='#142780')
plt.show()
121/58:
moving_avg = ts_log.rolling(12).mean()
plt.plot(ts_log)
plt.title('12 years of Moving average')
plt.plot(moving_avg, color='#169780')
plt.show()
121/59:
moving_avg = ts_log.rolling(12).mean()
plt.plot(ts_log)
plt.title('12 years of Moving average')
plt.plot(moving_avg, color='#369780')
plt.show()
121/60:
moving_avg = ts_log.rolling(12).mean()
plt.plot(ts_log)
plt.title('12 years of Moving average')
plt.plot(moving_avg, color='green')
plt.show()
121/61:
moving_avg = ts_log.rolling(12).mean()
plt.plot(ts_log)
plt.title('12 years of Moving average')
plt.plot(moving_avg, color='red')
plt.show()
121/62: ts_log_moving_avg_diff = ts_log - moving_avg
121/63:
ts_log_moving_avg_diff.dropna(inplace=True)
test_stationarity(ts_log_moving_avg_diff)
121/64:
expwighted_avg = ts_log.ewm(halflife=12).mean()
#parameter halflife is used to define the amount of exponential decay
plt.plot(ts_log)
plt.plot(expwighted_avg, color='red')
plt.show()
121/65:
ts_log_ewma_diff = ts_log - expwighted_avg
test_stationarity(ts_log_ewma_diff)
121/66:
ts_log_diff = ts_log - ts_log.shift()
plt.plot(ts_log_diff)
plt.show()
121/67:
ts_log_diff.dropna(inplace=True)
test_stationarity(ts_log_diff)
121/68:
from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(ts_log)

trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid

plt.subplot(411)
plt.plot(ts_log, label='Original')
plt.legend(loc='best')
plt.subplot(412)
plt.plot(trend, label='Trend')
plt.legend(loc='best')
plt.subplot(413)
plt.plot(seasonal,label='Seasonality')
plt.legend(loc='best')
plt.subplot(414)
plt.plot(residual, label='Residuals')
plt.legend(loc='best')
plt.show()
plt.tight_layout()
121/69:
from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(ts_log)

trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid

plt.subplot(411)
plt.plot(ts_log, label='Original')
plt.legend(loc='best')
plt.subplot(412)
plt.plot(trend, label='Trend')
plt.legend(loc='best')
plt.subplot(413)
plt.plot(seasonal,label='Seasonality')
plt.legend(loc='best')
plt.subplot(414)
plt.plot(residual, label='Residuals')
plt.legend(loc='best')
plt.show()
plt.tight_layout()
121/70:
ts_log_decompose = residual
ts_log_decompose.dropna(inplace=True)
test_stationarity(ts_log_decompose)
121/71:
from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(ts_log)

# trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid

plt.subplot(411)
plt.plot(ts_log, label='Original')
plt.legend(loc='best')
plt.subplot(412)
plt.plot(trend, label='Trend')
plt.legend(loc='best')
plt.subplot(413)
plt.plot(seasonal,label='Seasonality')
plt.legend(loc='best')
plt.subplot(414)
plt.plot(residual, label='Residuals')
plt.legend(loc='best')
plt.show()
plt.tight_layout()
121/72:
from statsmodels.tsa.seasonal import decompose
decomposition = seasonal_decompose(ts_log)

trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid

plt.subplot(411)
plt.plot(ts_log, label='Original')
plt.legend(loc='best')
plt.subplot(412)
plt.plot(trend, label='Trend')
plt.legend(loc='best')
plt.subplot(413)
plt.plot(seasonal,label='Seasonality')
plt.legend(loc='best')
plt.subplot(414)
plt.plot(residual, label='Residuals')
plt.legend(loc='best')
plt.show()
plt.tight_layout()
121/73:
from statsmodels.tsa.seasonal import STL
decomposition = seasonal_decompose(ts_log)

trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid

plt.subplot(411)
plt.plot(ts_log, label='Original')
plt.legend(loc='best')
plt.subplot(412)
plt.plot(trend, label='Trend')
plt.legend(loc='best')
plt.subplot(413)
plt.plot(seasonal,label='Seasonality')
plt.legend(loc='best')
plt.subplot(414)
plt.plot(residual, label='Residuals')
plt.legend(loc='best')
plt.show()
plt.tight_layout()
121/74:
from statsmodels.tsa.seasonal import STL
decomposition = STL(ts_log)

trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid

plt.subplot(411)
plt.plot(ts_log, label='Original')
plt.legend(loc='best')
plt.subplot(412)
plt.plot(trend, label='Trend')
plt.legend(loc='best')
plt.subplot(413)
plt.plot(seasonal,label='Seasonality')
plt.legend(loc='best')
plt.subplot(414)
plt.plot(residual, label='Residuals')
plt.legend(loc='best')
plt.show()
plt.tight_layout()
121/75: ts_log
121/76:
from statsmodels.tsa.arima_model import ARIMA

model = ARIMA(ts_log, order=(2, 1, 0), freq=ts_log.index.inferred_freq)  
results_AR = model.fit(disp=-1)  
plt.plot(ts_log_diff)
plt.plot(results_AR.fittedvalues, color='red')
plt.show()
121/77:
from statsmodels.tsa.arima_model import SARIMA

model = SARIMA(ts_log, order=(2, 1, 0), freq=ts_log.index.inferred_freq)  
results_AR = model.fit(disp=-1)  
plt.plot(ts_log_diff)
plt.plot(results_AR.fittedvalues, color='red')
plt.show()
121/78:
from statsmodels.tsa.arima_model import ARIMA

model = ARIMA(ts_log, order=(2, 1, 0), freq=ts_log.index.inferred_freq)  
results_AR = model.fit(disp=-1)  
plt.plot(ts_log_diff)
plt.plot(results_AR.fittedvalues, color='red')
plt.show()
121/79:
from statsmodels.tsa.arima_model import ARIMA

model = ARIMA(data, order=(2, 1, 0), freq=data.index.inferred_freq)  
results_AR = model.fit(disp=-1)  
plt.plot(ts_log_diff)
plt.plot(results_AR.fittedvalues, color='red')
plt.show()
121/80:
from statsmodels.tsa.arima_model import ARIMA

model = ARIMA(data, order=(2, 1, 0), freq=data.index.inferred_freq)  
results_AR = model.fit(disp=-1)  
plt.plot(ts_log_diff)
plt.plot(results_AR.fittedvalues, color='red')
plt.show()
121/81:
from statsmodels.tsa.arima_model import ARIMA

model = ARIMA(ts_log, order=(2, 1, 0))  
results_AR = model.fit(disp=-1)
plt.plot(ts_log_diff)
plt.plot(results_AR.fittedvalues, color='red')
plt.show()
121/82:
from statsmodels.tsa.arima_model import ARIMA

model = ARIMA(ts_log, order=(5, 1, 2), freq=ts_log.index.inferred_freq)  
results_AR = model.fit(disp=-1)
plt.plot(ts_log_diff)
plt.plot(results_AR.fittedvalues, color='red')
plt.show()
123/1:
a = "How are you"
b = "are you all fine"

for i in a:
    if i in b:
        print(i)
123/2:
a = "How are you"
b = "are you all fine"

for i in a:
    if a in b:
        print(i)
123/3:
a = "How are you"
b = "are you all fine"

for i in a:
    if i in b:
        print(i)
123/4:
a = "How are you"
b = "are you all fine"

for i in a:
    if i in b:
        i
123/5:
a = "How are you"
b = "are you all fine"

for i in a:
    if i in b:
        print(i)
123/6:
a = "How are you".split()
b = "are you all fine".split()

for i in a:
    if i in b:
        print(i)
123/7:
a = "How are you".split()
b = "are you all fine".split()
c = []

for i in a:
    if i in b:
        c.append(i)
c
123/8: a
123/9:
a = "A. (A) A)".split()
b = "A.are you all fine".split()
c = []

for i in a:
    if i in b:
        c.append(i)
c
123/10:
a = "A. (A) A)".split()
b = "A. are you all fine".split()
c = []

for i in a:
    if i in b:
        c.append(i)
c
123/11:
a = "A. (A) A)".split()
b = "A. are you (A) all fine".split()
c = []

for i in a:
    if i in b:
        c.append(i)
c
123/12:
a = "A. (A) A)".split()
b = "A. are you (A) all fine".split()
c = []

for i in a:
    print(i)
    if i in b:
        c.append(i)
c
123/13:
a = "A. (A) A)".split()
b = "A. are you (A) all fine".split()
c = []

for i in a:
    if i in b:
        c.append(i)
c
123/14:
a = "A. (A) A)".split()
b = "A. are you \n (A) all fine".split()
c = []

for i in a:
    if i in b:
        c.append(i)
c
123/15:
a = "A. (A) A)".split()
b = "A. are you \n(A) all fine".split()
c = []

for i in a:
    if i in b:
        c.append(i)
c
124/1:
import camelot

# PDF file to extract tables from
file = "C:/Users/Dell/Downloads/MostlyText/Stats-review1.pdf"
124/2:
# extract all the tables in the PDF file
tables = camelot.read_pdf(file)
124/3:
import ctypes
from ctypes.util import find_library
find_library("".join(("gsdll", str(ctypes.sizeof(ctypes.c_voidp) * 8), ".dll")))
124/4:
# extract all the tables in the PDF file
tables = camelot.read_pdf(file)
123/16:
import PyPDF2
import re

complete=[]

#Assign the pdf file
file_name = "file:///C:/Users/Dell/Downloads/MostlyText/ComparativeGovernment-1test.pdf"
doc = PyPDF2.PdfFileReader(file_name)

#Number of pages
pages = doc.getNumPages()

#List of tuples (all occurrences, page number)
list_pages = []

for i in range(pages):
    current_page = doc.getPage(i)
    text = current_page.extractText()
    print(text)
    complete.append(text)
123/17:
import PyPDF2
import re

complete=[]

#Assign the pdf file
file_name = "C:/Users/Dell/Downloads/MostlyText/ComparativeGovernment-1test.pdf"
doc = PyPDF2.PdfFileReader(file_name)

#Number of pages
pages = doc.getNumPages()

#List of tuples (all occurrences, page number)
list_pages = []

for i in range(pages):
    current_page = doc.getPage(i)
    text = current_page.extractText()
    print(text)
    complete.append(text)
124/5:
# import module
from pdf2image import convert_from_path


# Store Pdf with convert_from_path function
images = convert_from_path('C:/Users/Dell/Downloads/MostlyText/Stats-review1.pdf',500,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

for i in range(len(images)):

    # Save pages as images in the pdf
    images[i].save('page'+ str(i) +'.jpg', 'JPEG')
124/6:
import cv2
import os

folder = 'page/'
main_img = cv2.imread(folder+'template.jpg')

for img_name in os.listdir(folder):
    # Construct old file name
    source = folder + img_name
    
    #Reading perticular image
    nic_img = cv2.imread(source)
    #Template matching
    results = cv2.matchTemplate(nic_img,main_img,cv2.TM_CCOEFF_NORMED)
    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(results)
    w = main_img.shape[1]
    h = main_img.shape[0]
    cv2.rectangle(nic_img,max_loc,(max_loc[0]+w, max_loc[1]+h), (0,255,255), 2)
    x, y = max_loc
    
    again_img = cv2.imread(source)
    
    
    
    crop_img = again_img[y:y+h, x:x+w]
    if not os.path.isdir('Template'):
        os.mkdir('Template')
    cv2.imwrite("Template/"+img_name[:-5]+"_crop.jpg", crop_img)

print('Cropped all')
124/7:
import cv2
import os

folder = 'page/'
main_img = cv2.imread(folder+'template.jpg')

for img_name in os.listdir(folder):
    # Construct old file name
    source = folder + img_name
    
    #Reading perticular image
    nic_img = cv2.imread(source)
    #Template matching
    results = cv2.matchTemplate(nic_img,main_img,cv2.TM_CCOEFF_NORMED)
    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(results)
    w = main_img.shape[1]
    h = main_img.shape[0]
    cv2.rectangle(nic_img,max_loc,(max_loc[0]+w, max_loc[1]+h), (0,255,255), 2)
    x, y = max_loc
    
    again_img = cv2.imread(source)
    
    
    
    crop_img = again_img[y:y+h, x:x+w]
    if not os.path.isdir('Template'):
        os.mkdir('Template')
    cv2.imwrite("Template/"+img_name[:-5]+"_crop.jpg", crop_img)

print('Cropped all')
124/8:
import cv2
import os

folder = 'page/'
main_img = cv2.imread(folder+'template2.jpg')

for img_name in os.listdir(folder):
    # Construct old file name
    source = folder + img_name
    
    #Reading perticular image
    nic_img = cv2.imread(source)
    #Template matching
    results = cv2.matchTemplate(nic_img,main_img,cv2.TM_CCOEFF_NORMED)
    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(results)
    w = main_img.shape[1]
    h = main_img.shape[0]
    cv2.rectangle(nic_img,max_loc,(max_loc[0]+w, max_loc[1]+h), (0,255,255), 2)
    x, y = max_loc
    
    again_img = cv2.imread(source)
    
    
    
    crop_img = again_img[y:y+h, x:x+w]
    if not os.path.isdir('Template'):
        os.mkdir('Template')
    cv2.imwrite("Template/"+img_name[:-5]+"_crop.jpg", crop_img)

print('Cropped all')
124/9:
import cv2
import os

folder = 'page/'
main_img = cv2.imread(folder+'template2.jpg')

for img_name in os.listdir(folder):
    # Construct old file name
    source = folder + img_name
    
    #Reading perticular image
    nic_img = cv2.imread(source)
    #Template matching
    results = cv2.matchTemplate(nic_img,main_img,cv2.TM_CCOEFF_NORMED)
    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(results)
    w = main_img.shape[1]
    h = main_img.shape[0]
    cv2.rectangle(nic_img,max_loc,(max_loc[0]+w, max_loc[1]+h), (0,255,255), 2)
    x, y = max_loc
    
    again_img = cv2.imread(source)
    
    
    
    crop_img = again_img[y:y+h, x:x+w]
    if not os.path.isdir('Template'):
        os.mkdir('Template')
    cv2.imwrite("Template/"+img_name[:-5]+"_crop.jpg", crop_img)

print('Cropped all')
126/1:
# https://www.thepythoncode.com/article/extract-pdf-tables-in-python-camelot
import camelot

# PDF file to extract tables from
file = "C:/Users/Dell/Downloads/MostlyText/Stats-review1.pdf"
126/2:
# extract all the tables in the PDF file
tables = camelot.read_pdf(file)
126/3:
# https://www.thepythoncode.com/article/extract-pdf-tables-in-python-camelot
import camelot

# PDF file to extract tables from
tables = camelot.read_pdf("C:/Users/Dell/Downloads/MostlyText/Stats-review1.pdf",pages="all")
tables
126/4: tables[0].df
126/5: tables[1].df
126/6: tables[2].df
126/7: tables[3].df
126/8: tables[2].df
126/9:
# https://www.thepythoncode.com/article/extract-pdf-tables-in-python-camelot
import camelot

# PDF file to extract tables from
tables = camelot.read_pdf("C:/Users/Dell/Downloads/MostlyText/USHist-test1.pdf",pages="all")
tables
126/10: tables[0].df
126/11:
import fitz # PyMuPDF
import io
from PIL import Image
126/12: tables[0].df
126/13:
# https://www.thepythoncode.com/article/extract-pdf-tables-in-python-camelot
import camelot

# PDF file to extract tables from
tables = camelot.read_pdf("C:/Users/Dell/Downloads/MostlyText/CollegePanda-3tests.pdf",pages="all")
tables
126/14:
# https://www.thepythoncode.com/article/extract-pdf-tables-in-python-camelot
import camelot

# PDF file to extract tables from
tables = camelot.read_pdf("C:/Users/Dell/Downloads/MostlyText/ComparativeGovernment-1test.pdf",pages="all")
tables
126/15: tables[0].df
126/16: tables[1].df
126/17: tables[2].df
126/18: tables[3].df
126/19: tables[1].df
126/20:
from pdf2image import convert_from_path


# Store Pdf with convert_from_path function
images = convert_from_path('file:///C:/Users/Dell/Downloads/MostlyText/CollegePanda-3tests.pdf',500,poppler_path=r'C:\Program Files\poppler-21.11.0\Library\bin')

for i in range(len(images)):

    # Save pages as images in the pdf
    images[i].save('page'+ str(i) +'.jpg', 'JPEG')
126/21:
from pdf2image import convert_from_path


# Store Pdf with convert_from_path function
images = convert_from_path('file:///C:/Users/Dell/Downloads/MostlyText/CollegePanda-3tests.pdf',500,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

for i in range(len(images)):

    # Save pages as images in the pdf
    images[i].save('page'+ str(i) +'.jpg', 'JPEG')
126/22:
from pdf2image import convert_from_path


# Store Pdf with convert_from_path function
images = convert_from_path('C:/Users/Dell/Downloads/MostlyText/CollegePanda-3tests.pdf',500,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

for i in range(len(images)):

    # Save pages as images in the pdf
    images[i].save('page'+ str(i) +'.jpg', 'JPEG')
126/23:
import fitz as ft

file = ft.open("C:/Users/Dell/Downloads/MostlyText/ComparativeGovernment-1test.pdf")
for i in range(len(file)):
    for image in file.getPageImageList(i):
        xref = image[0]
        pix = ft.Pixmap(file, xref)
        if pix.n < 5: #RGB or Gray
            pix.writePNG("I%s-%s.png" % (i, xref))

        else:
            pixy = ft.Pixmap(ft.csRGB, pic)
            pixy.writePNG("I%s-%s.png" % (i, xref))
            pixy = None
        pix = None
print("Image Extraction from provided pdf is complete")
126/24: tables[0].df
126/25: tables[1].df
126/26: tables[3].df
130/1:
import PyPDF2 # Library for Text extraction

raw_text = ""

file_name = "C:/Users/Dell/Downloads/MostlyText/Stats-review1.pdf" #Assigning pdf

doc = PyPDF2.PdfFileReader(file_name) #Alloted pdf to file reader


pages = doc.getNumPages()  #Number of pages


for i in range(pages):
    current_page = doc.getPage(i) # Page Selection
    text = current_page.extractText() # Page Text Extraction
    raw_text = raw_text + text + '\n'
130/2: print(raw_text)
130/3: print(f[262:473])
130/4: print(raw_text[262:473])
132/1:
import camelot

# fetchinh PDF file to extract tables from
tables = camelot.read_pdf(file_name,pages="all")
tables
132/2:
import PyPDF2 # Library for Text extraction

raw_text = ""

file_name = "C:/Users/Dell/Downloads/MostlyText/Stats-review1.pdf" #Assigning pdf

doc = PyPDF2.PdfFileReader(file_name) #Alloted pdf to file reader


pages = doc.getNumPages()  #Number of pages


for i in range(pages):
    current_page = doc.getPage(i) # Page Selection
    text = current_page.extractText() # Page Text Extraction
    raw_text = raw_text + text + '\n' # All text of page is saved in a string
132/3: print(raw_text)
132/4: print(raw_text[262:473])
132/5:
count = 0 # To reach new question, we use counter
output = ""
while True:
    count+=1 # Next question
    
    if f.find('E.') != -1: # Until you find E
        ends = f.find('E.') + f[f.find('E.'):].find('\n') # Select text till option E where text ends
        
        output = output + f[f.find(str(count)+'.'):ends]+'\n\n' # Add to the final output
        f = f[ends:] # Neglect the upper text

    elif f.find('D.') != -1: # Until you find D if E if not available
        ends = f.find('D.') + f[f.find('D.'):].find('\n') # Select text till option D where text ends

        output = output + f[f.find(str(count)+'.'):ends]+'\n\n' # Add to the final output
        f = f[ends:] # Neglect the upper text

    elif f.find('C.') != -1: # Until you find C if D if not available
        ends = f.find('C.') + f[f.find('C.'):].find('\n') # Select text till option C where text ends

        output = output + f[f.find(str(count)+'.'):ends]+'\n\n' # Add to the final output
        f = f[ends:] # Neglect the upper text
        
    else: 
        break

print(output.rstrip()) #Ignore extra space from right
132/6:
count = 0 # To reach new question, we use counter
output = ""
while True:
    count+=1 # Next question
    
    if raw_text.find('E.') != -1: # Until you find E
        ends = raw_text.find('E.') + raw_text[raw_text.find('E.'):].find('\n') # Select text till option E where text ends
        
        output = output + raw_text[raw_text.find(str(count)+'.'):ends]+'\n\n' # Add to the final output
        f = raw_text[ends:] # Neglect the upper text

    elif raw_text.find('D.') != -1: # Until you find D if E if not available
        ends = raw_text.find('D.') + raw_text[raw_text.find('D.'):].find('\n') # Select text till option D where text ends

        output = output + raw_text[raw_text.find(str(count)+'.'):ends]+'\n\n' # Add to the final output
        f = raw_text[ends:] # Neglect the upper text

    elif raw_text.find('C.') != -1: # Until you find C if D if not available
        ends = raw_text.find('C.') + raw_text[raw_text.find('C.'):].find('\n') # Select text till option C where text ends

        output = output + raw_text[raw_text.find(str(count)+'.'):ends]+'\n\n' # Add to the final output
        f = raw_text[ends:] # Neglect the upper text
        
    else: 
        break

print(output.rstrip()) #Ignore extra space from right
132/7:
import PyPDF2 # Library for Text extraction

raw_text = ""

file_name = "C:/Users/Dell/Downloads/MostlyText/Stats-review1.pdf" #Assigning pdf

doc = PyPDF2.PdfFileReader(file_name) #Alloted pdf to file reader


pages = doc.getNumPages()  #Number of pages


for i in range(pages):
    current_page = doc.getPage(i) # Page Selection
    text = current_page.extractText() # Page Text Extraction
    raw_text = raw_text + text + '\n' # All text of page is saved in a string
132/8: print(raw_text)
132/9: print(raw_text[262:473])
132/10:
count = 0 # To reach new question, we use counter
output = ""
while True:
    count+=1 # Next question
    
    if raw_text.find('E.') != -1: # Until you find E
        ends = raw_text.find('E.') + raw_text[raw_text.find('E.'):].find('\n') # Select text till option E where text ends
        
        output = output + raw_text[raw_text.find(str(count)+'.'):ends]+'\n\n' # Add to the final output
        raw_text = raw_text[ends:] # Neglect the upper text

    elif raw_text.find('D.') != -1: # Until you find D if E if not available
        ends = raw_text.find('D.') + raw_text[raw_text.find('D.'):].find('\n') # Select text till option D where text ends

        output = output + raw_text[raw_text.find(str(count)+'.'):ends]+'\n\n' # Add to the final output
        raw_text = raw_text[ends:] # Neglect the upper text

    elif raw_text.find('C.') != -1: # Until you find C if D if not available
        ends = raw_text.find('C.') + raw_text[raw_text.find('C.'):].find('\n') # Select text till option C where text ends

        output = output + raw_text[raw_text.find(str(count)+'.'):ends]+'\n\n' # Add to the final output
        raw_text = raw_text[ends:] # Neglect the upper text
        
    else: 
        break

print(output.rstrip()) #Ignore extra space from right
132/11:
import camelot

# fetchinh PDF file to extract tables from
tables = camelot.read_pdf(file_name,pages="all")
tables
132/12: table[0].df
132/13: tables[0].df
132/14: tables[1].df
133/1:
import fitz as ft
import PyPDF2

file = PyPDF2.PdfFileReader(file_name) #Alloted pdf to file reader

# file = ft.open(file_name)
for i in range(len(file)):
    for image in file.getNumPages():
        xref = image[0]
        pix = ft.Pixmap(file, xref)
        if pix.n < 5: #RGB or Gray
            pix.writePNG("I%s-%s.png" % (i, xref))

        else:
            pixy = ft.Pixmap(ft.csRGB, pic)
            pixy.writePNG("I%s-%s.png" % (i, xref))
            pixy = None
        pix = None
print("Image Extraction from provided pdf is complete")
132/15:
import fitz as ft
import PyPDF2

file = PyPDF2.PdfFileReader(file_name) #Alloted pdf to file reader

# file = ft.open(file_name)
for i in range(len(file)):
    for image in file.getNumPages():
        xref = image[0]
        pix = ft.Pixmap(file, xref)
        if pix.n < 5: #RGB or Gray
            pix.writePNG("I%s-%s.png" % (i, xref))

        else:
            pixy = ft.Pixmap(ft.csRGB, pic)
            pixy.writePNG("I%s-%s.png" % (i, xref))
            pixy = None
        pix = None
print("Image Extraction from provided pdf is complete")
132/16:
import fitz as ft
import PyPDF2

file = PyPDF2.PdfFileReader(file_name) #Alloted pdf to file reader
file2 = ft.open(file_name)

for i in range(len(file2)):
    for image in file.getNumPages():
        xref = image[0]
        pix = ft.Pixmap(file, xref)
        if pix.n < 5: #RGB or Gray
            pix.writePNG("I%s-%s.png" % (i, xref))

        else:
            pixy = ft.Pixmap(ft.csRGB, pic)
            pixy.writePNG("I%s-%s.png" % (i, xref))
            pixy = None
        pix = None
print("Image Extraction from provided pdf is complete")
132/17: file.getNumPages()
132/18: file.getImageBbox()
132/19:
import fitz as ft
import PyPDF2

# file = PyPDF2.PdfFileReader(file_name) #Alloted pdf to file reader
file = ft.open(file_name)

for i in range(len(file2)):
    for image in file..getImageBbox():
        xref = image[0]
        pix = ft.Pixmap(file, xref)
        if pix.n < 5: #RGB or Gray
            pix.writePNG("I%s-%s.png" % (i, xref))

        else:
            pixy = ft.Pixmap(ft.csRGB, pic)
            pixy.writePNG("I%s-%s.png" % (i, xref))
            pixy = None
        pix = None
print("Image Extraction from provided pdf is complete")
132/20:
import fitz as ft
import PyPDF2

# file = PyPDF2.PdfFileReader(file_name) #Alloted pdf to file reader
file = ft.open(file_name)

for i in range(len(file2)):
    for image in file.getImageBbox():
        xref = image[0]
        pix = ft.Pixmap(file, xref)
        if pix.n < 5: #RGB or Gray
            pix.writePNG("I%s-%s.png" % (i, xref))

        else:
            pixy = ft.Pixmap(ft.csRGB, pic)
            pixy.writePNG("I%s-%s.png" % (i, xref))
            pixy = None
        pix = None
print("Image Extraction from provided pdf is complete")
132/21:
import fitz as ft
import PyPDF2

# file = PyPDF2.PdfFileReader(file_name) #Alloted pdf to file reader
file = ft.open(file_name)

for i in range(len(file)):
    for image in file.getPageFontList():
        xref = image[0]
        pix = ft.Pixmap(file, xref)
        if pix.n < 5: #RGB or Gray
            pix.writePNG("I%s-%s.png" % (i, xref))

        else:
            pixy = ft.Pixmap(ft.csRGB, pic)
            pixy.writePNG("I%s-%s.png" % (i, xref))
            pixy = None
        pix = None
print("Image Extraction from provided pdf is complete")
132/22:
import fitz as ft
import PyPDF2

# file = PyPDF2.PdfFileReader(file_name) #Alloted pdf to file reader
file = ft.open(file_name)

for i in range(len(file)):
    for image in file.getPageFontList():
        xref = image[0]
        pix = ft.Pixmap(file, xref)
        if pix.n < 5: #RGB or Gray
            pix.writePNG("I%s-%s.png" % (i, xref))

        else:
            pixy = ft.Pixmap(ft.csRGB, pic)
            pixy.writePNG("I%s-%s.png" % (i, xref))
            pixy = None
        pix = None
print("Image Extraction from provided pdf is complete")
132/23:
import fitz as ft
import PyPDF2

# file = PyPDF2.PdfFileReader(file_name) #Alloted pdf to file reader
file2 = ft.open(file_name)

for i in range(len(file)):
    for image in file.getPageImageList():
        xref = image[0]
        pix = ft.Pixmap(file, xref)
        if pix.n < 5: #RGB or Gray
            pix.writePNG("I%s-%s.png" % (i, xref))

        else:
            pixy = ft.Pixmap(ft.csRGB, pic)
            pixy.writePNG("I%s-%s.png" % (i, xref))
            pixy = None
        pix = None
print("Image Extraction from provided pdf is complete")
132/24:
import matplotlib.pyplot as plt

temp = cv2.imread('Template/template_crop.jpg')
resl = cv2.imread('Template/page_crop.jpg')
plt.show(temp)
132/25:
import cv2
import matplotlib.pyplot as plt

temp = cv2.imread('Template/template_crop.jpg')
resl = cv2.imread('Template/page_crop.jpg')
plt.show(temp)
132/26:
import cv2
import matplotlib.pyplot as plt

temp = cv2.imread('Template/template_crop.jpg')
temp = cv2.
resl = cv2.imread('Template/page_crop.jpg')
plt.im?show(temp)
132/27:
import cv2
import matplotlib.pyplot as plt

temp = cv2.imread('Template/template_crop.jpg')
# temp = cv2.
resl = cv2.imread('Template/page_crop.jpg')
plt.imshow(temp)
132/28:
import cv2
import matplotlib.pyplot as plt

temp = cv2.imread('Template/template_crop.jpg')
# temp = cv2.
resl = cv2.imread('Template/page_crop.jpg')
print('This is the template')
plt.imshow(temp)
132/29:
print('This is the result,')
plt.imshow(resl)
132/30:
print('This is the result, Weak approach')
plt.imshow(resl)
142/1:
import pandas as pd
import numpy as np
data = pd.read_csv('data.csv')
data.head()
142/2: data.shape
142/3:
data = pd.read_csv('data2.csv')
data.shape
142/4: data.head(2)
142/5:
import pandas as pd
import numpy as np
data = pd.read_csv('combine.csv')
data.head()
142/6: data.shape
142/7:
import seaborn as sns
corr = data.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="YlOrRd")
142/8: data['Date'][0]
142/9: data['Date'][0][4]
142/10: data['Date'][4]
142/11: data['Date'][][4]
142/12: data['Date'][4]
142/13: data['Date']
142/14: data['Date'][7]
142/15: data['Date'][45]
142/16: data['Date'][90
142/17: data['Date'][90]
142/18: data['Date'][84]
142/19: data['Date'][89]
142/20: data['Date'][87]
142/21: data['Date'][88]
142/22: data['Date'][112]
142/23: data['Date'][119]
142/24: data['Date'][114]
142/25: data['Date'][118]
142/26: data['Date'][117]
142/27: data['Date'][88:118]
142/28: april = data['Date'][88:118]
142/29:
corr = april.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="blue")
142/30: april = data[data['Date'][88:118]]
142/31: data[data['Date'][88:118]]
142/32: data['Date'][88:118]
142/33: pd.DataFrame(data['Date'][88:118])
142/34: np.where(data['Date'][117])
142/35: np.where(data['Date'][88:118])
142/36: data(data['Date'][88:118])
142/37: data['Date'][88:118]
142/38: data[data['Date'][88:118]]
142/39: data['Date'][88:118]
142/40: data.loc[(data['Date'][88:118])]
142/41: data.loc[data['Date'][88:118]]
142/42: data.loc(data['Date'][88:118])
142/43: data.loc[data['Date'][88:118]]
142/44: data.iloc[data['Date'][88:118]]
142/45: data.loc[data['Date'][88:118]]
142/46: data.loc[data['Date'][88:118].isin(['Date'])]
142/47: data.loc[data['Date'][88:118]]
142/48: data['Date'][88:118]
142/49: data['Date'].isin(['Date'][88:118])
142/50: data[data['Date'].isin(['Date'][88:118])]
142/51: data[data['Date'].items(['Date'][88:118])]
142/52: data[data['Date'].(['Date'][88:118])]
142/53: data[data['Date']['Date'][88:118]]
142/54: data[data['Date'][88:118]]
142/55: data['Date'][88:118]
142/56:
a = []
if data['Date'][88:118] in data['Date']:
    a.append(True)
else:
    a.append(False)
142/57:
a = []
for i in data['Date'][88:118]
    if i in data['Date']:
        a.append(True)
    else:
        a.append(False)
142/58:
a = []
for i in data['Date'][88:118]:
    if i in data['Date']:
        a.append(True)
    else:
        a.append(False)
142/59: a
142/60: data['Date'][88:118]
142/61:
mask = (df['date'] > '01.04.2021') & (data['date'] <= '01.04.2021')
print(data.loc[mask])
142/62:
mask = (data['date'] > '01.04.2021') & (data['date'] <= '01.04.2021')
print(data.loc[mask])
142/63:
mask = (data['Date'] > '01.04.2021') & (data['Date'] <= '01.04.2021')
print(data.loc[mask])
142/64:
mask = (data['Date'] > '2021-04-01') & (data['Date'] <= '2021-04-30')
print(data.loc[mask])
142/65:
mask = (data['Date'] > '01-04-2021') & (data['Date'] <= '30-04-2021')
print(data.loc[mask])
142/66:
mask = data(data['Date'] > '01-04-2021') & (data['Date'] <= '30-04-2021')
print(data.loc[mask])
142/67:
mask = data[(data['Date'] > '01-04-2021') & (data['Date'] <= '30-04-2021')]
print(data.loc[mask])
142/68:
mask = data[(data['Date'] > '01-04-2021') & (data['Date'] <= '30-04-2021')]
mask
# print(data.loc[mask])
142/69:
mask = data[(data['Date'] > '01-04-2021') & (data['Date'] <= '30-04-2021')]
mask[-1]
# print(data.loc[mask])
142/70:
mask = data[(data['Date'] > '01-04-2021') & (data['Date'] <= '30-04-2021')]
mask[:-1]
# print(data.loc[mask])
142/71:
mask = data[(data['Date'] > '01-04-2021') & (data['Date'] <= '30-04-2021')]
mask
# print(data.loc[mask])
142/72:
mask = data[(data['Date'] > '04-01-2021') & (data['Date'] <= '04-30-2021')]
mask
# print(data.loc[mask])
142/73:
mask = data[(data['Date'] > '01-04-2021') & (data['Date'] <= '30-04-2021')]
mask
# print(data.loc[mask])
142/74:
mask = data[(data['Date'] > '01-04-2021') & (data['Date'] <= '30-04-2021')]
mask
# print(data.loc[mask])
142/75:
mask = data[(data['Date'] > '01-04-2021') & (data['Date'] <= '30-04-2021')]
# print(data.loc[mask])
142/76:
mask = data[(data['Date'] > '01-04-2021') & (data['Date'] <= '30-04-2021')]
mask
# print(data.loc[mask])
142/77:
mask = data[(data['Date'] > '04-01-2021') & (data['Date'] <= '04-30-2021')]
mask
# print(data.loc[mask])
142/78:
mask = data[(data['Date'] > '01-04-2021') & (data['Date'] <= '30-04-2021')]
mask
# print(data.loc[mask])
142/79: data.iloc[:,data['Date'][88:118]]
142/80: data.iloc[data['Date'][88:118],:]
142/81: data.iloc[:,data['Date'][88:118]]
142/82: data.loc[:,data['Date'][88:118]]
142/83: data.iloc[data['Date'][88:118]]
142/84: data.iloc[:,data['Date'][88:118]]
142/85: data.iloc[data['Date'][88:118]]
142/86: data[data['Date'][88:118]]
142/87:
# data.iloc[data['Date'][88:118]]
data[list(data.columns[88:118])
142/88:
# data.iloc[data['Date'][88:118]]
data[list(data.columns[88:118])]
142/89:
# data.iloc[data['Date'][88:118]]
data[data[list(data.columns[88:118])]]
142/90:
# data.iloc[data['Date'][88:118]]
data[:,list(data.columns[88:118])]
142/91:
# data.iloc[data['Date'][88:118]]
data[list(data.columns[88:118])]
142/92:
# data.iloc[data['Date'][88:118]]
data[:,""(data.columns[88:118])]
142/93:
april = pd.read_csv('combine - Copy.csv')
data.head()
142/94:
april = pd.read_csv('combine - Copy.csv')
april.head()
142/95:
corr = april.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="blue")
142/96:
corr = april.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="BuGn_r")
142/97:
april = pd.read_csv('combine - Copy.csv')
april.drop('remarks2',axis=1,inplace=True)
april.head()
142/98:
corr = april.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="BuGn_r")
142/99:
corr = april.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="blue")
142/100:
corr = april.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="coolwarm_r")
142/101:
corr = april.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="copper")
142/102:
corr = april.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="plasma")
142/103:
corr = april.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="coolwarm_r")
142/104:
corr = april.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="copper")
142/105:
corr = april.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="fsdf")
142/106:
corr = april.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="gist_gray_r")
142/107:
corr = april.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="green")
142/108:
corr = april.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="icefire")
142/109:
corr = april.corr()
sns.heatmap(corr, annot=True, vmax=1.0, vmin=0.0, cmap="bone_r")
136/1:
import os
file_path = r'C:\Users\Dell\Downloads\MostlyText'
for i in os.path.dir(file_path):
    print(i)
136/2:
import os
file_path = r'C:\Users\Dell\Downloads\MostlyText'
for i in os.ispath.dir(file_path):
    print(i)
136/3:
import os
file_path = r'C:\Users\Dell\Downloads\MostlyText'
for i in os.listdir(file_path):
    print(i)
136/4:
import os
file_path = 'C:/Users/Dell/Downloads/MostlyText/'
for i in os.listdir(file_path):
    print(i)
136/5:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/MostlyText/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,500,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir(x[:-4]):
            os.mkdir(x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save('/'+ str(i) +'.jpg', 'JPEG')
136/6:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/MostlyText/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,500,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir(x[:-4]):
            os.mkdir(x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save(x + '/'+ str(i) +'.jpg', 'JPEG')
136/7:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/MostlyText/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,500,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir(x[:-4]):
            os.mkdir(x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save(x[:-4] + '/'+ str(i) +'.jpg', 'JPEG')
136/8:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/Found on internet/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,500,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir(x[:-4]):
            os.mkdir(x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save(x[:-4] + '/'+ str(i) +'.jpg', 'JPEG')
145/1:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/textsamples2/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,500,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir(x[:-4]):
            os.mkdir(x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save(x[:-4] + '/'+ str(i) +'.jpg', 'JPEG')
146/1:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/textsamples2/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,110,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir(x[:-4]):
            os.mkdir(x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save(x[:-4] + '/'+ str(i) +'.jpg', 'JPEG')
146/2:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/textsamples2/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,130,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir(x[:-4]):
            os.mkdir(x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save(x[:-4] + '/'+ str(i) +'.jpg', 'JPEG')
146/3:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/textsamples2/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,120,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir(x[:-4]):
            os.mkdir(x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save(x[:-4] + '/'+ str(i) +'.jpg', 'JPEG')
146/4:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/MostlyText/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,120,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir(x[:-4]):
            os.mkdir(x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save(x[:-4] + '/'+ str(i) +'.jpg', 'JPEG')
146/5:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/Found on internet/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,120,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir(x[:-4]):
            os.mkdir(x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save(x[:-4] + '/'+ str(i) +'.jpg', 'JPEG')
146/6:
import os
file_path = 'C:/Users/Dell/pdfs/*/'

for x in os.listdir(file_path):
    print(x)
146/7:
import os
file_path = 'C:/Users/Dell/pdfs/'

for x in os.listdir(file_path):
    for y in os.listdir(file_path+x):
        print(y)
146/8:
import os
file_path = 'C:/Users/Dell/pdfs/'

dest = 'C:/Users/Dell/pdf_resized' #the path where to save resized images
# create new folder
if not os.path.exists(dest):
    os.makedirs(dest)

counter = 0
for x in os.listdir(file_path):
    for y in os.listdir(file_path+x):
        counter+=1
        print(y)
        img = Image.open(file_path+x+'/'+y)
        img=img.convert('RGB').resize((960,1300))
        
        # save resized images to new folder with existing filename
        img.save('{}{}{}.jpg'.format(dest,'/',counter))
146/9:
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True
from PIL import Image

import os
file_path = 'C:/Users/Dell/pdfs/'

dest = 'C:/Users/Dell/pdf_resized' #the path where to save resized images
# create new folder
if not os.path.exists(dest):
    os.makedirs(dest)

counter = 0
for x in os.listdir(file_path):
    for y in os.listdir(file_path+x):
        counter+=1
        print(y)
        img = Image.open(file_path+x+'/'+y)
        img=img.convert('RGB').resize((960,1300))
        
        # save resized images to new folder with existing filename
        img.save('{}{}{}.jpg'.format(dest,'/',counter))
146/10:
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True
from PIL import Image

import os
file_path = 'C:/Users/Dell/MCQs extraction/pdf_resized/'

dest = 'C:/Users/Dell/MCQs extraction/rename/'

counter = 0
for x in os.listdir(file_path):
    counter+=1
    print(x)
    img = Image.open(file_path+x)

    # save resized images to new folder with existing filename
    img.save('{}{}{}.jpg'.format(dest,'/',counter))
154/1:
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True
from PIL import Image

import os
file_path = 'C:/Users/Dell/pdfs/'

dest = 'C:/Users/Dell/pdf_resized' #the path where to save resized images
# create new folder
if not os.path.exists(dest):
    os.makedirs(dest)

counter = 0
for x in os.listdir(file_path):
    for y in os.listdir(file_path+x):
        counter+=1
        print(y)
        img = Image.open(file_path+x+'/'+y)
        img=img.convert('RGB').resize((960,1280))
        
        # save resized images to new folder with existing filename
        img.save('{}{}{}.jpg'.format(dest,'/',counter))
155/1:
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True
from PIL import Image

import os
file_path = 'C:/Users/Dell/MCQs extraction/pdfs/'

dest = 'C:/Users/Dell/MCQs extraction/pdf_resized' #the path where to save resized images
# create new folder
if not os.path.exists(dest):
    os.makedirs(dest)

counter = 0
for x in os.listdir(file_path):
    for y in os.listdir(file_path+x):
        counter+=1
        print(y)
        img = Image.open(file_path+x+'/'+y)
        img=img.convert('RGB').resize((960,1300))
        
        # save resized images to new folder with existing filename
        img.save('{}{}{}.jpg'.format(dest,'/',counter))
155/2:
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True
from PIL import Image

import os
file_path = 'C:/Users/Dell/MCQs extraction/pdfs/'

dest = 'C:/Users/Dell/MCQs extraction/pdf_resized' #the path where to save resized images
# create new folder
if not os.path.exists(dest):
    os.makedirs(dest)

counter = 0
for x in os.listdir(file_path):
    for y in os.listdir(file_path+x):
        counter+=1
        print(y)
        img = Image.open(file_path+x+'/'+y)
        img=img.convert('RGB').resize((960,1280))
        
        # save resized images to new folder with existing filename
        img.save('{}{}{}.jpg'.format(dest,'/',counter))
156/1:
import os
os.listdir
156/2:
import os
os.listdir()
156/3:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/textsamples2/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,150,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir('pdfs/' + x[:-4]):
            os.mkdir('pdfs/' + x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save('pdfs/' + x[:-4] + '/'+ str(i) +'.jpg', 'JPEG')
156/4:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/textsamples2/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,200,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir('pdfs/' + x[:-4]):
            os.mkdir('pdfs/' + x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save('pdfs/' + x[:-4] + '/'+ str(i) +'.jpg', 'JPEG')
156/5:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/MostlyText/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,200,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir('pdfs/' + x[:-4]):
            os.mkdir('pdfs/' + x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save('pdfs/' + x[:-4] + '/'+ str(i) +'.jpg', 'JPEG')
156/6:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/Found on internet/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,200,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir('pdfs/' + x[:-4]):
            os.mkdir('pdfs/' + x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save('pdfs/' + x[:-4] + '/'+ str(i) +'.jpg', 'JPEG')
156/7:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/Mostly Text 3/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,200,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir('pdfs/' + x[:-4]):
            os.mkdir('pdfs/' + x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save('pdfs/' + x[:-4] + '/'+ str(i) +'.jpg', 'JPEG')
156/8:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/sample4/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,200,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir('pdfs/' + x[:-4]):
            os.mkdir('pdfs/' + x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save('pdfs/' + x[:-4] + '/'+ str(i) +'.jpg', 'JPEG')
156/9:
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True
from PIL import Image

import os
file_path = 'C:/Users/Dell/MCQs extraction/pdfs/'

dest = 'C:/Users/Dell/MCQs extraction/pdf_resized' #the path where to save resized images
# create new folder
if not os.path.exists(dest):
    os.makedirs(dest)

counter = 0
for x in os.listdir(file_path):
    for y in os.listdir(file_path+x):
        counter+=1
        print(y)
        img = Image.open(file_path+x+'/'+y)
        img=img.convert('RGB').resize((1472,1952))
        
        # save resized images to new folder with existing filename
        img.save('{}{}{}.jpg'.format(dest,'/',counter))
159/1: data = pd.read_csv('testing(3months).csv')
159/2:
import numpy as np
import pandas as pd
159/3: data = pd.read_csv('testing(3months).csv')
159/4: data['Timestamp'][0].split(' ')[0]
159/5: data.head(3)
159/6:
for i in range(len(data)):
    data['Timestamp'][i] = data['Timestamp'][i].split(' ')[0]

data.head()
159/7: data = pd.read_csv('testing(3months).csv')
159/8: data['Timestamp'][0].split(' ')[0]
159/9:
for i in range(len(data)):
    data['Timestamp'][i] = data['Timestamp'][i].split(' ')[0]

data.head()
159/10: ip_starting = data.loc[0::2,'Inlet Pressure (Bars)'] #Starting
159/11: it_starting = data.loc[0::2,'Inlet Temperature (Degree Celsius)'] #Starting
159/12: ot_starting = data.loc[0::2,'Outlet Temperature (Degree Celsius)'] #Starting
164/1: import easyocr
164/2: import easyocr
164/3:
import easyocr
import cv2
import matplotlib.pyplot as plt
import numpy as np
164/4:
reader = easyocr.Reader[['en'], gpu=False]
result = reader.readtext('checking.jpg')
result
164/5:
reader = easyocr.Reader[['en'], gpu = False]
result = reader.readtext('checking.jpg')
result
164/6:
reader = easyocr.Reader(['en'], gpu=False)
result = reader.readtext('checking.jpg')
result
164/7:
for i in len(result):
    print(result[i][4])
164/8:
for i in range(len(result)):
    print(result[i][4])
164/9: type(result)
164/10: type(result[0])
164/11: result[0][4]
164/12: result[0][3]
164/13: result[0]
164/14: result[0][1]
164/15:
for i in range(len(result)):
    print(result[i][1])
164/16:
reader2 = easyocr.Reader(['en'], gpu=False)
result2 = reader.readtext('1596 - Copy.jpg')

for i in range(len(result2)):
    print(result2[i][1])
164/17:
reader2 = easyocr.Reader(['en'], gpu=False)
result2 = reader.readtext('1596 - Copy.jpg')

for i in range(len(result2)):
    print(result2[i][1])
164/18:
reader2 = easyocr.Reader(['en'], gpu=False)
result2 = reader.readtext('1596 - Copy.jpg')

for i in range(len(result2)):
    print(result2[i][1])
164/19:
reader = easyocr.Reader(['en'], gpu=False)
result3 = reader.readtext('1596.jpg')

for i in range(len(result3)):
    print(result3[i][1])
164/20:
reader = easyocr.Reader(['en'], gpu=False)
result = reader.readtext('checking.jpg')
result
164/21:
for i in range(len(result)):
    print(result[i][1])
164/22:
reader = easyocr.Reader(['en'], gpu=False)
result3 = reader.readtext('new.jpg')

for i in range(len(result3)):
    print(result3[i][1])
164/23: plt.show('new.jpg')
164/24: plt.imshow('new.jpg')
164/25:

plt.imshow(cv2.imread('new.jpg'))
164/26:
reader = easyocr.Reader(['en'], gpu=False)
result3 = reader.readtext('new.jpg')
text = ''
for i in range(len(result3)):
    text += result3[i][1]
#     print(result3[i][1])
text
164/27:
plt.imshow(cv2.imread('new.jpg'))
reader = easyocr.Reader(['en'], gpu=False)
result3 = reader.readtext('new.jpg')
text = ''
for i in range(len(result3)):
    text += result3[i][1]
#     print(result3[i][1])
text
164/28:
plt.imshow(cv2.imread('142.jpg'))
reader = easyocr.Reader(['en'], gpu=False)
result3 = reader.readtext('142.jpg')
text = ''
for i in range(len(result3)):
    text += result3[i][1]
#     print(result3[i][1])
text
164/29:
file_name = '480.jpg'
plt.imshow(cv2.imread(file_name))
reader = easyocr.Reader(['en'], gpu=False)
result4 = reader.readtext(file_name)
text = ''
for i in range(len(result4)):
    text += result4[i][1]
text
169/1:
import easyocr
import cv2
import matplotlib.pyplot as plt
import numpy as np
169/2:
file_name = 'pic_mcq.jpg'
plt.imshow(cv2.imread(file_name))
reader = easyocr.Reader(['en'], gpu=False)
result4 = reader.readtext(file_name)
text = ''
for i in range(len(result4)):
    text = text + result4[i][1] + '\n\n'
text
169/3:
file_name = 'pic_mcq.jpg'
plt.imshow(cv2.imread(file_name))
reader = easyocr.Reader(['en'], gpu=False)
result4 = reader.readtext(file_name)
text = ''
for i in range(len(result4)):
    text = text + result4[i][1] + '\n'
text
169/4: print(text)
169/5:
import cv2

def preprocess(img):
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img_blur = cv2.GaussianBlur(img_gray, (5, 5), 1)
    img_canny = cv2.Canny(img_blur, 50, 50)
    return img_canny

def get_roi(img, pad=3):
    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
    max_area = 0
    
    for cnt in contours:
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        x, y, w, h = cv2.boundingRect(approx)
        rect_area = w * h
        if rect_area > max_area:
            max_area = rect_area
            dim = x, y, w, h

    if max_area:
        x, y, w, h = dim
        return x - pad, y - pad, w + pad * 2, h + pad * 2 

img = cv2.imread("pic_mcq.jpg")

img_processed = preprocess(img)
x, y, w, h = get_roi(img_processed)

cv2.imshow("Image", img[y:y + h, x:x + w])
cv2.waitKey(0)
169/6:
import cv2

def preprocess(img):
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img_blur = cv2.GaussianBlur(img_gray, (5, 5), 1)
    img_canny = cv2.Canny(img_blur, 50, 50)
    return img_canny

def get_roi(img, pad=3):
    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
    max_area = 0
    
    for cnt in contours:
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        x, y, w, h = cv2.boundingRect(approx)
        rect_area = w * h
        if rect_area > max_area:
            max_area = rect_area
            dim = x, y, w, h

    if max_area:
        x, y, w, h = dim
        return x - pad, y - pad, w + pad * 2, h + pad * 2 

img = cv2.imread("pic_mcq.jpg")

img_processed = preprocess(img)
x, y, w, h = get_roi(img_processed)

cv2.imshow("Image", img[y:y + h, x:x + w])
cv2.waitKey(0)
170/1:
import cv2

def preprocess(img):
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img_blur = cv2.GaussianBlur(img_gray, (5, 5), 1)
    img_canny = cv2.Canny(img_blur, 50, 50)
    return img_canny

def get_roi(img, pad=3):
    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
    max_area = 0
    
    for cnt in contours:
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        x, y, w, h = cv2.boundingRect(approx)
        rect_area = w * h
        if rect_area > max_area:
            max_area = rect_area
            dim = x, y, w, h

    if max_area:
        x, y, w, h = dim
        return x - pad, y - pad, w + pad * 2, h + pad * 2 

img = cv2.imread("pic_mcq.jpg")

img_processed = preprocess(img)
x, y, w, h = get_roi(img_processed)

cv2.imshow("Image", img[y:y + h, x:x + w])
cv2.waitKey(0)
171/1:
import cv2

def preprocess(img):
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img_blur = cv2.GaussianBlur(img_gray, (5, 5), 1)
    img_canny = cv2.Canny(img_blur, 50, 50)
    return img_canny

def get_roi(img, pad=3):
    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
    max_area = 0
    
    for cnt in contours:
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        x, y, w, h = cv2.boundingRect(approx)
        rect_area = w * h
        if rect_area > max_area:
            max_area = rect_area
            dim = x, y, w, h

    if max_area:
        x, y, w, h = dim
        return x - pad, y - pad, w + pad * 2, h + pad * 2 

img = cv2.imread("pic_mcq.jpg")

img_processed = preprocess(img)
x, y, w, h = get_roi(img_processed)

cv2.imshow("Image", img[y:y + h, x:x + w])
cv2.waitKey(0)
172/1:
import cv2

def preprocess(img):
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img_blur = cv2.GaussianBlur(img_gray, (5, 5), 1)
    img_canny = cv2.Canny(img_blur, 50, 50)
    return img_canny

def get_roi(img, pad=3):
    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
    max_area = 0
    
    for cnt in contours:
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        x, y, w, h = cv2.boundingRect(approx)
        rect_area = w * h
        if rect_area > max_area:
            max_area = rect_area
            dim = x, y, w, h

    if max_area:
        x, y, w, h = dim
        return x - pad, y - pad, w + pad * 2, h + pad * 2 

img = cv2.imread("pic_mcq.jpg")

img_processed = preprocess(img)
x, y, w, h = get_roi(img_processed)

cv2.imshow("Image", img[y:y + h, x:x + w])
cv2.waitKey(0)
172/2:
import cv2

def preprocess(img):
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img_blur = cv2.GaussianBlur(img_gray, (5, 5), 1)
    img_canny = cv2.Canny(img_blur, 50, 50)
    return img_canny

def get_roi(img, pad=3):
    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
    max_area = 0
    
    for cnt in contours:
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        x, y, w, h = cv2.boundingRect(approx)
        rect_area = w * h
        if rect_area > max_area:
            max_area = rect_area
            dim = x, y, w, h

    if max_area:
        x, y, w, h = dim
        return x - pad, y - pad, w + pad * 2, h + pad * 2 

img = cv2.imread("pic_mcq.jpg")

img_processed = preprocess(img)
x, y, w, h = get_roi(img_processed)

cv2.imshow("Image", img[y:y + h, x:x + w])
cv2.waitKey(0)
173/1:
import easyocr
import cv2
import matplotlib.pyplot as plt
import numpy as np
173/2:
import cv2

def preprocess(img):
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img_blur = cv2.GaussianBlur(img_gray, (5, 5), 1)
    img_canny = cv2.Canny(img_blur, 50, 50)
    return img_canny

def get_roi(img, pad=3):
    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
    max_area = 0
    
    for cnt in contours:
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        x, y, w, h = cv2.boundingRect(approx)
        rect_area = w * h
        if rect_area > max_area:
            max_area = rect_area
            dim = x, y, w, h

    if max_area:
        x, y, w, h = dim
        return x - pad, y - pad, w + pad * 2, h + pad * 2 

img = cv2.imread("pic_mcq.jpg")

img_processed = preprocess(img)
x, y, w, h = get_roi(img_processed)

cv2.imshow("Image", img[y:y + h, x:x + w])
cv2.waitKey(0)
174/1:
import easyocr
import cv2
import matplotlib.pyplot as plt
import numpy as np
174/2:
import cv2

def preprocess(img):
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img_blur = cv2.GaussianBlur(img_gray, (5, 5), 1)
    img_canny = cv2.Canny(img_blur, 50, 50)
    return img_canny

def get_roi(img, pad=3):
    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
    max_area = 0
    
    for cnt in contours:
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        x, y, w, h = cv2.boundingRect(approx)
        rect_area = w * h
        if rect_area > max_area:
            max_area = rect_area
            dim = x, y, w, h

    if max_area:
        x, y, w, h = dim
        return x - pad, y - pad, w + pad * 2, h + pad * 2 

img = cv2.imread("pic_mcq.jpg")

img_processed = preprocess(img)
x, y, w, h = get_roi(img_processed)

cv2.imshow("Image", img[y:y + h, x:x + w])
cv2.waitKey(0)
174/3:
import cv2

def preprocess(img):
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img_blur = cv2.GaussianBlur(img_gray, (5, 5), 1)
    img_canny = cv2.Canny(img_blur, 50, 50)
    return img_canny

def get_roi(img, pad=3):
    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
    max_area = 0
    
    for cnt in contours:
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        x, y, w, h = cv2.boundingRect(approx)
        rect_area = w * h
        if rect_area > max_area:
            max_area = rect_area
            dim = x, y, w, h

    if max_area:
        x, y, w, h = dim
        return x - pad, y - pad, w + pad * 2, h + pad * 2 

img = cv2.imread("pic_mcq.jpg")

img_processed = preprocess(img)
x, y, w, h = get_roi(img_processed)

cv2.imshow("Image", img[y:y + h, x:x + w])
cv2.waitKey(0)
174/4:
import cv2

def preprocess(img):
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img_blur = cv2.GaussianBlur(img_gray, (5, 5), 1)
    img_canny = cv2.Canny(img_blur, 50, 50)
    return img_canny

def get_roi(img, pad=3):
    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
    max_area = 0
    
    for cnt in contours:
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        x, y, w, h = cv2.boundingRect(approx)
        rect_area = w * h
        if rect_area > max_area:
            max_area = rect_area
            dim = x, y, w, h

    if max_area:
        x, y, w, h = dim
        return x - pad, y - pad, w + pad * 2, h + pad * 2 

img = cv2.imread("142.jpg")

img_processed = preprocess(img)
x, y, w, h = get_roi(img_processed)

cv2.imshow("Image", img[y:y + h, x:x + w])
cv2.waitKey(0)
174/5:
import cv2

def preprocess(img):
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img_blur = cv2.GaussianBlur(img_gray, (5, 5), 1)
    img_canny = cv2.Canny(img_blur, 50, 50)
    return img_canny

def get_roi(img, pad=3):
    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
    max_area = 0
    
    for cnt in contours:
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        x, y, w, h = cv2.boundingRect(approx)
        rect_area = w * h
        if rect_area > max_area:
            max_area = rect_area
            dim = x, y, w, h

    if max_area:
        x, y, w, h = dim
        return x - pad, y - pad, w + pad * 2, h + pad * 2 

img = cv2.imread("919.jpg")

img_processed = preprocess(img)
x, y, w, h = get_roi(img_processed)

cv2.imshow("Image", img[y:y + h, x:x + w])
cv2.waitKey(0)
174/6:
import cv2
enter = "pic_mcq.jpg"
def preprocess(img):
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img_blur = cv2.GaussianBlur(img_gray, (5, 5), 1)
    img_canny = cv2.Canny(img_blur, 50, 50)
    return img_canny

def get_roi(img, pad=3):
    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
    max_area = 0
    
    for cnt in contours:
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        x, y, w, h = cv2.boundingRect(approx)
        rect_area = w * h
        if rect_area > max_area:
            max_area = rect_area
            dim = x, y, w, h

    if max_area:
        x, y, w, h = dim
        return x - pad, y - pad, w + pad * 2, h + pad * 2 

img = cv2.imread(enter)

img_processed = preprocess(img)
x, y, w, h = get_roi(img_processed)

cv2.imshow("Image", img[y:y + h, x:x + w])
cv2.waitKey(0)
plt.imshow(cv2.imread(enter))
174/7:
import easyocr
import cv2
import matplotlib.pyplot as plt
import numpy as np
174/8:
plt.imshow(cv2.imread('new.jpg'))
reader = easyocr.Reader(['en'], gpu=False)
result3 = reader.readtext('new.jpg')
text = ''
for i in range(len(result3)):
    text += result3[i][1]
#     print(result3[i][1])
text
174/9:
plt.imshow(cv2.imread('new.jpg'))
reader = easyocr.Reader(['en'], gpu=False)
result3 = reader.readtext('new.jpg')
text = ''
for i in range(len(result3)):
#     text += result3[i][1]
    print(result3[i][1])
# text
174/10:
plt.imshow(cv2.imread('142.jpg'))
reader = easyocr.Reader(['en'], gpu=False)
result3 = reader.readtext('142.jpg')
text = ''
for i in range(len(result3)):
    text += result3[i][1]+'\n'
#     print(result3[i][1])
text
174/11: text
174/12:
output = text
    
if raw_text.find('E') != -1: # Until you find E
    ends = raw_text.find('E') + raw_text[raw_text.find('E'):].find('\n') # Select text till option E where text ends

    output = output + raw_text[raw_text.find('.'):ends]+'\n\n' # Add to the final output
    raw_text = raw_text[ends:] # Neglect the upper text

elif raw_text.find('D') != -1: # Until you find D if E if not available
    ends = raw_text.find('D') + raw_text[raw_text.find('D'):].find('\n') # Select text till option D where text ends

    output = output + raw_text[raw_text.find('.'):ends]+'\n\n' # Add to the final output
    raw_text = raw_text[ends:] # Neglect the upper text

elif raw_text.find('C') != -1: # Until you find C if D if not available
    ends = raw_text.find('C') + raw_text[raw_text.find('.'):].find('\n') # Select text till option C where text ends

    output = output + raw_text[raw_text.find(str(count)+'.'):ends]+'\n\n' # Add to the final output
    raw_text = raw_text[ends:] # Neglect the upper text


print(output.rstrip()) #Ignore extra space from right
174/13:
output = text
    
if output.find('E') != -1: # Until you find E
    ends = output.find('E') + output[output.find('E'):].find('\n') # Select text till option E where text ends

    output = output + output[output.find('.'):ends]+'\n\n' # Add to the final output
    output = output[ends:] # Neglect the upper text

elif output.find('D') != -1: # Until you find D if E if not available
    ends = output.find('D') + output[output.find('D'):].find('\n') # Select text till option D where text ends

    output = output + output[output.find('.'):ends]+'\n\n' # Add to the final output
    output = output[ends:] # Neglect the upper text

elif output.find('C') != -1: # Until you find C if D if not available
    ends = output.find('C') + output[output.find('.'):].find('\n') # Select text till option C where text ends

    output = output + output[output.find(str(count)+'.'):ends]+'\n\n' # Add to the final output
    output = output[ends:] # Neglect the upper text


print(output.rstrip()) #Ignore extra space from right
179/1:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/sample5'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,120,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir(x[:-4]):
            os.mkdir(x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save(x[:-4] + '/'+ str(i) +'.jpg', 'JPEG')
179/2:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/sample5/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,120,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir(x[:-4]):
            os.mkdir(x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save(x[:-4] + '/'+ str(i) +'.jpg', 'JPEG')
179/3:
from pdf2image import convert_from_path
import os

file_path = 'C:/Users/Dell/Downloads/sample5/'

for x in os.listdir(file_path):
    
    # Store Pdf with convert_from_path function
    images = convert_from_path(file_path+x,120,poppler_path=r'C:\Program Files\poppler-22.04.0\Library\bin')

    # Saving images folder
    if not os.path.isdir(x[:-4]):
            os.mkdir(x[:-4])

    for i in range(len(images)):

        # Save pages as images in the pdf
        images[i].save(x[:-4] + '/'+ str(i) +'.jpg', 'JPEG')
179/4:
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True
from PIL import Image

import os
file_path = 'C:/Users/Dell/new_pdfs'

dest = 'C:/Users/Dell/newpdf_resized' #the path where to save resized images
# create new folder
if not os.path.exists(dest):
    os.makedirs(dest)

counter = 1664
for x in os.listdir(file_path):
    for y in os.listdir(file_path+x):
        counter+=1
        print(y)
        img = Image.open(file_path+x+'/'+y)
        img=img.convert('RGB').resize((960,1280))
        
        # save resized images to new folder with existing filename
        img.save('{}{}{}.jpg'.format(dest,'/',counter))
179/5:
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True
from PIL import Image

import os
file_path = 'C:/Users/Dell/new_pdfs/'

dest = 'C:/Users/Dell/newpdf_resized/' #the path where to save resized images
# create new folder
if not os.path.exists(dest):
    os.makedirs(dest)

counter = 1664
for x in os.listdir(file_path):
    for y in os.listdir(file_path+x):
        counter+=1
        print(y)
        img = Image.open(file_path+x+'/'+y)
        img=img.convert('RGB').resize((960,1280))
        
        # save resized images to new folder with existing filename
        img.save('{}{}{}.jpg'.format(dest,'/',counter))
181/1: import pandas as pd
181/2:
file = pd.read_excel("sample-task.xlsx")
file.head()
181/3:
file = pd.read_excel("sample-task.xlsx")
file
181/4:
a = {}
file.Quantity
181/5:
a = {}
file.Quantity*2
181/6:
a = {}
file.Quantity*8
181/7:
a = {}
a['Number x4'] = file.Quantity*4
a
181/8:
a = {}
a['Number x4'] = file.Quantity*4[1]
a
181/9:
a = {}
calc = file.Quantity*4
# a['Number x4'] = 
calc
181/10:
a = {}
calc = file.Quantity*4
# a['Number x4'] = 
calc.values
181/11: a
181/12:
a = {}
calc = file.Quantity*4
a['Number x4'] = calc.values
a
181/13: file.join(a)
181/14: file.loc[2] = calc
181/15: file
181/16:
file = pd.read_excel("sample-task.xlsx")
file
181/17:
a = {}
calc = file.Quantity*4
a['Number x4'] = calc.values
a
181/18: file.loc[3] = calc
181/19: file
181/20:
file = pd.read_excel("sample-task.xlsx")
file
181/21:
a = {}
calc = file.Quantity*4
a['Number x4'] = calc.values
a
181/22: file = pd.concat([file, a], axis=1)
181/23:
a = {}
calc = file.Quantity*4
a['Number x4'] = calc.values
a = pd.DataFrame(a)
181/24: file = pd.concat([file, a], axis=1)
181/25: file
181/26:
a = {}
four = file.Quantity*4
f_six = file.Quantity*4*6
a['Number x4'] = four.values
a['Number x4 x6'] = f_six.values
a = pd.DataFrame(a)
a
181/27: file.Quantity.values
181/28: file.Quantity.values.square
181/29: file.Quantity.values.square()
181/30: file.Quantity.values*file.Quantity.values
181/31: file.Quantity.values**2
181/32:
a = {}

a['Number x4'] = file.Quantity.values.*4
a['Number x4 x6'] = file.Quantity.values*4*6
a['Number Square'] = file.Quantity.values**2
a['Number Cube'] = file.Quantity.values**3
a = pd.DataFrame(a)
a
181/33:
a = {}

a['Number x4'] = file.Quantity.values*4
a['Number x4 x6'] = file.Quantity.values*4*6
a['Number Square'] = file.Quantity.values**2
a['Number Cube'] = file.Quantity.values**3
a = pd.DataFrame(a)
a
181/34:
folder = pd.concat([folder, a], axis=1)
folder
181/35:
file = pd.concat([file, a], axis=1)
folder
181/36:
file = pd.concat([file, a], axis=1)
file
181/37:
file = pd.read_excel("sample-task.xlsx")
file
181/38:
a = {}

a['Number x4'] = file.Quantity.values*4
a['Number x4 x6'] = file.Quantity.values*4*6
a['Number Square'] = file.Quantity.values**2
a['Number Cube'] = file.Quantity.values**3
a = pd.DataFrame(a)
a
181/39:
file = pd.concat([file, a], axis=1)
file
181/40: file.to_excel('sample-task.xlsx',index=False)
181/41:

writer = pd.ExcelWriter(file, engine='xlsxwriter')

# Convert the dataframe to an XlsxWriter Excel object.
df.to_excel(writer, sheet_name='Sheet2', index=False)

# Close the Pandas Excel writer and output the Excel file.
writer.save()
181/42:

writer = pd.ExcelWriter(path, engine = 'xlsxwriter')
file.to_excel(writer, sheet_name = 'Sheet1')
file.to_excel(writer, sheet_name = 'Sheet2')
writer.save()
writer.close()
181/43:

writer = pd.ExcelWriter('sample-task.xlsx', engine = 'xlsxwriter')
file.to_excel(writer, sheet_name = 'Sheet1')
file.to_excel(writer, sheet_name = 'Sheet2')
writer.save()
writer.close()
181/44:

writer = pd.ExcelWriter('sample-task.xlsx', engine = 'xlsxwriter')
file.to_excel(writer, sheet_name = 'Sheet1')
file.to_excel(writer, sheet_name = 'Sheet2')
writer.save()
writer.close()
181/45:
file = pd.read_excel("sample-task.xlsx")
file
181/46: import pandas as pd
181/47:
file = pd.read_excel("sample-task.xlsx")
file
181/48:
a = {}

a['Number x4'] = file.Quantity.values*4
a['Number x4 x6'] = file.Quantity.values*4*6
a['Number Square'] = file.Quantity.values**2
a['Number Cube'] = file.Quantity.values**3
a = pd.DataFrame(a)
a
181/49:
file = pd.concat([file, a], axis=1)
file
181/50: file.to_excel('sample-task.xlsx',index=False)
181/51: file.to_excel('sample-task.xlsx',index=False)
181/52:

writer = pd.ExcelWriter('sample-task.xlsx', engine = 'xlsxwriter')
file.to_excel(writer, sheet_name = 'Sheet1',index=False)
file.to_excel(writer, sheet_name = 'Sheet2',index=False)
181/53:

writer = pd.ExcelWriter('sample-task.xlsx', engine = 'xlsxwriter')
file.to_excel(writer, sheet_name = 'Sheet1',index=False)
file.to_excel(writer, sheet_name = 'Sheet2',index=False)
writer.save()
writer.close()
181/54: import pandas as pd
181/55:
file = pd.read_excel("sample-task.xlsx")
file
181/56:
a = {}

a['Number x4'] = file.Quantity.values*4
a['Number x4 x6'] = file.Quantity.values*4*6
a['Number Square'] = file.Quantity.values**2
a['Number Cube'] = file.Quantity.values**3
a = pd.DataFrame(a)
a
181/57:
file = pd.concat([file, a], axis=1)
file
181/58: file.to_excel('sample-task.xlsx',index=False)
181/59: file.to_excel('sample-task.xlsx',index=False)
181/60:

writer = pd.ExcelWriter('sample-task.xlsx', engine = 'xlsxwriter')
file.to_excel(writer, sheet_name = 'Sheet1',index=False)
file.to_excel(writer, sheet_name = 'Sheet2',index=False)
writer.save()
writer.close()
181/61:
file = pd.read_excel("sample-task.xlsx")
file
181/62:
a = {}

a['Number x4'] = file.Quantity.values*4
a['Number x4 x6'] = file.Quantity.values*4*6
a['Number Square'] = file.Quantity.values**2
a['Number Cube'] = file.Quantity.values**3
a = pd.DataFrame(a)
a
181/63:
file = pd.concat([file, a], axis=1)
file
181/64: file.to_excel('sample-task.xlsx',index=False)
181/65:

writer = pd.ExcelWriter('sample-task.xlsx', engine = 'xlsxwriter')
file.to_excel(writer, sheet_name = 'Sheet1',index=False)
file.to_excel(writer, sheet_name = 'Sheet2',index=False)
writer.save()
writer.close()
182/1:
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy
import joblib
182/2:
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib
182/3:
def train_model():
    iris_ds = datasets.load_iris()
    x = iris_ds.data
    y = iris_ds.target
    X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.25)
    dt = DecissionTreeClassifier.fit(X_train,y_train).fit()
    preds = dt.predict(X_test)
    accuracy = accuracy_score(y_test, preds)
    print('The accuracy of your model is {}'.format(accuracy))
    joblib.dump(dt, 'iris_model.model')
182/4:
import os
from flask import jsonify, Flask, request
from flask_restful import Api, Resource
182/5: !python -V
182/6:
import os
from flask import jsonify, Flask, request
from flask_restful import Api, Resource
182/7:
if not os.path.isfile('iris_model.model'):
    train_model()

model = joblib.load('iris_model.model')
182/8:
def train_model():
    iris_ds = datasets.load_iris()
    x = iris_ds.data
    y = iris_ds.target
    X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.25)
    dt = DecisionTreeClassifier.fit(X_train,y_train).fit()
    preds = dt.predict(X_test)
    accuracy = accuracy_score(y_test, preds)
    print('The accuracy of your model is {}'.format(accuracy))
    joblib.dump(dt, 'iris_model.model')
182/9:
if not os.path.isfile('iris_model.model'):
    train_model()

model = joblib.load('iris_model.model')
182/10:
from sklearn import datasets
from sklearn.tree import DecissionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib
182/11:
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib
182/12:
def train_model():
    iris_ds = datasets.load_iris()
    x = iris_ds.data
    y = iris_ds.target
    X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.25)
    dt = DecisionTreeClassifier.fit(X_train,y_train).fit()
    preds = dt.predict(X_test)
    accuracy = accuracy_score(y_test, preds)
    print('The accuracy of your model is {}'.format(accuracy))
    joblib.dump(dt, 'iris_model.model')
182/13:
if not os.path.isfile('iris_model.model'):
    train_model()

model = joblib.load('iris_model.model')
182/14:
def train_model():
    iris_ds = datasets.load_iris()
    x = iris_ds.data
    y = iris_ds.target
    X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.25)
    dt = DecisionTreeClassifier.fit(X_train,y_train)
    preds = dt.predict(X_test)
    accuracy = accuracy_score(y_test, preds)
    print('The accuracy of your model is {}'.format(accuracy))
    joblib.dump(dt, 'iris_model.model')
182/15:
if not os.path.isfile('iris_model.model'):
    train_model()

model = joblib.load('iris_model.model')
182/16:
def train_model():
    iris_ds = datasets.load_iris()
    x = iris_ds.data
    y = iris_ds.target
    X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.25)
    dt = DecisionTreeClassifier().fit(X_train,y_train)
    preds = dt.predict(X_test)
    accuracy = accuracy_score(y_test, preds)
    print('The accuracy of your model is {}'.format(accuracy))
    joblib.dump(dt, 'iris_model.model')
182/17:
if not os.path.isfile('iris_model.model'):
    train_model()

model = joblib.load('iris_model.model')
183/1:
import os
from flask import Flask, jsonify, request
from flask_restful import Api, Resource
import joblib

app = Flask(__name__)
api = Api(app)

model = joblib.load('iris_model.model')

class MakePrediction(Resource):
    @staticmethod
    def post():
        posted_data = request.get_json()
        sepal_length = posted_data['sepal_length']
        sepal_width = posted_data['sepal_width']
        petal_length = posted_data['petal_length']
        petal_width = posted_data['petal_width']

        prediction = model.predict([[sepal_length, sepal_width, petal_length, petal_width]])[0]
        if prediction == 0:
            predicted_class = 'Iris-setosa'
        elif prediction == 1:
            predicted_class = 'Iris-versicolor'
        else:
            predicted_class = 'Iris-virginica'

        return jsonify({
            'Prediction': predicted_class
        })
    

api.add_resource(MakePrediction, '/predict')


if __name__ == '__main__':
    app.run(debug=True)
185/1:
def palendrom(a):
    # if the len is odd then neglact the middle char
    # nested if: compare then first chr with last and then we will and a + counter and - counter for the the chars
    if a%2!=0:
        length = len(a)//2 + 1
        a = a[:length]+ a[length+1:]
        return a
    # for i in range(len(a)//2):
    #     a = len(a) # 6

palendrom('aaadaaa')
185/2:
def palendrom(a):
    # if the len is odd then neglact the middle char
    # nested if: compare then first chr with last and then we will and a + counter and - counter for the the chars
    if len(a)%2!=0:
        length = len(a)//2 + 1
        a = a[:length]+ a[length+1:]
        return a
    # for i in range(len(a)//2):
    #     a = len(a) # 6

palendrom('aaadaaa')
185/3:
def palendrom(a):
    # if the len is odd then neglact the middle char
    # nested if: compare then first chr with last and then we will and a + counter and - counter for the the chars
    if len(a)%2!=0:
        length = len(a)//2 + 1
        a = a[:length+1]+ a[length:]
        return a
    # for i in range(len(a)//2):
    #     a = len(a) # 6

palendrom('aaadaaa')
185/4:
def palendrom(a):
    # if the len is odd then neglact the middle char
    # nested if: compare then first chr with last and then we will and a + counter and - counter for the the chars
    if len(a)%2!=0:
        length = len(a)//2
        a = a[:length]+ a[length+1:]
        return a
    # for i in range(len(a)//2):
    #     a = len(a) # 6

palendrom('aaadaaa')
186/1:
import pyrebase

config = {
  apiKey: "AIzaSyC38NC-iE-L6tlb5z-J4RMLIVwftTsZoKs",
  authDomain: "methena.firebaseapp.com",
  databaseURL: "https://methena-default-rtdb.firebaseio.com",
  projectId: "methena",
  storageBucket: "methena.appspot.com",
  messagingSenderId: "378103774136",
  appId: "1:378103774136:web:2e7611a3518d9db3bb2d4d"
}
186/2:
import pyrebase

config = {
    apiKey: "AIzaSyC38NC-iE-L6tlb5z-J4RMLIVwftTsZoKs",
    authDomain: "methena.firebaseapp.com",
    databaseURL: "https://methena-default-rtdb.firebaseio.com",
    projectId: "methena",
    storageBucket: "methena.appspot.com",
    messagingSenderId: "378103774136",
    appId: "1:378103774136:web:2e7611a3518d9db3bb2d4d"
}
186/3:
import pyrebase

config = {
    apiKey: "AIzaSyC38NC-iE-L6tlb5z-J4RMLIVwftTsZoKs",
    authDomain: "methena.firebaseapp.com",
    databaseURL: "https://methena-default-rtdb.firebaseio.com",
    projectId: "methena",
    storageBucket: "methena.appspot.com",
    messagingSenderId: "378103774136",
    appId: "1:378103774136:web:2e7611a3518d9db3bb2d4d"
}
186/4:
import pyrebase

config = {
    apiKey: "AIzaSyC38NC-iE-L6tlb5z-J4RMLIVwftTsZoKs",
    authDomain: "methena.firebaseapp.com",
    databaseURL: "https://methena-default-rtdb.firebaseio.com",
    projectId: "methena",
    storageBucket: "methena.appspot.com",
    messagingSenderId: "378103774136",
    appId: "1:378103774136:web:2e7611a3518d9db3bb2d4d"
}
186/5:
import pyrebase

config = {
    "apiKey": "AIzaSyC38NC-iE-L6tlb5z-J4RMLIVwftTsZoKs",
    "authDomain": "methena.firebaseapp.com",
    "databaseURL": "https://methena-default-rtdb.firebaseio.com",
    "projectId": "methena",
    "storageBucket": "methena.appspot.com",
    "messagingSenderId": "378103774136",
    "appId": "1:378103774136:web:2e7611a3518d9db3bb2d4d"
}
186/6:
import pyrebase

config = {
    "apiKey": "AIzaSyC38NC-iE-L6tlb5z-J4RMLIVwftTsZoKs",
    "authDomain": "methena.firebaseapp.com",
    "databaseURL": "https://methena-default-rtdb.firebaseio.com",
    "projectId": "methena",
    "storageBucket": "methena.appspot.com",
    "messagingSenderId": "378103774136",
    "appId": "1:378103774136:web:2e7611a3518d9db3bb2d4d",
    "serviceAccount": "serviceAccount.json"
}
186/7: firebase = pyrebase.initialize_app(config)
186/8:
import pyrebase

config = {
    "apiKey": "AIzaSyC38NC-iE-L6tlb5z-J4RMLIVwftTsZoKs",
    "authDomain": "methena.firebaseapp.com",
    "databaseURL": "https://methena-default-rtdb.firebaseio.com",
    "projectId": "methena",
    "storageBucket": "methena.appspot.com",
    "messagingSenderId": "378103774136",
    "appId": "1:378103774136:web:2e7611a3518d9db3bb2d4d",
    "serviceAccount": r"serviceAccount.json"
}
186/9: firebase = pyrebase.initialize_app(config)
186/10:
import pyrebase

config = {
    "apiKey": "AIzaSyC38NC-iE-L6tlb5z-J4RMLIVwftTsZoKs",
    "authDomain": "methena.firebaseapp.com",
    "databaseURL": "https://methena-default-rtdb.firebaseio.com",
    "projectId": "methena",
    "storageBucket": "methena.appspot.com",
    "messagingSenderId": "378103774136",
    "appId": "1:378103774136:web:2e7611a3518d9db3bb2d4d",
    "serviceAccount": "serviceAccount.json/"
}
186/11:
import pyrebase

config = {
    "apiKey": "AIzaSyC38NC-iE-L6tlb5z-J4RMLIVwftTsZoKs",
    "authDomain": "methena.firebaseapp.com",
    "databaseURL": "https://methena-default-rtdb.firebaseio.com",
    "projectId": "methena",
    "storageBucket": "methena.appspot.com",
    "messagingSenderId": "378103774136",
    "appId": "1:378103774136:web:2e7611a3518d9db3bb2d4d",
    "serviceAccount": "serviceAccount.json/"
}
186/12: firebase = pyrebase.initialize_app(config)
186/13:
import pyrebase

config = {
    "apiKey": "AIzaSyC38NC-iE-L6tlb5z-J4RMLIVwftTsZoKs",
    "authDomain": "methena.firebaseapp.com",
    "databaseURL": "https://methena-default-rtdb.firebaseio.com",
    "projectId": "methena",
    "storageBucket": "methena.appspot.com",
    "messagingSenderId": "378103774136",
    "appId": "1:378103774136:web:2e7611a3518d9db3bb2d4d",
    "serviceAccount": "serviceAccount.json"
}
186/14: firebase = pyrebase.initialize_app(config)
186/15:
import pyrebase

config = {
    "apiKey": "AIzaSyC38NC-iE-L6tlb5z-J4RMLIVwftTsZoKs",
    "authDomain": "methena.firebaseapp.com",
    "databaseURL": "https://methena-default-rtdb.firebaseio.com",
    "projectId": "methena",
    "storageBucket": "methena.appspot.com",
    "messagingSenderId": "378103774136",
    "appId": "1:378103774136:web:2e7611a3518d9db3bb2d4d",
    "serviceAccount": "serviceAccount.json"
}
186/16: firebase = pyrebase.initialize_app(config)
186/17:
import pyrebase

config = {
    "apiKey": "AIzaSyC38NC-iE-L6tlb5z-J4RMLIVwftTsZoKs",
    "authDomain": "methena.firebaseapp.com",
    "databaseURL": "https://methena-default-rtdb.firebaseio.com",
    "projectId": "methena",
    "storageBucket": "methena.appspot.com",
    "messagingSenderId": "378103774136",
    "appId": "1:378103774136:web:2e7611a3518d9db3bb2d4d",
    "serviceAccount": "serviceAccount.json",
    "databaseURL": "https://methena-default-rtdb.firebaseio.com/"
}
186/18:
firebase = pyrebase.initialize_app(config)

storage = firebase.storage()
storage.child("logo1.png").put("logo.png")
186/19:
import pyrebase

config = {
    "apiKey": "AIzaSyC38NC-iE-L6tlb5z-J4RMLIVwftTsZoKs",
    "authDomain": "methena.firebaseapp.com",
    "databaseURL": "https://methena-default-rtdb.firebaseio.com",
    "projectId": "methena",
    "storageBucket": "methena.appspot.com",
    "messagingSenderId": "378103774136",
    "appId": "1:378103774136:web:2e7611a3518d9db3bb2d4d",
    "serviceAccount": r"‪C:\Users\Dell\firebase\serviceAccount.json",
    "databaseURL": "https://methena-default-rtdb.firebaseio.com/"
}
186/20:
firebase = pyrebase.initialize_app(config)

storage = firebase.storage()
storage.child("logo1.png").put("logo.png")
186/21:
import pyrebase

config = {
    "apiKey": "AIzaSyC38NC-iE-L6tlb5z-J4RMLIVwftTsZoKs",
    "authDomain": "methena.firebaseapp.com",
    "databaseURL": "https://methena-default-rtdb.firebaseio.com",
    "projectId": "methena",
    "storageBucket": "methena.appspot.com",
    "messagingSenderId": "378103774136",
    "appId": "1:378103774136:web:2e7611a3518d9db3bb2d4d",
    "serviceAccount": "‪C:/Users/Dell/firebase/serviceAccount.json",
    "databaseURL": "https://methena-default-rtdb.firebaseio.com/"
}
186/22:
firebase = pyrebase.initialize_app(config)

storage = firebase.storage()
storage.child("logo1.png").put("logo.png")
186/23:
import pyrebase
f = open("C:/Users/Dell/firebase/serviceAccount.json")
config = {
    "apiKey": "AIzaSyC38NC-iE-L6tlb5z-J4RMLIVwftTsZoKs",
    "authDomain": "methena.firebaseapp.com",
    "databaseURL": "https://methena-default-rtdb.firebaseio.com",
    "projectId": "methena",
    "storageBucket": "methena.appspot.com",
    "messagingSenderId": "378103774136",
    "appId": "1:378103774136:web:2e7611a3518d9db3bb2d4d",
    "serviceAccount": f,
    "databaseURL": "https://methena-default-rtdb.firebaseio.com/"
}
186/24:
firebase = pyrebase.initialize_app(config)

storage = firebase.storage()
storage.child("logo1.png").put("logo.png")
186/25:
firebase = pyrebase.initialize_app(config)

storage = firebase.storage()
storage.child("1.jpg").put("unnamed.jpg")
187/1:
import pyrebase
f = open("C:/Users/Dell/firebase/serviceAccount.json")
config = {
    "apiKey": "AIzaSyC38NC-iE-L6tlb5z-J4RMLIVwftTsZoKs",
    "authDomain": "methena.firebaseapp.com",
    "databaseURL": "https://methena-default-rtdb.firebaseio.com",
    "projectId": "methena",
    "storageBucket": "methena.appspot.com",
    "messagingSenderId": "378103774136",
    "appId": "1:378103774136:web:2e7611a3518d9db3bb2d4d",
    "serviceAccount": f,
    "databaseURL": "https://methena-default-rtdb.firebaseio.com/"
}
187/2:
firebase = pyrebase.initialize_app(config)

storage = firebase.storage()
storage.child("1.jpg").put("unnamed.jpg")
187/3:
firebase = pyrebase.initialize_app(config)

storage = firebase.storage()
storage.child("1.jpg").put("unnamed.jpg")
187/4:
firebase = pyrebase.initialize_app(config)

storage = firebase.storage()
storage.child("1.jpg").put(open("unnamed.jpg"))
187/5:
firebase = pyrebase.initialize_app(config)

storage = firebase.storage()
storage.child("1.jpg").put(open("C:/Users/Dell/firebase/unnamed.jpg"))
187/6:
firebase = pyrebase.initialize_app(config)

storage = firebase.storage()
storage.child("1.jpg").put("C:/Users/Dell/firebase/unnamed.jpg")
188/1:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
188/2:
#install deepface
!conda activate hack
188/3:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
188/4:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
188/5:
#install deepface, pyaudio
!python -m speech_recognition
188/6:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
188/7:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
188/8:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
188/9:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
189/1:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
190/1:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
192/1:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
193/1:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
195/1:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
195/2:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
196/1:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
196/2:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
196/3:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base.pt")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
196/4:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model(open("base.pt"))
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
196/5:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



class AudioRecorder:

    # Audio class based on pyAudio and Wave
    def __init__(self):

        self.open = True
        self.rate = 44100
        self.frames_per_buffer = 1024
        self.channels = 2
        self.format = pyaudio.paInt16
        self.audio_filename = "temp_audio.wav"
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=self.format,
                                      channels=self.channels,
                                      rate=self.rate,
                                      input=True,
                                      frames_per_buffer=self.frames_per_buffer)
        self.audio_frames = []

    # Audio starts being recorded
    def record(self):

        self.stream.start_stream()
        while (self.open == True):
            data = self.stream.read(self.frames_per_buffer)
            self.audio_frames.append(data)
            if self.open == False:
                break

    # Finishes the audio recording therefore the thread too    
    def stop(self):

        if self.open == True:
            self.open = False
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()

            waveFile = wave.open(self.audio_filename, 'wb')
            waveFile.setnchannels(self.channels)
            waveFile.setsampwidth(self.audio.get_sample_size(self.format))
            waveFile.setframerate(self.rate)
            waveFile.writeframes(b''.join(self.audio_frames))
            waveFile.close()

        pass

    # Launches the audio recording function using a thread
    def start(self):
        audio_thread = threading.Thread(target=self.record)
        audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    audio_thread = AudioRecorder()

    audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base.pt")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    #whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        print("Muxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

    else:

        print("Normal recording\nMuxing")
        cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
        subprocess.call(cmd, shell=True)

        print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
196/6:
model = whisper.load_model("base")
result = model.transcribe("temp_audio.wav")
print(result["text"])
196/7:
import whisper
model = whisper.load_model("base")
result = model.transcribe("temp_audio.wav")
print(result["text"])
196/8:
import whisper
model = whisper.load_model("base")
result = model.transcribe(r"C:\Users\Dell\Downloads\file_example_MP3_700KB.mp3")
print(result["text"])
196/9:
import whisper
model = whisper.load_model("base")
result = model.transcribe(r"C:\Users\Dell\Downloads\file_example_MP3_700KB.mp3", fp16=False)
print(result["text"])
199/1:
from deepface import DeepFace
from datetime import datetime
import csv
import pytz
import cv2
# import pyaudio
import wave
import threading
import time
import subprocess
import os
import speech_recognition as sr
import pprint
import whisper
from whisper import load_model
from flask import Response

import base64
import io

global timeNow
max_emotion = "Default"
emotionlist = []
lock = threading.Lock()

class VideoRecorder:
    # Video class based on openCV
    def __init__(self):
        # print("in __init__")
        self.open = True
        self.device_index = 0
        self.fps = 6  # fps should be the minimum constant rate at which the camera can
        self.fourcc = "MJPG"  # capture images (with no decrease in speed over time; testing is required)
        self.frameSize = (640, 480)  # video formats and sizes also depend and vary according to the camera used
        self.video_filename = "temp_video.avi"
        self.video_cap = cv2.VideoCapture(self.device_index)
        self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)
        self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)
        self.frame_counts = 1
        self.start_time = time.time()

    def timeFunction(self, emotion):
        # print("in timeFunction")
        print(emotion)
        emotionlist.append(emotion)
        # now = datetime.now()
        dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
        timeNow = dt_hongkong.strftime("%H:%M:%S")
        emotion_list = [timeNow, emotion, max_emotion]
        print(emotion_list)
        with open('emotion_time.csv', 'a', newline='') as csvfile:
            my_writer = csv.writer(csvfile, delimiter=',')
            my_writer.writerow(emotion_list)
            print("HEre")

    def most_common(self, emotionlist):
        print("in most_common")
        a = max(set(emotionlist), key=emotionlist.count, default=0)
        return a

    def deepFace(self, frame_deep):
        print("in deepFace")
        obj = DeepFace.analyze(img_path=frame_deep, actions=['emotion'])
        # print("Dominant Emotion: ")
        emotion = obj['dominant_emotion']
        # print(emotion)
        self.timeFunction(emotion)

    def generate(self, gen_frame):
        print("in generate")
        global lock
        #cv2.imshow('Frame Deepface 2', gen_frame)
        print("in generate")
        # loop over frames from the output stream
        while True:
            # wait until the lock is acquired
            with lock:
                # check if the output frame is available, otherwise skip
                # the iteration of the loop
                if gen_frame is None:
                    continue
                # encode the frame in JPEG format
                (flag, encodedImage) = cv2.imencode(".jpg", gen_frame)
                # ensure the frame was successfully encoded
                if not flag:
                    continue
            # yield the output frame in the byte format
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' +
                   bytearray(encodedImage) + b'\r\n')
    # Video starts being recorded

    def video_feed(self):
        # return the response generated along with the specific media
        # type (mime type)
        return Response(self.generate(),
                        mimetype="multipart/x-mixed-replace; boundary=frame")


    def record(self):

        global max_emotion
        # counter = 1
        timer_current = 0
        if not self.video_cap.isOpened():
            print("Error opening video file")

        while self.video_cap.isOpened():
            dt_hongkong = datetime.now(pytz.timezone('Hongkong'))
            timeNow = dt_hongkong.strftime("%H:%M:%S")

            timeSec = timeNow.split(':')
            timeSec = timeSec[2]
            print(timeSec)
            if timeSec == "30" or timeSec == "00":
                max_emotion = str(self.most_common(emotionlist))

            ret, video_frame = self.video_cap.read()
            if ret:
                cv2.imshow('Frame Deepface', video_frame)

                self.video_out.write(video_frame)
                self.frame_counts += 1

                try:
                    # print("try ret")
                    #self.generate(video_frame)
                    self.deepFace(video_frame)

                except:
                    print("No Face Detected")

                # Press Q on keyboard to exit
                if cv2.waitKey(25) & 0xFF == ord('q'):
                    break

                # Break the loop
            else:
                break

                # time.sleep(0.16)

            # cv2.waitKey(1)

            # 0.16 delay -> 6 fps
            #
        self.video_cap.release()
        # Closes all the frames
        cv2.destroyAllWindows()

    # Finishes the video recording therefore the thread too
    def stop(self):

        if self.open == True:
            self.open = False
            self.video_out.release()
            self.video_cap.release()
            cv2.destroyAllWindows()

        else:
            pass

    # Launches the video recording function using a thread
    def start(self):
        video_thread = threading.Thread(target=self.record)
        video_thread.start()



# class AudioRecorder:

#     # Audio class based on pyAudio and Wave
#     def __init__(self):

#         self.open = True
#         self.rate = 44100
#         self.frames_per_buffer = 1024
#         self.channels = 2
#         self.format = pyaudio.paInt16
#         self.audio_filename = "temp_audio.wav"
#         self.audio = pyaudio.PyAudio()
#         self.stream = self.audio.open(format=self.format,
#                                       channels=self.channels,
#                                       rate=self.rate,
#                                       input=True,
#                                       frames_per_buffer=self.frames_per_buffer)
#         self.audio_frames = []

#     # Audio starts being recorded
#     def record(self):

#         self.stream.start_stream()
#         while (self.open == True):
#             data = self.stream.read(self.frames_per_buffer)
#             self.audio_frames.append(data)
#             if self.open == False:
#                 break

#     # Finishes the audio recording therefore the thread too    
#     def stop(self):

#         if self.open == True:
#             self.open = False
#             self.stream.stop_stream()
#             self.stream.close()
#             self.audio.terminate()

#             waveFile = wave.open(self.audio_filename, 'wb')
#             waveFile.setnchannels(self.channels)
#             waveFile.setsampwidth(self.audio.get_sample_size(self.format))
#             waveFile.setframerate(self.rate)
#             waveFile.writeframes(b''.join(self.audio_frames))
#             waveFile.close()

#         pass

#     # Launches the audio recording function using a thread
#     def start(self):
#         audio_thread = threading.Thread(target=self.record)
#         audio_thread.start()


def start_AVrecording(filename):
    global video_thread
    global audio_thread

    video_thread = VideoRecorder()
    # audio_thread = AudioRecorder()

    # audio_thread.start()
    video_thread.start()

    return filename


def start_video_recording(filename):
    global video_thread

    video_thread = VideoRecorder()
    video_thread.start()

    return filename


def start_audio_recording(filename):
    global audio_thread

    audio_thread = AudioRecorder()
    audio_thread.start()

    return filename

def googleAI():
    print("TEST TEST")
    print(AudioRecorder.audio_filename)
    audio_file = "sample1.wav"

    r = sr.Recognizer()
    pp = pprint.PrettyPrinter(width=90, compact=True)

    AUDIO_FILE = (audio_file)
    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.adjust_for_ambient_noise(source)
        audio = r.record(source)
        # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        captured_text = r.recognize_google(audio)
        print("Google Speech Recognition transcribed your audio to 'captured_text' ")
        print("Google Speech Recognition thinks you said: \n \n ")
        pp.pprint(captured_text)

    except sr.UnknownValueError:
        pp.pprint("Google Speech Recognition could not understand audio")
    except sr.RequestError as e:
        pp.pprint("Could not request results from Google Speech Recognition service; {0}".format(e))


def whisper():
    audio_file = "sample1.wav"
    model = load_model("base")
    result = model.transcribe(audio_file)
    print(result["text"])
    with open("audio_sample.txt", "w+") as f:
        f.write(result["text"])

def stop_AVrecording(filename):
    # audio_thread.stop()
    frame_counts = video_thread.frame_counts
    elapsed_time = time.time() - video_thread.start_time
    recorded_fps = frame_counts / elapsed_time
    print("total frames " + str(frame_counts))
    print("elapsed time " + str(elapsed_time))
    print("recorded fps " + str(recorded_fps))
    video_thread.stop()
    # whisper()
    # googleAI()

    # Makes sure the threads have finished
    # while threading.active_count() > 1:
    #     print(threading.active_count())
    #     print("STOP HERE")
    time.sleep(5)
    #     print("STOP after SLEEP")


    print("MERGE")
    #    Merging audio and video signal

    if abs(recorded_fps - 6) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected

        print("Re-encoding")
        cmd = "ffmpeg -r " + str(recorded_fps) + " -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi"
        subprocess.call(cmd, shell=True)

        # print("Muxing")
        # cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p " + filename + ".avi"
        # subprocess.call(cmd, shell=True)

    # else:

    #     print("Normal recording\nMuxing")
    #     cmd = "ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p " + filename + ".avi"
    #     subprocess.call(cmd, shell=True)

    #     print("..")


# Required and wanted processing of final files
def file_manager(filename):
    local_path = os.getcwd()

    if os.path.exists(str(local_path) + "/temp_audio.wav"):
        os.remove(str(local_path) + "/temp_audio.wav")

    if os.path.exists(str(local_path) + "/temp_video.avi"):
        os.remove(str(local_path) + "/temp_video.avi")

    if os.path.exists(str(local_path) + "/temp_video2.avi"):
        os.remove(str(local_path) + "/temp_video2.avi")

    if os.path.exists(str(local_path) + "/" + filename + ".avi"):
        os.remove(str(local_path) + "/" + filename + ".avi")


if __name__ == "__main__":
    filename = "Default_user"
    file_manager(filename)

    start_AVrecording(filename)

    time.sleep(30)

    stop_AVrecording(filename)
    print("Done")
199/2: os.getcwd()
200/1:
import cv2, pafy # pip install pafy ,  pip3 install youtube-dl
import numpy as np

url = "https://www.youtube.com/watch?v=_bCJ6RiU36U&t=398s"

video = pafy.new(url)
best  = video.getbest(preftype="webm")
#documentation: https://pypi.org/project/pafy/

capture = cv2.VideoCapture(best.url)
check, frame = capture.read()
print (check, frame)

cv2.imshow('frame',frame)
cv2.waitKey(10)

capture.release()
cv2.destroyAllWindows()
200/2:
import cv2, pafy # pip install pafy ,  pip3 install youtube-dl
import numpy as np

url = "https://www.youtube.com/watch?v=_bCJ6RiU36U&t=398s"

video = pafy.new(url)
best  = video.getbest(preftype="webm")
#documentation: https://pypi.org/project/pafy/

capture = cv2.VideoCapture(best.url)
check, frame = capture.read()
print (check, frame)

cv2.imshow('frame',frame)
cv2.waitKey(10)

capture.release()
cv2.destroyAllWindows()
201/1:
import cv2, pafy # pip install pafy ,  pip3 install youtube-dl
import numpy as np

url = "https://www.youtube.com/watch?v=_bCJ6RiU36U&t=398s"

video = pafy.new(url)
best  = video.getbest(preftype="webm")
#documentation: https://pypi.org/project/pafy/

capture = cv2.VideoCapture(best.url)
check, frame = capture.read()
print (check, frame)

cv2.imshow('frame',frame)
cv2.waitKey(10)

capture.release()
cv2.destroyAllWindows()
201/2:
import cv2, pafy # pip install pafy ,  pip3 install youtube-dl
import numpy as np

url = "https://www.youtube.com/watch?v=_bCJ6RiU36U&t=398s"

video = pafy.ydl(url)
best  = video.getbest(preftype="webm")
#documentation: https://pypi.org/project/pafy/

capture = cv2.VideoCapture(best.url)
check, frame = capture.read()
print (check, frame)

cv2.imshow('frame',frame)
cv2.waitKey(10)

capture.release()
cv2.destroyAllWindows()
202/1:
import cv2, pafy # pip install pafy ,  pip3 install youtube-dl
import numpy as np

url = "https://www.youtube.com/watch?v=_bCJ6RiU36U&t=398s"

video = pafy.ydl(url)
best  = video.getbest(preftype="webm")
#documentation: https://pypi.org/project/pafy/

capture = cv2.VideoCapture(best.url)
check, frame = capture.read()
print (check, frame)

cv2.imshow('frame',frame)
cv2.waitKey(10)

capture.release()
cv2.destroyAllWindows()
203/1:
import cv2, pafy # pip install pafy ,  pip3 install youtube-dl
import numpy as np

url = "https://www.youtube.com/watch?v=_bCJ6RiU36U&t=398s"

video = pafy.urlopen(url)
best  = video.getbest(preftype="webm")
#documentation: https://pypi.org/project/pafy/

capture = cv2.VideoCapture(best.url)
check, frame = capture.read()
print (check, frame)

cv2.imshow('frame',frame)
cv2.waitKey(10)

capture.release()
cv2.destroyAllWindows()
204/1:
import cv2, pafy # pip install pafy ,  pip3 install youtube-dl
import numpy as np

url = "https://www.youtube.com/watch?v=_bCJ6RiU36U&t=398s"

video = pafy.YtdlPafy(url)
best  = video.getbest(preftype="webm")
#documentation: https://pypi.org/project/pafy/

capture = cv2.VideoCapture(best.url)
check, frame = capture.read()
print (check, frame)

cv2.imshow('frame',frame)
cv2.waitKey(10)

capture.release()
cv2.destroyAllWindows()
205/1:
import cv2, pafy # pip install pafy ,  pip3 install youtube-dl
import numpy as np

url = "https://www.youtube.com/watch?v=_bCJ6RiU36U&t=398s"

video = pafy.url(url)
best  = video.getbest(preftype="webm")
#documentation: https://pypi.org/project/pafy/

capture = cv2.VideoCapture(best.url)
check, frame = capture.read()
print (check, frame)

cv2.imshow('frame',frame)
cv2.waitKey(10)

capture.release()
cv2.destroyAllWindows()
205/2:
url = "https://www.youtube.com/watch?v=bMt47wvK6u0"
video = pafy.new(url)
205/3: video
205/4:
import cv2, pafy # pip install pafy ,  pip3 install youtube-dl
import numpy as np

url = "https://www.youtube.com/watch?v=_bCJ6RiU36U&t=398s"

video = pafy.new(url)
best  = video.getbest(preftype="webm")
#documentation: https://pypi.org/project/pafy/

capture = cv2.VideoCapture(best.url)
check, frame = capture.read()
print (check, frame)

cv2.imshow('frame',frame)
cv2.waitKey(10)

capture.release()
cv2.destroyAllWindows()
205/5:
import cv2, pafy # pip install pafy ,  pip3 install youtube-dl
import numpy as np

url = "https://www.youtube.com/watch?v=_bCJ6RiU36U&t=398s"

video = pafy.new(url)
# best  = video.getbest(preftype="webm")
#documentation: https://pypi.org/project/pafy/

capture = cv2.VideoCapture(video)
check, frame = capture.read()
print (check, frame)

cv2.imshow('frame',frame)
cv2.waitKey(10)

capture.release()
cv2.destroyAllWindows()
205/6:
from pytube import Youtube
link = ""
yt = YouTube(link)

stream = yt.streams.get_highest_resolution()
%% time stream.download()

print("Download Successful!!")
205/7:
from pytube import Youtube
link = ""
yt = YouTube(link)

stream = yt.streams.get_highest_resolution()
%% time stream.download()

print("Download Successful!!")
205/8:
from pytube import YouTube
link = ""
yt = YouTube(link)

stream = yt.streams.get_highest_resolution()
%% time stream.download()

print("Download Successful!!")
205/9:
from pytube import YouTube
link = "https://www.youtube.com/watch?v=LSUFJmtWx-E"
yt = YouTube(link)

stream = yt.streams.get_highest_resolution()
%% time stream.download()

print("Download Successful!!")
205/10:
from pytube import YouTube

link = "https://www.youtube.com/watch?v=LSUFJmtWx-E"
yt = YouTube(link)

stream = yt.streams.get_highest_resolution()
%% time stream.download()

print("Download Successful!!")
205/11:
from pytube import YouTube
import time
start_time = time.time()

link = "https://www.youtube.com/watch?v=LSUFJmtWx-E"
yt = YouTube(link)

stream = yt.streams.get_highest_resolution()
stream.download()
print("--- %s seconds ---" % (time.time() - start_time))
print("Download Successful!!")
205/12:
# Python Program to Convert seconds
# into hours, minutes and seconds
 
def convert(seconds):
    seconds = seconds % (24 * 3600)
    hour = seconds // 3600
    seconds %= 3600
    minutes = seconds // 60
    seconds %= 60
     
    return "%d:%02d:%02d" % (hour, minutes, seconds)

convert(33.34911489486694)
205/13:
# Python Program to Convert seconds
# into hours, minutes and seconds
 
def convert(seconds):
    seconds = seconds % (24 * 3600)
    hour = seconds // 3600
    seconds %= 3600
    minutes = seconds // 60
    seconds %= 60
     
    return "%d:%02d:%02d" % (hour, minutes, seconds)

convert(133.34911489486694)
205/14:
# Python Program to Convert seconds
# into hours, minutes and seconds
 
def convert(seconds):
    seconds = seconds % (24 * 3600)
    hour = seconds // 3600
    seconds %= 3600
    minutes = seconds // 60
    seconds %= 60
     
    return "%d:%02d:%02d" % (hour, minutes, seconds)

convert(133.34911489486694) + 'minutes'
205/15:
# Python Program to Convert seconds
# into hours, minutes and seconds
 
def convert(seconds):
    seconds = seconds % (24 * 3600)
    hour = seconds // 3600
    seconds %= 3600
    minutes = seconds // 60
    seconds %= 60
     
    return "%d:%02d:%02d" % (hour, minutes, seconds)

convert(133.34911489486694), 'minutes'
205/16:
# Python Program to Convert seconds
# into hours, minutes and seconds
 
def convert(seconds):
    seconds = seconds % (24 * 3600)
    hour = seconds // 3600
    seconds %= 3600
    minutes = seconds // 60
    seconds %= 60
     
    return "%d:%02d:%02d" % (hour, minutes, seconds)

convert(133.34911489486694) +' minutes'
205/17:
# Python Program to Convert seconds
# into hours, minutes and seconds
 
def convert(seconds):
    seconds = seconds % (24 * 3600)
    hour = seconds // 3600
    seconds %= 3600
    minutes = seconds // 60
    seconds %= 60
     
    return "%d:%02d:%02d" % (hour, minutes, seconds)

convert(133.34911489486694) +' min'
205/18:
import cv2

cap = cv2.VideoCapture('./One Of The Funniest Try Not To Laughs Ever.mp4')

def getFrame(sec):
    cap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)
    hasFrames,image = cap.read()
    if hasFrames:
        cv2.imwrite("image"+str(count)+".jpg", image)     # save frame as JPG file
    return hasFrames
sec = 0
frameRate = 0.5 #//it will capture image in each 0.5 second
count=1
success = getFrame(sec)
while success:
    count = count + 1
    sec = sec + frameRate
    sec = round(sec, 2)
    success = getFrame(sec)
205/19: os.listdir('./')
205/20:
import os
os.listdir('./')
205/21:
import os
c= 0
for i in os.listdir('./'):
    if i[:-3] = 'jpg':
        c+=1
print(str(c),'images')
205/22:
import os
c= 0
for i in os.listdir('./'):
    if i[:-3] == 'jpg':
        c+=1
print(str(c),'images')
205/23: os.listdir('./')
205/24: os.listdir('./')[0]
205/25: os.listdir('./')[23]
205/26:
import os
c= 0
for i in os.listdir('./'):
    if i[:-3] == 'jpg':
        c+=1
print(str(c),'images')
205/27: os.listdir('./')[23][:-3]
205/28: os.listdir('./')[23][-3]
205/29: os.listdir('./')[23][-3:]
205/30:
import os
c= 0
for i in os.listdir('./'):
    if i[-3:] == 'jpg':
        c+=1
print(str(c),'images')
205/31: print("image number 616 and 617 has the code")
207/1:
import cv2
import pytesseract
from pytesseract import Output

img = cv2.imread('image616.jpg')
d = pytesseract.image_to_data(img, output_type=Output.DICT)
keys = list(d.keys())

# date_pattern = '^(0[1-9]|[12][0-9]|3[01])/(0[1-9]|1[012])/(19|20)\d\d$'
date_pattern = '.'

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 60:
        if re.match(date_pattern, d['text'][i]):
            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
            img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('img', img)
cv2.waitKey(0)
208/1:
import cv2
import tesseract
from tesseract import Output

img = cv2.imread('image616.jpg')
d = pytesseract.image_to_data(img, output_type=Output.DICT)
keys = list(d.keys())

# date_pattern = '^(0[1-9]|[12][0-9]|3[01])/(0[1-9]|1[012])/(19|20)\d\d$'
date_pattern = '.'

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 60:
        if re.match(date_pattern, d['text'][i]):
            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
            img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('img', img)
cv2.waitKey(0)
209/1:
import cv2
import pytesseract
from pytesseract import Output

img = cv2.imread('image616.jpg')
d = pytesseract.image_to_data(img, output_type=Output.DICT)
keys = list(d.keys())

# date_pattern = '^(0[1-9]|[12][0-9]|3[01])/(0[1-9]|1[012])/(19|20)\d\d$'
date_pattern = '.'

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 60:
        if re.match(date_pattern, d['text'][i]):
            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
            img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('img', img)
cv2.waitKey(0)
209/2:
import re
import cv2
import pytesseract
from pytesseract import Output

img = cv2.imread('image616.jpg')
d = pytesseract.image_to_data(img, output_type=Output.DICT)
keys = list(d.keys())

# date_pattern = '^(0[1-9]|[12][0-9]|3[01])/(0[1-9]|1[012])/(19|20)\d\d$'
date_pattern = '.'

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 60:
        if re.match(date_pattern, d['text'][i]):
            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
            img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('img', img)
cv2.waitKey(0)
209/3:
import re
import cv2
import pytesseract
from pytesseract import Output

img = cv2.imread('image616.jpg')
d = pytesseract.image_to_data(img, output_type=Output.DICT)
keys = list(d.keys())

date_pattern = '^(0[1-9]|[12][0-9]|3[01])/(0[1-9]|1[012])/(19|20)\d\d$'
# date_pattern = '.'

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 60:
        if re.match(date_pattern, d['text'][i]):
            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
            img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('img', img)
cv2.waitKey(0)
209/4:
import re
import cv2
import pytesseract
from pytesseract import Output

img = cv2.imread('image616.jpg')
d = pytesseract.image_to_data(img, output_type=Output.DICT)
keys = list(d.keys())

# date_pattern = '^(0[1-9]|[12][0-9]|3[01])/(0[1-9]|1[012])/(19|20)\d\d$'
date_pattern = '.'

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 60:
        if re.match(date_pattern, d['text'][i]):
            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
            img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('img', img)
cv2.waitKey(0)
209/5: print(d)
209/6:
import matplotlib.pyplot as plt

plt.imshow(img[720:1440,720:1440]) # y: y+h, x: x+w
209/7:
import matplotlib.pyplot as plt

plt.imshow(img[:,:]) # y: y+h, x: x+w
209/8:
import matplotlib.pyplot as plt

plt.imshow(img[2:,:]) # y: y+h, x: x+w
209/9:
import matplotlib.pyplot as plt

plt.imshow(img[120:,:]) # y: y+h, x: x+w
209/10:
import matplotlib.pyplot as plt

plt.imshow(img[100:,:]) # y: y+h, x: x+w
209/11:
import matplotlib.pyplot as plt

plt.imshow(img[80:,:]) # y: y+h, x: x+w
209/12:
import matplotlib.pyplot as plt

plt.imshow(img[50:,:]) # y: y+h, x: x+w
209/13:
import matplotlib.pyplot as plt

plt.imshow(img[50:,:20]) # y: y+h, x: x+w
209/14:
import matplotlib.pyplot as plt

plt.imshow(img[50:,20:]) # y: y+h, x: x+w
209/15:
import matplotlib.pyplot as plt

plt.imshow(img[50:,200:]) # y: y+h, x: x+w
209/16:
import matplotlib.pyplot as plt

plt.imshow(img[50:,1200:]) # y: y+h, x: x+w
209/17:
import matplotlib.pyplot as plt

plt.imshow(img[50:,120:]) # y: y+h, x: x+w
209/18:
import matplotlib.pyplot as plt

plt.imshow(img[50:,120:500]) # y: y+h, x: x+w
209/19:
import matplotlib.pyplot as plt

plt.imshow(img[50:,120:600]) # y: y+h, x: x+w
209/20:
import matplotlib.pyplot as plt

plt.imshow(img[50:,120:570]) # y: y+h, x: x+w
209/21:
import matplotlib.pyplot as plt

plt.imshow(img[50:,120:550]) # y: y+h, x: x+w
209/22:
import matplotlib.pyplot as plt

plt.imshow(img[50:300,120:550]) # y: y+h, x: x+w
209/23:
import matplotlib.pyplot as plt

plt.imshow(img[50:270,120:550]) # y: y+h, x: x+w
209/24:
import matplotlib.pyplot as plt

plt.imshow(img[50:250,120:550]) # y: y+h, x: x+w
209/25:
import matplotlib.pyplot as plt

plt.imshow(img[54:250,120:550]) # y: y+h, x: x+w
209/26:
import matplotlib.pyplot as plt

plt.imshow(img[60:250,120:550]) # y: y+h, x: x+w
209/27:
import matplotlib.pyplot as plt

plt.imshow(img[40:250,120:550]) # y: y+h, x: x+w
209/28:
import re
import cv2
import pytesseract
from pytesseract import Output

img = cv2.imread('image616.jpg')
d = pytesseract.image_to_data(img, output_type=Output.DICT)
keys = list(d.keys())

date_pattern = '^(0[1-9]|[12][0-9]|3[01])/(0[1-9]|1[012])/(19|20)\d\d$'
# date_pattern = '.'

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 60:
        if re.match(date_pattern, d['text'][i]):
            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
            img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('img', img)
cv2.waitKey(0)
209/29:
import re
import cv2
import pytesseract
from pytesseract import Output

img = cv2.imread('image616.jpg')
d = pytesseract.image_to_data(img, output_type=Output.DICT)
keys = list(d.keys())

date_pattern = '^(0[1-9]|[12][0-9]|3[01])/(0[1-9]|1[012])/(19|20)\d\d$'
# date_pattern = '.'

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 60:
        if re.match(date_pattern, d['text'][i]):
            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
            img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('img', img)
cv2.waitKey(0)
210/1:
import re
import cv2
import pytesseract
from pytesseract import Output

img = cv2.imread('image616.jpg')
d = pytesseract.image_to_data(img, output_type=Output.DICT)
keys = list(d.keys())

date_pattern = '^(0[1-9]|[12][0-9]|3[01])/(0[1-9]|1[012])/(19|20)\d\d$'
# date_pattern = '.'

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 60:
        if re.match(date_pattern, d['text'][i]):
            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
            img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('img', img)
cv2.waitKey(0)
210/2: keys
210/3: import easyocr
210/4:
reader = easyocr.Reader(['en'])

result = reader.readtext('image616.jpg',paragraph="FALSE")
result
210/5:
result = reader.readtext('image617.jpg',paragraph="FALSE")
result
210/6:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
print(pytesseract.image_to_string(img))

cv2.imshow('Result', img)
cv2.waitKey(0)
210/7:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
print(pytesseract.image_to_string(img))

# cv2.imshow('Result', img)
# cv2.waitKey(0)
210/8:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
print(pytesseract.image_to_string(img))

# cv2.imshow('Result', img)
# cv2.waitKey(0)
210/9:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
print(pytesseract.image_to_string(img))

cv2.imshow('Result', img)
cv2.waitKey(0)
210/10:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
print(pytesseract.image_to_string(img))

cv2.imshow('Result', img)
cv2.waitKey(0)
210/11:
top_left = tuple(result[0][0][0])
bottom_right = tuple(result[0][0][2])

text = result[0][1]
font = cv2.FONT_HERSHEY_SIMPLEX

img = cv3.imread('image616.jpg')
img = cv2.rectangle(img, text, bottom_right, font, (0,255,0), 2, cv2.LINE_AA)

plt.figure(figsize=(10,10))
plt.imshow(img)
plt.show()
210/12:
top_left = tuple(result[0][0][0])
bottom_right = tuple(result[0][0][2])

text = result[0][1]
font = cv2.FONT_HERSHEY_SIMPLEX

img = cv2.imread('image616.jpg')
img = cv2.rectangle(img, text, bottom_right, font, (0,255,0), 2, cv2.LINE_AA)

plt.figure(figsize=(10,10))
plt.imshow(img)
plt.show()
210/13:
top_left = tuple(result[0][0][0])
bottom_right = tuple(result[0][0][2])

text = result[0][1]
font = cv2.FONT_HERSHEY_SIMPLEX

img = cv2.imread('image616.jpg')
img = cv2.rectangle(img, text, bottom_right, font, (0,255,0), 2, cv2.LINE_AA)

plt.figure(figsize=(10,10))
plt.imshow(img)
plt.show()
210/14:
top_left = tuple(result[0][0][0])
bottom_right = tuple(result[0][0][2])

text = result[1][1]
font = cv2.FONT_HERSHEY_SIMPLEX

img = cv2.imread('image616.jpg')
img = cv2.rectangle(img, text, bottom_right, font, (0,255,0), 2, cv2.LINE_AA)

plt.figure(figsize=(10,10))
plt.imshow(img)
plt.show()
210/15:
top_left = tuple(result[0][0][0])
bottom_right = tuple(result[0][0][2])

text = result[2][1]
font = cv2.FONT_HERSHEY_SIMPLEX

img = cv2.imread('image616.jpg')
img = cv2.rectangle(img, text, bottom_right, font, (0,255,0), 2, cv2.LINE_AA)

plt.figure(figsize=(10,10))
plt.imshow(img)
plt.show()
210/16:
top_left = tuple(result[0][0][0])
bottom_right = tuple(result[0][0][2])

text = result[0][1]
font = cv2.FONT_HERSHEY_SIMPLEX

img = cv2.imread('image616.jpg')
img = cv2.rectangle(img, text, bottom_right, font, (0,255,0), 2, cv2.LINE_AA)

plt.figure(figsize=(10,10))
plt.imshow(img)
plt.show()
210/17:
top_left = tuple(result[0][0][0])
bottom_right = tuple(result[0][0][2])

text = result[0][1]
font = cv2.FONT_HERSHEY_SIMPLEX

img = cv2.imread('image616.jpg')
img = cv2.rectangle(img, text, bottom_right, font, (0,255,0), 2)

plt.figure(figsize=(10,10))
plt.imshow(img)
plt.show()
210/18:
top_left = tuple(result[0][0][0])
bottom_right = tuple(result[0][0][2])

text = result[0][1]
font = cv2.FONT_HERSHEY_SIMPLEX

img = cv2.imread('image616.jpg')
img = cv2.rectangle(img, text, bottom_right, (0,255,0), 2, cv2.LINE_AA)

plt.figure(figsize=(10,10))
plt.imshow(img)
plt.show()
210/19:
top_left = tuple(result[0][0][0])
bottom_right = tuple(result[0][0][2])

text = result[0][1]
font = cv2.FONT_HERSHEY_SIMPLEX

img = cv2.imread('demo.png')
img = cv2.rectangle(img, text, bottom_right, font, (0,255,0), 2, cv2.LINE_AA)

plt.figure(figsize=(10,10))
plt.imshow(img)
plt.show()
210/20:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('demo.png')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
print(pytesseract.image_to_string(img))

cv2.imshow('Result', img)
cv2.waitKey(0)
210/21:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('demo.png')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
print(pytesseract.image_to_string(img))

cv2.imshow('Result', img)
cv2.waitKey(0)
210/22:
import cv2 as cv
import numpy as np
from matplotlib import pyplot as plt
img = cv.imread('demo.png',0)
ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv.threshold(img,127,255,cv.THRESH_BINARY_INV)
ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
for i in range(6):
    plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
    plt.title(titles[i])
    plt.xticks([]),plt.yticks([])
plt.show()
210/23:
import cv2 as cv
import numpy as np
from matplotlib import pyplot as plt
img = cv.imread('demo.png',0)
# ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv.threshold(img,127,255,cv.THRESH_BINARY_INV)
# ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
# ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
# ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
# titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
# images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
# for i in range(6):
#     plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
#     plt.title(titles[i])
#     plt.xticks([]),plt.yticks([])
plt.show(thresh2)
210/24:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('demo.png',0)
# ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV)
# ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
# ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
# ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
# titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
# images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
# for i in range(6):
#     plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
#     plt.title(titles[i])
#     plt.xticks([]),plt.yticks([])
plt.show(thresh2)
210/25:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('demo.png',0)
# ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV)
# ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
# ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
# ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
# titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
# images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
# for i in range(6):
#     plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
#     plt.title(titles[i])
#     plt.xticks([]),plt.yticks([])
plt.imshow(thresh2)
210/26:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
# ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV)
# ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
# ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
# ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
# titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
# images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
# for i in range(6):
#     plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
#     plt.title(titles[i])
#     plt.xticks([]),plt.yticks([])
plt.imshow(thresh2)
210/27:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
# ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY_INV)
# ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
# ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
# ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
# titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
# images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
# for i in range(6):
#     plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
#     plt.title(titles[i])
#     plt.xticks([]),plt.yticks([])
plt.imshow(thresh2)
210/28:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
# ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
# ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
# ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
# ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
# titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
# images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
# for i in range(6):
#     plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
#     plt.title(titles[i])
#     plt.xticks([]),plt.yticks([])
plt.imshow(thresh2)
210/29:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('demo.png')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
print(pytesseract.image_to_string(thresh2))

cv2.imshow('Result', img)
cv2.waitKey(0)
210/30:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
print(pytesseract.image_to_string(thresh2))

cv2.imshow('Result', img)
cv2.waitKey(0)
210/31:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('demo.png')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
print(pytesseract.image_to_string(thresh2))

cv2.imshow('Result', img)
cv2.waitKey(0)
210/32:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
# ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
# ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
# ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
# ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
# titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
# images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
# for i in range(6):
#     plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
#     plt.title(titles[i])
#     plt.xticks([]),plt.yticks([])
# plt.imshow(thresh2)
cv2.imshow('Result', thresh2)
cv2.waitKey(0)
210/33:
import re
import cv2
import pytesseract
from pytesseract import Output

img = cv2.imread('image616.jpg')
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
d = pytesseract.image_to_data(thresh, output_type=Output.DICT)
keys = list(d.keys())

date_pattern = '^([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])-([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])-([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])'
# date_pattern = '.'

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 60:
        if re.match(date_pattern, d['text'][i]):
            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
            img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('img', img)
cv2.waitKey(0)
210/34:
import re
import cv2
import pytesseract
from pytesseract import Output

img = cv2.imread('image616.jpg')
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
d = pytesseract.image_to_data(thresh, output_type=Output.DICT)
keys = list(d.keys())

date_pattern = '^([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])-([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])-([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])'
# date_pattern = '.'

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        if re.match(date_pattern, d['text'][i]):
            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
            img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('img', img)
cv2.waitKey(0)
210/35: print(d)
210/36:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
# ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
# ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
# ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
# ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
# titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
# images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
# for i in range(6):
#     plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
#     plt.title(titles[i])
#     plt.xticks([]),plt.yticks([])
# plt.imshow(thresh2)
cv2.imshow('Result', thresh2)
cv2.waitKey(0)
210/37:
import re
import cv2
import pytesseract
from pytesseract import Output

img = cv2.imread('image616.jpg')
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
d = pytesseract.image_to_data(thresh, output_type=Output.DICT)
keys = list(d.keys())

# date_pattern = '^([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])-([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])-([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])'
# date_pattern = '.'

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
#       if re.match(date_pattern, d['text'][i]):
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('img', img)
cv2.waitKey(0)
210/38: print(d)
210/39: print(KEYS)
210/40: print(keys)
210/41: print(['text'])
210/42: print(d['text'])
210/43:
import re
import cv2
import pytesseract
from pytesseract import Output

img = cv2.imread('demo.png')
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
d = pytesseract.image_to_data(thresh, output_type=Output.DICT)
keys = list(d.keys())

# date_pattern = '^([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])-([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])-([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])'
# date_pattern = '.'

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
#       if re.match(date_pattern, d['text'][i]):
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('img', img)
cv2.waitKey(0)
210/44:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
# ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
# ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
# ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
# ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
# titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
# images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
# for i in range(6):
#     plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
#     plt.title(titles[i])
#     plt.xticks([]),plt.yticks([])
# plt.imshow(thresh2)
cv2.imshow('Result', thresh2)
cv2.waitKey(0)
210/45: print(d['text'])
210/46: print(d['text'].join(''))
210/47: print(d['text'].join(' '))
210/48: print(d['text'])
210/49:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('demo.png')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
print(pytesseract.image_to_string(thresh2))

cv2.imshow('Result', img)
cv2.waitKey(0)
210/50:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
# ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
# ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
# ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
# ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
# titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
# images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
# for i in range(6):
#     plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
#     plt.title(titles[i])
#     plt.xticks([]),plt.yticks([])
# plt.imshow(thresh2)
cv2.imshow('Result', thresh2)
cv2.waitKey(0)
210/51:
import re
import cv2
import pytesseract
from pytesseract import Output

img = cv2.imread('demo.png')
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
d = pytesseract.image_to_data(thresh, output_type=Output.DICT)
keys = list(d.keys())

# date_pattern = '^([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])-([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])-([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])'
# date_pattern = '.'

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
#       if re.match(date_pattern, d['text'][i]):
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('img', img)
cv2.waitKey(0)
210/52:
import re
import cv2
import pytesseract
from pytesseract import Output

img = cv2.imread('iamge616.jpg')
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
d = pytesseract.image_to_data(thresh, output_type=Output.DICT)
keys = list(d.keys())

# date_pattern = '^([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])-([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])-([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])'
# date_pattern = '.'

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
#       if re.match(date_pattern, d['text'][i]):
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('img', img)
cv2.waitKey(0)
210/53:
import re
import cv2
import pytesseract
from pytesseract import Output

img = cv2.imread('image616.jpg')
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
d = pytesseract.image_to_data(thresh, output_type=Output.DICT)
keys = list(d.keys())

# date_pattern = '^([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])-([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])-([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])'
# date_pattern = '.'

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
#       if re.match(date_pattern, d['text'][i]):
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('img', img)
cv2.waitKey(0)
210/54: print(d['text'])
210/55:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
# ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
# ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
# ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
# ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
# titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
# images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
# for i in range(6):
#     plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
#     plt.title(titles[i])
#     plt.xticks([]),plt.yticks([])
# plt.imshow(thresh2)
cv2.imshow('Result', thresh2)
cv2.waitKey(0)
210/56:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
# ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
# ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
# ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
# ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
# titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
# images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
# for i in range(6):
#     plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
#     plt.title(titles[i])
#     plt.xticks([]),plt.yticks([])
# plt.imshow(thresh2)
cv2.imshow('Result', thresh2)
cv2.waitKey(0)
210/57:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
# ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
img = cv2.blur(thresh2, (10,10))
# ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
# ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
# ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
# titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
# images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
# for i in range(6):
#     plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
#     plt.title(titles[i])
#     plt.xticks([]),plt.yticks([])
# plt.imshow(thresh2)
cv2.imshow('Result', img)
cv2.waitKey(0)
210/58:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
# ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
img = cv2.blur(thresh2, (3,3))
# ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
# ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
# ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
# titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
# images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
# for i in range(6):
#     plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
#     plt.title(titles[i])
#     plt.xticks([]),plt.yticks([])
# plt.imshow(thresh2)
cv2.imshow('Result', img)
cv2.waitKey(0)
210/59:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
# ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
img = cv2.blur(thresh2, (1,1))
# ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
# ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
# ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
# titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
# images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
# for i in range(6):
#     plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
#     plt.title(titles[i])
#     plt.xticks([]),plt.yticks([])
# plt.imshow(thresh2)
cv2.imshow('Result', img)
cv2.waitKey(0)
210/60:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
# ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
img = cv2.blur(thresh2, (2,2))
# ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
# ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
# ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
# titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
# images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
# for i in range(6):
#     plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
#     plt.title(titles[i])
#     plt.xticks([]),plt.yticks([])
# plt.imshow(thresh2)
cv2.imshow('Result', img)
cv2.waitKey(0)
210/61:
sr = cv2.dnn_superres.DnnSuperResImpl_create()
 
path = "EDSR_x4.pb"
 
sr.readModel(path)
 
sr.setModel("edsr",4)
 
result = sr.upsample(thresh2)

cv2.imshow('Result', result)
cv2.waitKey(0)
210/62:
sr = cv2.dnn_superres.DnnSuperResImpl_create()
 
path = "EDSR_x4.pb"
 
sr.readModel(path)
 
sr.setModel("edsr",1)
 
result = sr.upsample(thresh2)

cv2.imshow('Result', result)
cv2.waitKey(0)
210/63:
sr = cv2.dnn_superres.DnnSuperResImpl_create()
 
path = "EDSR_x4.pb"
 
sr.readModel(path)
 
sr.setModel("edsr",4)

colored = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
 
result = sr.upsample(colored)

cv2.imshow('Result', result)
cv2.waitKey(0)
210/64:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread(result)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
# ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
print(pytesseract.image_to_string(img))

cv2.imshow('Result', img)
cv2.waitKey(0)
210/65: plt.show(result)
210/66: plt.imshow(result)
210/67:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread(result)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
print(pytesseract.image_to_string(result))

cv2.imshow('Result', img)
cv2.waitKey(0)
210/68:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('demo.png')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
print(pytesseract.image_to_string(result))

cv2.imshow('Result', result)
cv2.waitKey(0)
210/69:
import re
import cv2
import pytesseract
from pytesseract import Output

img = cv2.imread('image616.jpg')
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
d = pytesseract.image_to_data(result, output_type=Output.DICT)
keys = list(d.keys())

# date_pattern = '^([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])-([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])-([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])([0-9]|[A-Z])'
# date_pattern = '.'

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
#       if re.match(date_pattern, d['text'][i]):
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('img', img)
cv2.waitKey(0)
210/70: print(d['text'])
210/71:
from PIL import Image, ImageEnhance

image = Image.open("image616.jpg")

enhancer = ImageEnhance.Contrast(image)

image_output = enhancer.enhance(1)

plt.show(cv2.imread(image_output))
210/72:
from PIL import Image, ImageEnhance
import numpy as np

image = Image.open("image616.jpg")

enhancer = ImageEnhance.Contrast(image)

image_output = enhancer.enhance(1)

plt.show(np.array(image_output))
210/73:
from PIL import Image, ImageEnhance
import numpy as np

image = Image.open("image616.jpg")

enhancer = ImageEnhance.Contrast(image)

image_output = enhancer.enhance(1)
type(image_output)
plt.show(np.array(image_output))
210/74:
from PIL import Image, ImageEnhance
import numpy as np

image = Image.open("image616.jpg")

enhancer = ImageEnhance.Contrast(image)

image_output = enhancer.enhance(1)
print(type(image_output))
plt.show(np.array(image_output))
210/75:
from PIL import Image, ImageEnhance
import numpy as np

image = Image.open("image616.jpg")

enhancer = ImageEnhance.Contrast(image)

image_output = enhancer.enhance(1)
print(type(image_output))
plt.imshow(np.array(image_output))
210/76:
from PIL import Image, ImageEnhance
import numpy as np

image = Image.open("image616.jpg")
ret,thresh = cv2.threshold(image,200,255,cv2.THRESH_BINARY)

enhancer = ImageEnhance.Contrast(Image.fromarray(thresh))

image_output = enhancer.enhance(1)
print(type(image_output))
plt.imshow(np.array(image_output))
210/77:
from PIL import Image, ImageEnhance
import numpy as np

image = cv2.imread("image616.jpg")
ret,thresh = cv2.threshold(image,200,255,cv2.THRESH_BINARY)

enhancer = ImageEnhance.Contrast(Image.fromarray(thresh))

image_output = enhancer.enhance(1)
print(type(image_output))
plt.imshow(np.array(image_output))
210/78:
from PIL import Image, ImageEnhance
import numpy as np

image = cv2.imread("image616.jpg")
ret,thresh = cv2.threshold(image,200,255,cv2.THRESH_BINARY)

enhancer = ImageEnhance.Contrast(Image.fromarray(thresh))

image_output = enhancer.enhance(2)
print(type(image_output))
plt.imshow(np.array(image_output))
210/79:
from PIL import Image, ImageEnhance
import numpy as np

image = cv2.imread("image616.jpg")
ret,thresh = cv2.threshold(image,200,255,cv2.THRESH_BINARY)

enhancer = ImageEnhance.Contrast(Image.fromarray(thresh))

image_output = enhancer.enhance(6)
print(type(image_output))
plt.imshow(np.array(image_output))
210/80:
from PIL import Image, ImageEnhance
import numpy as np

image = cv2.imread("image616.jpg")
ret,thresh = cv2.threshold(image,200,255,cv2.THRESH_BINARY)

enhancer = ImageEnhance.Contrast(Image.fromarray(thresh))

image_output = enhancer.enhance(6)

print(pytesseract.image_to_string(cv2.imread(image_output)))

# cv2.imshow('Result', result)
# cv2.waitKey(0)

plt.imshow(np.array(image_output))
210/81:
from PIL import Image, ImageEnhance
import numpy as np

image = cv2.imread("image616.jpg")
ret,thresh = cv2.threshold(image,200,255,cv2.THRESH_BINARY)

enhancer = ImageEnhance.Contrast(Image.fromarray(thresh))

image_output = enhancer.enhance(6)

print(pytesseract.image_to_string(image_output))

# cv2.imshow('Result', result)
# cv2.waitKey(0)

plt.imshow(np.array(image_output))
210/82:
from PIL import Image, ImageEnhance
import numpy as np

image = cv2.imread("image616.jpg")
ret,thresh = cv2.threshold(image,200,255,cv2.THRESH_BINARY)

enhancer = ImageEnhance.Contrast(Image.fromarray(thresh))

image_output = enhancer.enhance(15)

print(pytesseract.image_to_string(image_output))

# cv2.imshow('Result', result)
# cv2.waitKey(0)

plt.imshow(np.array(image_output))
210/83:
from PIL import Image, ImageEnhance
import numpy as np

image = cv2.imread("image616.jpg")
ret,thresh = cv2.threshold(image,210,255,cv2.THRESH_BINARY)

enhancer = ImageEnhance.Contrast(Image.fromarray(thresh))

image_output = enhancer.enhance(15)

print(pytesseract.image_to_string(image_output))

# cv2.imshow('Result', result)
# cv2.waitKey(0)

plt.imshow(np.array(image_output))
210/84:
from PIL import Image, ImageEnhance
import numpy as np

image = cv2.imread("image616.jpg")
ret,thresh = cv2.threshold(image,210,255,cv2.THRESH_BINARY)

enhancer = ImageEnhance.Contrast(Image.fromarray(thresh))

image_output = enhancer.enhance(15)

print(pytesseract.image_to_string(image_output))

cv2.imshow('Result', cv2.imread(image_output))
cv2.waitKey(0)

plt.imshow(np.array(image_output))
210/85:
from PIL import Image, ImageEnhance
import numpy as np

image = cv2.imread("image616.jpg")
ret,thresh = cv2.threshold(image,210,255,cv2.THRESH_BINARY)

enhancer = ImageEnhance.Contrast(Image.fromarray(thresh))

image_output = enhancer.enhance(15)

print(pytesseract.image_to_string(image_output))

cv2.imshow('Result', np.array(image_output))
cv2.waitKey(0)

plt.imshow(np.array(image_output))
210/86:
from PIL import Image, ImageEnhance
import numpy as np

image = cv2.imread("image616.jpg")
ret,thresh = cv2.threshold(image,210,255,cv2.THRESH_BINARY)

enhancer = ImageEnhance.Contrast(Image.fromarray(thresh))

image_output = enhancer.enhance(15)

# print(pytesseract.image_to_string(image_output))

# cv2.imshow('Result', np.array(image_output))
# cv2.waitKey(0)

# plt.imshow(np.array(image_output))

d = pytesseract.image_to_data(image_output, output_type=Output.DICT)
keys = list(d.keys())


n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
#       if re.match(date_pattern, d['text'][i]):
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        img = cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)
210/87: d
210/88:
result = reader.readtext(image_output,paragraph="FALSE")
result
210/89:
result = reader.readtext(np.array(image_output),paragraph="FALSE")
result
210/90:
result = reader.readtext(thresh,paragraph="FALSE")
result
210/91:
result = reader.readtext(thresh2,paragraph="FALSE")
result
210/92:
result = reader.readtext(thresh,paragraph="FALSE")
result
210/93:
result = reader.readtext(image,paragraph="FALSE")
result
210/94:
cany = cv2.Canny(thresh, 100, 200)

cv2.imshow('Result', cany)
cv2.waitKey(0)
210/95:
cany = cv2.Canny(np.array(image_output), 100, 200)

cv2.imshow('Result', cany)
cv2.waitKey(0)
210/96:
img = cv2.blur(np.array(image_output), (2,2))
cany = cv2.Canny(img, 100, 200)

cv2.imshow('Result', cany)
cv2.waitKey(0)
210/97:
img = cv2.blur(np.array(image_output), (5,5))
cany = cv2.Canny(img, 100, 200)

cv2.imshow('Result', cany)
cv2.waitKey(0)
210/98:
img = cv2.blur(np.array(image_output), (8,8))
cany = cv2.Canny(img, 100, 200)

cv2.imshow('Result', cany)
cv2.waitKey(0)
210/99:
img = cv2.blur(np.array(image_output), (6,6))
cany = cv2.Canny(img, 100, 200)

cv2.imshow('Result', cany)
cv2.waitKey(0)
210/100:
img = cv2.blur(np.array(image_output), (4,4))
cany = cv2.Canny(img, 100, 200)

cv2.imshow('Result', cany)
cv2.waitKey(0)
210/101:
d = pytesseract.image_to_data(cany, output_type=Output.DICT)
d
210/102:
d = pytesseract.image_to_data(cany, output_type=Output.DICT)
print(d)
210/103: result
210/104:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
# img = cv2.blur(thresh2, (2,2))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
210/105:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
img = cv2.blur(thresh, (2,2))

cv2.imshow('Result', img)
cv2.waitKey(0)
210/106:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
img = cv2.blur(thresh, (3,3))

cv2.imshow('Result', img)
cv2.waitKey(0)
210/107:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
img = cv2.blur(thresh, (2,2))


cv2.imshow('Result', img)
cv2.waitKey(0)
211/1:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('correct_dim.png')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
# ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
print(pytesseract.image_to_string(img))

cv2.imshow('Result', result)
cv2.waitKey(0)
211/2:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('correct_dim.png')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
# ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
print(pytesseract.image_to_string(img))

cv2.imshow('Result', img)
cv2.waitKey(0)
211/3:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('correct_dim.png')
medianBlurred = cv2.medianBlur(img, 7)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
# ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/4:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('correct_dim.png')
medianBlurred = cv2.medianBlur(img, 3)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
# ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/5:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('correct_dim.png')
medianBlurred = cv2.medianBlur(img, 1)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
# ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/6:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('correct_dim.png')
medianBlurred = cv2.medianBlur(img, 3)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
# ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/7:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('correct_dim.png')
medianBlurred = cv2.medianBlur(img, 3)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
# ret,thresh2 = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/8:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 3)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/9:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 3)

print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/10:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/11:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/12:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)

print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/13:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
medianBlurred = cv2.medianBlur(thresh, 1)

print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/14:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
medianBlurred = cv2.medianBlur(thresh, 2)

print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/15:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
medianBlurred = cv2.medianBlur(thresh, 3)

print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/16:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
# medianBlurred = cv2.medianBlur(thresh, 3)

print(pytesseract.image_to_string(thresh))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/17:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV)
# medianBlurred = cv2.medianBlur(thresh, 3)

print(pytesseract.image_to_string(thresh))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/18:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,240,255,cv2.THRESH_BINARY_INV)
# medianBlurred = cv2.medianBlur(thresh, 3)

print(pytesseract.image_to_string(thresh))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/19:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,0,255,cv2.THRESH_BINARY_INV)
# medianBlurred = cv2.medianBlur(thresh, 3)

print(pytesseract.image_to_string(thresh))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/20:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
# ret,thresh = cv2.threshold(img,0,255,cv2.THRESH_BINARY_INV)
# medianBlurred = cv2.medianBlur(thresh, 3)

print(pytesseract.image_to_string(img))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/21:
import matplotlib.pyplot as plt 
plt.show(img)
211/22:
import matplotlib.pyplot as plt 
plt.imshow(img)
211/23:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
# ret,thresh = cv2.threshold(img,0,255,cv2.THRESH_BINARY_INV)
# medianBlurred = cv2.medianBlur(thresh, 3)

print(pytesseract.image_to_string(img))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/24:
import matplotlib.pyplot as plt 
plt.imshow(img)
211/25:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
# ret,thresh = cv2.threshold(img,0,255,cv2.THRESH_BINARY_INV)
# medianBlurred = cv2.medianBlur(thresh, 3)

print(pytesseract.image_to_string(img))

cv2.imshow('Result', img)
cv2.waitKey(0)
211/26:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
# ret,thresh = cv2.threshold(img,0,255,cv2.THRESH_BINARY_INV)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', img)
cv2.waitKey(0)
211/27:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,0,255,cv2.THRESH_BINARY_INV)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/28:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY_INV)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/29:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/30:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,128,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/31:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,245,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/32:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,230,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/33:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,230,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/34:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
# ret,thresh = cv2.threshold(img,230,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', img)
cv2.waitKey(0)
211/35:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,120,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/36:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,120,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/37:
import cv2
import numpy as np
from matplotlib import pyplot as plt
img = cv2.imread('image616.jpg',0)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
img = cv2.blur(thresh, (2,2))


cv2.imshow('Result', img)
cv2.waitKey(0)
211/38:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,120,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/39:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,222,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/40:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/41:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,190,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/42:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,190,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/43:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,150,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/44:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,170,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/45:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

# print(pytesseract.image_to_string(img))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/46:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

print(pytesseract.image_to_string(thresh))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/47:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 3)

print(pytesseract.image_to_string(thresh))

cv2.imshow('Result', thresh)
cv2.waitKey(0)
211/48:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 3)

print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/49:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)

print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/50:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)

print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/51:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)

print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
211/52:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)

print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
212/1:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)

print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
212/2:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)

print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
212/3:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
flip = cv2.flip(medianBlurred,90)

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', medianBlurred)
cv2.waitKey(0)
212/4:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
flip = cv2.flip(medianBlurred,90)

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', flip)
cv2.waitKey(0)
212/5:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
flip = cv2.rotate(medianBlurred,90)

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', flip)
cv2.waitKey(0)
212/6:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
flip = cv2.rotate(medianBlurred,1)

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', flip)
cv2.waitKey(0)
212/7:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
flip = cv2.rotate(medianBlurred,2)

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', flip)
cv2.waitKey(0)
212/8:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
flip = cv2.rotate(medianBlurred,5)

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', flip)
cv2.waitKey(0)
212/9:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
flip = cv2.rotate(medianBlurred,4)

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', flip)
cv2.waitKey(0)
212/10:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
flip = cv2.rotate(medianBlurred,3)

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', flip)
cv2.waitKey(0)
212/11:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
flip = cv2.rotate(medianBlurred,2)

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', flip)
cv2.waitKey(0)
212/12:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
flip = cv2.rotate(medianBlurred,-2)

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', flip)
cv2.waitKey(0)
212/13:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = img.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 45, 2.0) # (Center, angle of rotation, scale of zooming)

rotate = cv2.wrapAffine(img, M, (w,h)

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', flip)
cv2.waitKey(0)
212/14:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 45, 2.0) # (Center, angle of rotation, scale of zooming)

rotate = cv2.wrapAffine(medianBlurred, M, (w,h)

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/15:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 45, 2.0) # (Center, angle of rotation, scale of zooming)

rotate = cv2.wrapAffine(medianBlurred, M, (w,h)

# print(pytesseract.image_to_string(medianBlurred))

cv2.imShow('Result', rotate)
cv2.waitKey(0)
212/16:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 45, 2.0) # (Center, angle of rotation, scale of zooming)

rotate = cv2.wrapAffine(medianBlurred, M, (w,h)

# print(pytesseract.image_to_string(medianBlurred))
plt.imshow(rotate)
# cv2.imshow('Result', rotate)
# cv2.waitKey(0)
212/17:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 45, 2.0) # (Center, angle of rotation, scale of zooming)

rotate = cv2.wrapAffine(medianBlurred, M, (w,h)

# print(pytesseract.image_to_string(medianBlurred))
rotate
# cv2.imshow('Result', rotate)
# cv2.waitKey(0)
212/18:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 45, 2.0) # (Center, angle of rotation, scale of zooming)

rotate = cv2.wrapAffine(medianBlurred, M, (w,h))

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/19:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 45, 2.0) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/20:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 45, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/21:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 135, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/22:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 255, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/23:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 300, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/24:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 350, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/25:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 340, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

# print(pytesseract.image_to_string(medianBlurred))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/26:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 340, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/27:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 340, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

# print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/28:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 345, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

# print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/29:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

# print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/30:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/31:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/32:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 3)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/33:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/34:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/35:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 1)


h, w = thresh.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(thresh, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/36:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(thresh, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/37:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(thresh, (1,1))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/38:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(thresh, (2,1))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/39:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(thresh, (1,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/40:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(thresh, (3,3))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/41:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(thresh, (3,3))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/42:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 3)
# blur = cv2.blur(thresh, (3,3))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/43:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 2)
# blur = cv2.blur(thresh, (3,3))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/44:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
# blur = cv2.blur(thresh, (3,3))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/45:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
# blur = cv2.blur(thresh, (3,3))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/46:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 2)
# blur = cv2.blur(thresh, (3,3))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/47:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
# blur = cv2.blur(thresh, (3,3))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/48:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,200,255,cv2.THRESH_BINARY)
# medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(thresh, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/49:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
# blur = cv2.blur(thresh, (2,2))


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/50:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
# blur = cv2.blur(thresh, (2,2))


h, w = medianBlurred.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(medianBlurred, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/51:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (1,1))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/52:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (1,1))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/53:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,1))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/54:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (1,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/55:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/56:
x,y,w,h = 0,100,10,20
plt.imshow(blur[y:y+h,w:x+w])
212/57:
import matplotlib.pyplot as plt
x,y,w,h = 0,100,10,20
plt.imshow(blur[y:y+h,w:x+w])
212/58:
import matplotlib.pyplot as plt
x,y,w,h = 100,100,100,100
plt.imshow(blur[y:y+h,w:x+w])
212/59:
import matplotlib.pyplot as plt
x,y,w,h = 100,100,100,200
plt.imshow(blur[y:y+h,w:x+w])
212/60:
import matplotlib.pyplot as plt
x,y,w,h = 100,100,100,200
plt.imshow(rotate[y:y+h,w:x+w])
212/61:
import matplotlib.pyplot as plt
x,y,w,h = 200,200,200,200
plt.imshow(rotate[y:y+h,w:x+w])
212/62:
import matplotlib.pyplot as plt
x,y,w,h = 600,600,600,600
plt.imshow(rotate[y:y+h,w:x+w])
212/63:
import matplotlib.pyplot as plt
x,y,w,h = 600,600,600,600
plt.imshow(rotate)
212/64:
import matplotlib.pyplot as plt
x,y,w,h = 100,60,600,600
plt.imshow(rotate[y:y+h,w:x+w])
212/65:
import matplotlib.pyplot as plt
x,y,w,h = 600,600,100,60
plt.imshow(rotate[y:y+h,w:x+w])
212/66:
import matplotlib.pyplot as plt
x,y,w,h = 100,600,100,60
plt.imshow(rotate[y:y+h,w:x+w])
212/67:
import matplotlib.pyplot as plt
x,y,w,h = 100,60,600,60
plt.imshow(rotate[y:y+h,w:x+w])
212/68:
import matplotlib.pyplot as plt
x,y,w,h = 100,60,600,60
plt.imshow(rotate)
212/69:
import matplotlib.pyplot as plt
x,y,w,h = 0,1000,1000,1000
plt.imshow(rotate)
212/70:
import matplotlib.pyplot as plt
x,y,w,h = 10,1000,1000,1000
plt.imshow(rotate)
212/71:
import matplotlib.pyplot as plt
x,y,w,h = 0,1000,1000,1000
plt.imshow(rotate)
212/72:
import matplotlib.pyplot as plt
x,y,w,h = 0,100,1000,1000
plt.imshow(rotate)
212/73:
import matplotlib.pyplot as plt
x,y,w,h = 0,150,1000,1000
plt.imshow(rotate)
212/74:
import matplotlib.pyplot as plt
x,y,w,h = 0,150,100,1000
plt.imshow(rotate)
212/75:
import matplotlib.pyplot as plt
x,y,w,h = 0,150,100,100
plt.imshow(rotate)
212/76:
import matplotlib.pyplot as plt
x,y,w,h = 50,150,100,100
plt.imshow(rotate)
212/77:
import matplotlib.pyplot as plt
x,y,w,h = 150,150,100,100
plt.imshow(rotate)
212/78:
import matplotlib.pyplot as plt
x,y,w,h = 0,1000,1000,1000
plt.imshow(rotate[y:y+h,w:x+w])
212/79:
import matplotlib.pyplot as plt
x,y,w,h = 10,1000,1000,1000
plt.imshow(rotate[y:y+h,w:x+w])
212/80:
import matplotlib.pyplot as plt
x,y,w,h = 1000,1000,1000,1000
plt.imshow(rotate[y:y+h,w:x+w])
212/81:
import matplotlib.pyplot as plt
x,y,w,h = 1000,1000,100,1000
plt.imshow(rotate[y:y+h,w:x+w])
212/82:
import matplotlib.pyplot as plt
x,y,w,h = 10,1000,1000,1000
plt.imshow(rotate[y:y+h,w:x+w])
212/83:
import matplotlib.pyplot as plt
x,y,w,h = 1000,10,100,100
plt.imshow(rotate[y:y+h,w:x+w])
212/84:
import matplotlib.pyplot as plt
x,y,w,h = 500,10,100,100
plt.imshow(rotate[y:y+h,w:x+w])
212/85:
import matplotlib.pyplot as plt
x,y,w,h = 500,50,100,100
plt.imshow(rotate[y:y+h,w:x+w])
212/86:
import matplotlib.pyplot as plt
x,y,w,h = 500,10,100,100
plt.imshow(rotate[y:y+h,w:x+w])
212/87:
import matplotlib.pyplot as plt
x,y,w,h = 500,10,130,100
plt.imshow(rotate[y:y+h,w:x+w])
212/88:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

print(pytesseract.image_to_string(rotate[y:y+h,w:x+w]))

cv2.imshow('Result', rotate[y:y+h,w:x+w])
cv2.waitKey(0)
212/89:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))

# print(pytesseract.image_to_string(rotate[y:y+h,w:x+w]))

cv2.imshow('Result', rotate[y:y+h,w:x+w])
cv2.waitKey(0)
212/90:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
rotate = rotate[y:y+h,w:x+w]

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/91:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 500,10,130,100
rotate = rotate[10:10+100,130:500+130]

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/92:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 500,10,130,100
rotate = rotate[10:10+100,130:500+130]

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/93:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,155,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 500,10,130,100
rotate = rotate[10:10+100,130:500+130]

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/94:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,155,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 500,10,130,100
rotate = rotate[10:10+100,130:500+130]

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/95:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,165,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 500,10,130,100
rotate = rotate[10:10+100,130:500+130]

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/96:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 500,10,130,100
rotate = rotate[10:10+100,130:500+130]

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/97:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,185,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 500,10,130,100
rotate = rotate[10:10+100,130:500+130]

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/98:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 500,10,130,100
rotate = rotate[10:10+100,130:500+130]

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/99:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 490,10,130,100
plt.imshow(rotate[y:y+h,w:x+w])
212/100:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 490,10,120,100
plt.imshow(rotate[y:y+h,w:x+w])
212/101:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 490,10,130,100
plt.imshow(rotate[y:y+h,w:x+w])
212/102:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 490,8,130,100
plt.imshow(rotate[y:y+h,w:x+w])
212/103:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 490,10,80,100
plt.imshow(rotate[y:y+h,w:x+w])
212/104:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 490,10,30,100
plt.imshow(rotate[y:y+h,w:x+w])
212/105:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 490,10,30,90
plt.imshow(rotate[y:y+h,w:x+w])
212/106:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 490,10,30,70
plt.imshow(rotate[y:y+h,w:x+w])
212/107:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 490,10,30,50
plt.imshow(rotate[y:y+h,w:x+w])
212/108:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 480,10,30,50
plt.imshow(rotate[y:y+h,w:x+w])
212/109:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 470,10,30,50
plt.imshow(rotate[y:y+h,w:x+w])
212/110:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 450,10,30,50
plt.imshow(rotate[y:y+h,w:x+w])
212/111:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 450,10,50,50
plt.imshow(rotate[y:y+h,w:x+w])
212/112:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 450,10,30,50
plt.imshow(rotate[y:y+h,w:x+w])
212/113:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 450,14,30,50
plt.imshow(rotate[y:y+h,w:x+w])
212/114:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 450,20,30,50
plt.imshow(rotate[y:y+h,w:x+w])
212/115:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 450,25,30,50
plt.imshow(rotate[y:y+h,w:x+w])
212/116:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 450,30,30,50
plt.imshow(rotate[y:y+h,w:x+w])
212/117:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 450,30,30,30
plt.imshow(rotate[y:y+h,w:x+w])
212/118:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,w:x+w])
212/119:
import matplotlib.pyplot as plt
# x,y,w,h = 500,10,130,100
x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/120:
import matplotlib.pyplot as plt
x,y,w,h = 500,10,130,100
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/121:
import matplotlib.pyplot as plt
x,y,w,h = 50,10,130,100
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/122:
import matplotlib.pyplot as plt
x,y,w,h = 450,10,130,100
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/123:
import matplotlib.pyplot as plt
x,y,w,h = 50,10,130,100
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/124:
import matplotlib.pyplot as plt
x,y,w,h = 50,440,130,100
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/125:
import matplotlib.pyplot as plt
x,y,w,h = 50,40,130,100
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/126:
import matplotlib.pyplot as plt
x,y,w,h = 50,40,10,100
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/127:
import matplotlib.pyplot as plt
x,y,w,h = 50,40,10,200
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/128:
import matplotlib.pyplot as plt
x,y,w,h = 50,400,10,200
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/129:
import matplotlib.pyplot as plt
x,y,w,h = 500,40,10,200
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/130:
import matplotlib.pyplot as plt
x,y,w,h = 10,200,10,200
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/131:
import matplotlib.pyplot as plt
x,y,w,h = 10,200,100,200
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/132:
import matplotlib.pyplot as plt
x,y,w,h = 10,200,100,20
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/133:
import matplotlib.pyplot as plt
x,y,w,h = 10,200,1000,202
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/134:
import matplotlib.pyplot as plt
x,y,w,h = 103,200,1000,202
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/135:
import matplotlib.pyplot as plt
x,y,w,h = 1,200,10,200
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/136:
import matplotlib.pyplot as plt
x,y,w,h = 50,500,550,200
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/137:
import matplotlib.pyplot as plt
x,y,w,h = 50,500,550,1000
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/138:
import matplotlib.pyplot as plt
x,y,w,h = 50,500,50,50
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/139:
import matplotlib.pyplot as plt
x,y,w,h = 50,500,150,50
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/140:
import matplotlib.pyplot as plt
x,y,w,h = 50,500,0,50
# x,y,w,h = 440,30,30,30
plt.imshow(rotate)
212/141:
import matplotlib.pyplot as plt
x,y,w,h = 500,10,130,100
# x,y,w,h = 440,30,30,30
plt.imshow(rotate)
212/142:
import matplotlib.pyplot as plt
x,y,w,h = 500,10,130,100
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,w:x+w])
212/143:
import matplotlib.pyplot as plt
x,y,w,h = 500,10,130,100
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/144:
import matplotlib.pyplot as plt
x,y,w,h = 50,10,130,100
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/145: blur.shape
212/146:
import matplotlib.pyplot as plt
x,y,w,h = 1280,720,1280,720
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/147:
import matplotlib.pyplot as plt
x,y,w,h = 640,360,640,360
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/148:
import matplotlib.pyplot as plt
x,y,w,h = 600,360,640,360
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/149:
import matplotlib.pyplot as plt
x,y,w,h = 640,300,640,360
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/150:
import matplotlib.pyplot as plt
x,y,w,h = 640,360,600,360
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/151:
import matplotlib.pyplot as plt
x,y,w,h = 340,360,640,360
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/152:
import matplotlib.pyplot as plt
x,y,w,h = 300,360,640,360
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/153:
import matplotlib.pyplot as plt
x,y,w,h = 300,180,640,360
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/154:
import matplotlib.pyplot as plt
x,y,w,h = 300,90,640,360
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/155:
import matplotlib.pyplot as plt
x,y,w,h = 250,90,640,360
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/156:
import matplotlib.pyplot as plt
x,y,w,h = 150,90,640,360
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/157:
import matplotlib.pyplot as plt
x,y,w,h = 100,90,640,360
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/158:
import matplotlib.pyplot as plt
x,y,w,h = 100,10,640,360
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/159:
import matplotlib.pyplot as plt
x,y,w,h = 100,50,640,360
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/160:
import matplotlib.pyplot as plt
x,y,w,h = 100,50,420,360
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/161:
import matplotlib.pyplot as plt
x,y,w,h = 100,50,420,200
# x,y,w,h = 440,30,30,30
plt.imshow(blur[y:y+h,x:x+w])
212/162:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 500,10,130,100
# rotate = rotate[10:10+100,130:500+130]

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/163:
import matplotlib.pyplot as plt
x,y,w,h = 100,50,420,200
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/164:
import matplotlib.pyplot as plt
x,y,w,h = 100,20,420,200
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/165:
import matplotlib.pyplot as plt
x,y,w,h = 150,20,420,200
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/166:
import matplotlib.pyplot as plt
x,y,w,h = 150,20,420,50
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/167:
import matplotlib.pyplot as plt
x,y,w,h = 150,25,420,50
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/168:
import matplotlib.pyplot as plt
x,y,w,h = 150,35,420,50
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/169:
import matplotlib.pyplot as plt
x,y,w,h = 150,35,440,50
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/170:
import matplotlib.pyplot as plt
x,y,w,h = 150,35,450,50
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/171:
import matplotlib.pyplot as plt
x,y,w,h = 150,35,450,35
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/172:
import matplotlib.pyplot as plt
x,y,w,h = 150,40,450,35
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/173:
import matplotlib.pyplot as plt
x,y,w,h = 150,40,450,30
# x,y,w,h = 440,30,30,30
plt.imshow(rotate[y:y+h,x:x+w])
212/174:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]

print(pytesseract.image_to_string(rotate))

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/175:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]

print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(image_output, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 60:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        img = cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/176:
import cv2
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]

print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 60:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        img = cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/177:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]

print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 60:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        img = cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/178:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]

print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
    img = cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/179: n_boxes
212/180: d['text']
212/181: ''.join(d['text'])
212/182:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]

print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        img = cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/183:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]

print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/184:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]

print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        rotate = cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/185:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]

print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        rotate = cv2.rectangle(rotate, (x, y), (x + w-10, y + h-10), (0, 255, 0), 2)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/186:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]

print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        rotate = cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('Result', cv2.cvtColor(rotate, cv2.COLOR_GRAY2BGR))
cv2.waitKey(0)
212/187:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]

print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        rotate = cv2.rectangle(cv2.cvtColor(rotate, cv2.COLOR_GRAY2BGR), (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
212/188:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))
blur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR)


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]


print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        rotate = cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
213/1:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))
blur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR)


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]


print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        rotate = cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
213/2:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))
blur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR)


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]


print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
213/3:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))
blur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR)


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]


print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.putText(rotate, d['text'][i], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 
                   1, (0, 255, 0), 2, cv2.LINE_AA)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
213/4:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))
blur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR)


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]


print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.putText(rotate, d['text'][i], (x, y+10), cv2.FONT_HERSHEY_SIMPLEX, 
                   1, (0, 255, 0), 2, cv2.LINE_AA)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
213/5:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))
blur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR)


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]


print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.putText(rotate, d['text'][i], (x, y+20), cv2.FONT_HERSHEY_SIMPLEX, 
                   1, (0, 255, 0), 2, cv2.LINE_AA)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
213/6:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))
blur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR)


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]


print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.putText(rotate, d['text'][i], (x, y+20), 4, 
                   1, (0, 255, 0), 2, cv2.LINE_AA)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
213/7:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))
blur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR)


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]


print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 5)
        cv2.putText(rotate, d['text'][i], (x, y+20), 4, 
                   1, (0, 255, 0), 2, cv2.LINE_AA)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
213/8:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))
blur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR)


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]


print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 0)
        cv2.putText(rotate, d['text'][i], (x, y+20), 4, 
                   1, (0, 255, 0), 2, cv2.LINE_AA)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
213/9:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))
blur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR)


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]


print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 1)
        cv2.putText(rotate, d['text'][i], (x, y+20), 4, 
                   1, (0, 255, 0), 2, cv2.LINE_AA)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
213/10:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))
blur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR)


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]


print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 1)
        cv2.putText(rotate, d['text'][i], (x, y+20), 4, 
                   1, (0, 255, 0), 1, cv2.LINE_AA)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
213/11:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))
blur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR)


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]


print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 1)
        cv2.putText(rotate, d['text'][i], (x, y+20), cv2.FONT_HERSHEY_PLAIN, 
                   1, (0, 255, 0), 1, cv2.LINE_AA)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
213/12:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))
blur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR)


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]


print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.putText(rotate, d['text'][i], (x, y+10), cv2.FONT_HERSHEY_PLAIN, 
                   1, (0, 255, 0), 1, cv2.LINE_AA)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
213/13:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))
blur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR)


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]


print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.putText(rotate, d['text'][i], (x, y+13), cv2.FONT_HERSHEY_PLAIN, 
                   1, (0, 255, 0), 1, cv2.LINE_AA)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
213/14:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))
blur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR)


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]


print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.putText(rotate, d['text'][i], (x+10, y+13), cv2.FONT_HERSHEY_PLAIN, 
                   1, (0, 255, 0), 1, cv2.LINE_AA)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
213/15:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))
blur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR)


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]


print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.putText(rotate, d['text'][i], (x+5, y+13), cv2.FONT_HERSHEY_PLAIN, 
                   1, (0, 255, 0), 1, cv2.LINE_AA)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
213/16:
import cv2
import pytesseract
from pytesseract import Output

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

img = cv2.imread('image616.jpg',0)
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
ret,thresh = cv2.threshold(img,175,255,cv2.THRESH_BINARY)
medianBlurred = cv2.medianBlur(thresh, 1)
blur = cv2.blur(medianBlurred, (2,2))
blur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR)


h, w = blur.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 343, 1) # (Center, angle of rotation, scale of zooming)

rotate = cv2.warpAffine(blur, M, (w,h))
# x,y,w,h = 150,40,450,30
rotate = rotate[40:40+30,150:150+450]


print(pytesseract.image_to_string(rotate))
d = pytesseract.image_to_data(rotate, output_type=Output.DICT)

n_boxes = len(d['text'])
for i in range(n_boxes):
    if int(d['conf'][i]) > 0:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        cv2.rectangle(rotate, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.putText(rotate, d['text'][i], (x, y+13), cv2.FONT_HERSHEY_PLAIN, 
                   1, (0, 255, 0), 1, cv2.LINE_AA)

cv2.imshow('Result', rotate)
cv2.waitKey(0)
213/17:
import time
start_time = time.time()
time.sleep(10)
print("--- %m minutes ---" % (time.time() - start_time))
213/18:
import time
start_time = time.time()
time.sleep(10)
print("--- %M minutes ---" % (time.time() - start_time))
213/19:
import time
start_time = time.time()
time.sleep(10)
print("--- %Mm minutes ---" % (time.time() - start_time))
213/20:
import time
start_time = time.time()
time.sleep(10)
print("--- %s minutes ---" % (time.time() - start_time))
213/21:
import time
start_time = time.time()
time.sleep(10)
print("--- %s minutes ---" % (time.time() - start_time)*60)
213/22:
# import time
# start_time = time.time()
# time.sleep(10)
# print("--- %s minutes ---" % (time.time() - start_time))
print(start_time)
213/23:
# import time
# start_time = time.time()
# time.sleep(10)
# print("--- %s minutes ---" % (time.time() - start_time))
print(time.time)
213/24:
# import time
# start_time = time.time()
# time.sleep(10)
# print("--- %s minutes ---" % (time.time() - start_time))
print(time.time())
214/1:
asdf = "sadf asdfdsa asdfsf"
asdf[:]
214/2:
asdf = "sadf asdfdsa asdfsf"
asdf[:50]
214/3:
asdf = "sadf asdfdsa asdfsf"
asdf[:100]
215/1:
import os
import numpy as np
# for i in os.listdir():
#     if '.' not in i:
#         print(os.path.join(os.getcwd(),i))
deck = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0, 10:0, 11:0, 12:0, 13:0, 14:0, 15:0, 16:0, 17:0, 18:0, 19:0, 20:0, 21:0, 22:0, 23:0, 24:0, 25:0, 26:0, 27:0, 28:0, 29:0, 30:0, 31:0, 32:0, 33:0, 34:0, 35:0, 36:0, 37:0, 38:0, 39:0, 40:0, 41:0, 42:0, 43:0, 44:0, 45:0, 46:0, 47:0, 48:0, 49:0, 50:0, 51:0}
train = []
# train = os.listdir('C:/Users/Dell/Downloads/archive (1)/train/labels')

for i in os.listdir('C:/Users/Dell/Downloads/archive (1)/train/labels'):
    with open(os.path.join('C:/Users/Dell/Downloads/archive (1)/train/labels', i)) as f:
        # if f.read().split(' ')[0] == deck.values
        # train.append(f.read().split(' ')[0])
        train.append(f.read())
        # print(f.read())
# for x in train:
#     if len(x) not in range(316,317):
#         for y in x:
#             train.append(y)
        # if type(x) == list:
        #     train.remove(x)
# for x in train:
#     # if type(x) == list:
#     for y in x:
#         train.append(y)
        
# print(train)
# print(len(train[-1]))
train
215/2:
if '\n' in train:
    print(True)
215/3:
for x in train:
    if '\n' in x:
        print(x)
215/4:
for x in train:
    if '\n' in x:
        train.append(x)
    train.remove(x)
215/5: len(train)
215/6:
if type(x) in list:
    print(True)
215/7:
import os
import numpy as np
# for i in os.listdir():
#     if '.' not in i:
#         print(os.path.join(os.getcwd(),i))
deck = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0, 10:0, 11:0, 12:0, 13:0, 14:0, 15:0, 16:0, 17:0, 18:0, 19:0, 20:0, 21:0, 22:0, 23:0, 24:0, 25:0, 26:0, 27:0, 28:0, 29:0, 30:0, 31:0, 32:0, 33:0, 34:0, 35:0, 36:0, 37:0, 38:0, 39:0, 40:0, 41:0, 42:0, 43:0, 44:0, 45:0, 46:0, 47:0, 48:0, 49:0, 50:0, 51:0}
train = []
# train = os.listdir('C:/Users/Dell/Downloads/archive (1)/train/labels')

for i in os.listdir('C:/Users/Dell/Downloads/archive (1)/train/labels'):
    with open(os.path.join('C:/Users/Dell/Downloads/archive (1)/train/labels', i)) as f:
        # if f.read().split(' ')[0] == deck.values
        # train.append(f.read().split(' ')[0])
        train.append(f.read())
        # print(f.read())
# for x in train:
#     if len(x) not in range(316,317):
#         for y in x:
#             train.append(y)
        # if type(x) == list:
        #     train.remove(x)
# for x in train:
#     # if type(x) == list:
#     for y in x:
#         train.append(y)
        
# print(train)
# print(len(train[-1]))
train
215/8:
import os
import numpy as np
# for i in os.listdir():
#     if '.' not in i:
#         print(os.path.join(os.getcwd(),i))
deck = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0, 10:0, 11:0, 12:0, 13:0, 14:0, 15:0, 16:0, 17:0, 18:0, 19:0, 20:0, 21:0, 22:0, 23:0, 24:0, 25:0, 26:0, 27:0, 28:0, 29:0, 30:0, 31:0, 32:0, 33:0, 34:0, 35:0, 36:0, 37:0, 38:0, 39:0, 40:0, 41:0, 42:0, 43:0, 44:0, 45:0, 46:0, 47:0, 48:0, 49:0, 50:0, 51:0}
train = []
# train = os.listdir('C:/Users/Dell/Downloads/archive (1)/train/labels')

for i in os.listdir('C:/Users/Dell/Downloads/archive (1)/train/labels'):
    with open(os.path.join('C:/Users/Dell/Downloads/archive (1)/train/labels', i)) as f:
        # if f.read().split(' ')[0] == deck.values
        # train.append(f.read().split(' ')[0])
        train.append(f.read())
        # print(f.read())
# for x in train:
#     if len(x) not in range(316,317):
#         for y in x:
#             train.append(y)
        # if type(x) == list:
        #     train.remove(x)
# for x in train:
#     # if type(x) == list:
#     for y in x:
#         train.append(y)
        
# print(train)
# print(len(train[-1]))
train
215/9:
for x in train:
    if type(x) in list:
        print(True)
215/10:
for x in train:
    if type(x) in list:
        print(True)
215/11:
for x in train:
    if '\n' in x:
        print(True)
215/12:
import os
import numpy as np
# for i in os.listdir():
#     if '.' not in i:
#         print(os.path.join(os.getcwd(),i))
deck = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0, 10:0, 11:0, 12:0, 13:0, 14:0, 15:0, 16:0, 17:0, 18:0, 19:0, 20:0, 21:0, 22:0, 23:0, 24:0, 25:0, 26:0, 27:0, 28:0, 29:0, 30:0, 31:0, 32:0, 33:0, 34:0, 35:0, 36:0, 37:0, 38:0, 39:0, 40:0, 41:0, 42:0, 43:0, 44:0, 45:0, 46:0, 47:0, 48:0, 49:0, 50:0, 51:0}
train = []
# train = os.listdir('C:/Users/Dell/Downloads/archive (1)/train/labels')

for i in os.listdir('C:/Users/Dell/Downloads/archive (1)/train/labels'):
    with open(os.path.join('C:/Users/Dell/Downloads/archive (1)/train/labels', i)) as f:
        # if f.read().split(' ')[0] == deck.values
        # train.append(f.read().split(' ')[0])
        train.append(f.read())
        # print(f.read())
# for x in train:
#     if len(x) not in range(316,317):
#         for y in x:
#             train.append(y)
        # if type(x) == list:
        #     train.remove(x)
# for x in train:
#     # if type(x) == list:
#     for y in x:
#         train.append(y)
        
# print(train)
# print(len(train[-1]))
train
215/13:
import os
import numpy as np
# for i in os.listdir():
#     if '.' not in i:
#         print(os.path.join(os.getcwd(),i))
deck = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0, 10:0, 11:0, 12:0, 13:0, 14:0, 15:0, 16:0, 17:0, 18:0, 19:0, 20:0, 21:0, 22:0, 23:0, 24:0, 25:0, 26:0, 27:0, 28:0, 29:0, 30:0, 31:0, 32:0, 33:0, 34:0, 35:0, 36:0, 37:0, 38:0, 39:0, 40:0, 41:0, 42:0, 43:0, 44:0, 45:0, 46:0, 47:0, 48:0, 49:0, 50:0, 51:0}
train = []
# train = os.listdir('C:/Users/Dell/Downloads/archive (1)/train/labels')

for i in os.listdir('C:/Users/Dell/Downloads/archive (1)/train/labels'):
    with open(os.path.join('C:/Users/Dell/Downloads/archive (1)/train/labels', i)) as f:
        # if f.read().split(' ')[0] == deck.values
        # train.append(f.read().split(' ')[0])
        train.append(f.read())
        # print(f.read())
# for x in train:
#     if len(x) not in range(316,317):
#         for y in x:
#             train.append(y)
        # if type(x) == list:
        #     train.remove(x)
# for x in train:
#     # if type(x) == list:
#     for y in x:
#         train.append(y)
        
# print(train)
# print(len(train[-1]))
print(train)
215/14:
import os
import numpy as np
# for i in os.listdir():
#     if '.' not in i:
#         print(os.path.join(os.getcwd(),i))
deck = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0, 10:0, 11:0, 12:0, 13:0, 14:0, 15:0, 16:0, 17:0, 18:0, 19:0, 20:0, 21:0, 22:0, 23:0, 24:0, 25:0, 26:0, 27:0, 28:0, 29:0, 30:0, 31:0, 32:0, 33:0, 34:0, 35:0, 36:0, 37:0, 38:0, 39:0, 40:0, 41:0, 42:0, 43:0, 44:0, 45:0, 46:0, 47:0, 48:0, 49:0, 50:0, 51:0}
train = []
# train = os.listdir('C:/Users/Dell/Downloads/archive (1)/train/labels')

for i in os.listdir('C:/Users/Dell/Downloads/archive (1)/train/labels'):
    with open(os.path.join('C:/Users/Dell/Downloads/archive (1)/train/labels', i)) as f:
        # if f.read().split(' ')[0] == deck.values
        # train.append(f.read().split(' ')[0])
        train.append(f.read())
        # print(f.read())
# for x in train:
#     if len(x) not in range(316,317):
#         for y in x:
#             train.append(y)
        # if type(x) == list:
        #     train.remove(x)
# for x in train:
#     # if type(x) == list:
#     for y in x:
#         train.append(y)
        
# print(train)
# print(len(train[-1]))
train
215/15:
aaaa=0
for x in train:
    if '\n' in x:
        aaaa+=1
print(aaaa)
215/16: print(train)
215/17:
for z in train:
    if '\n' in z:
        print(z)
215/18:
for z in train:
    if '\n' in z:
        return z
215/19:
for z in train:
    if '\n' in z:
        z
215/20:
abcd = []
for z in train:
    if '\n' in z:
        abcd.append(z.split('\n'))
len(abcd)
215/21:
abcd = []
for z in train:
    if '\n' in z:
        print(z.split('\n'))
215/22:
list1 = []
for z in abcd:
    list1.append(z)

len(list1)
215/23:
list1 = []
for z in abcd:
    print(z)

# len(list1)
215/24:
abcd = []
for z in train:
    if '\n' in z:
        for k in z.split('\n'):
            abcd.append(k)
215/25:
print(len(abcd))

# len(list1)
215/26:
print(abcd)

# len(list1)
215/27:
abcd

# len(list1)
215/28:
lab = []
for v in abcd.split(' ')[0]:
    lab.append(v)

print(len(v))
v
215/29:
lab = []
for z in abcd:
    for v in z.split(' ')[0]:
        lab.append(v)

print(len(v))
v
215/30: len(abcd)
215/31:
for r in abcd:
    print(r.split(' ')[0])
215/32:
for r in abcd:
    lab.append(r.split(' ')[0])
215/33: len(lab)
215/34: lab
215/35: abcd
215/36: lab
215/37:
for r in abcd:
    lab.append(r.split(' '))
215/38: lab
215/39:
for r in abcd:
    print(r.split(' ')[0])
215/40: lab
215/41:
for r in abcd:
    print(r.split(' ')[0])
215/42: lab.clear()
215/43:
for r in abcd:
    lab.append(r.split(' ')[0])
215/44:
len(lab)
lab
215/45: len(lab)
215/46:
import numpy as np
np.unique(np.array(lab))
215/47: np.unique(np.array(lab), return_counts=True)
215/48:
import os
import numpy as np

test = []

for i in os.listdir('C:/Users/Dell/Downloads/archive (1)/test/labels'):
    with open(os.path.join('C:/Users/Dell/Downloads/archive (1)/test/labels', i)) as f:
        
        test.append(f.read())
test
215/49:
efgh = []
for z in test:
    if '\n' in z:
        for k in z.split('\n'):
            efgh.append(k)
215/50:
lab2 = []
for r in efgh:
    lab2.append(r.split(' ')[0])
215/51:
print(len(lab2))
lab2
215/52: np.unique(np.array(lab2), return_counts=True)
215/53:
train_data = [986,  999, 1098, 1066,  985,  997, 1117, 1005,  942, 1001, 1039,
        1004, 1005,  997, 1039,  993, 1011,  994, 1085,  978,  978, 1012,
        1171, 1032,  989,  998,  950,  980, 1049, 1006, 1062, 1015, 1028,
        1042, 1062, 1022, 1075,  971, 1013, 1014, 1014, 1034, 1037,  938,
        1050, 1071, 1046,  995, 1046,  955, 1034,  973]
train_data
215/54:
train_data = [986,  999, 1098, 1066,  985,  997, 1117, 1005,  942, 1001, 1039,
        1004, 1005,  997, 1039,  993, 1011,  994, 1085,  978,  978, 1012,
        1171, 1032,  989,  998,  950,  980, 1049, 1006, 1062, 1015, 1028,
        1042, 1062, 1022, 1075,  971, 1013, 1014, 1014, 1034, 1037,  938,
        1050, 1071, 1046,  995, 1046,  955, 1034,  973]
test_data = [138, 171, 170, 165, 143, 140, 129, 136, 153, 174, 170, 169,
        150, 138, 168, 149, 156, 162, 155, 168, 136, 141, 131, 152, 156,
        124, 140, 129, 108, 152, 136, 147, 159, 155, 140, 146, 131, 155,
        147, 148, 115, 167, 145, 121, 128, 159, 138, 119, 157, 154, 156,
        144]

train_data+test_data
215/55:
train_data = np.array([986,  999, 1098, 1066,  985,  997, 1117, 1005,  942, 1001, 1039,
        1004, 1005,  997, 1039,  993, 1011,  994, 1085,  978,  978, 1012,
        1171, 1032,  989,  998,  950,  980, 1049, 1006, 1062, 1015, 1028,
        1042, 1062, 1022, 1075,  971, 1013, 1014, 1014, 1034, 1037,  938,
        1050, 1071, 1046,  995, 1046,  955, 1034,  973])
test_data = np.array([138, 171, 170, 165, 143, 140, 129, 136, 153, 174, 170, 169,
        150, 138, 168, 149, 156, 162, 155, 168, 136, 141, 131, 152, 156,
        124, 140, 129, 108, 152, 136, 147, 159, 155, 140, 146, 131, 155,
        147, 148, 115, 167, 145, 121, 128, 159, 138, 119, 157, 154, 156,
        144])

np.add(train_data,test_data)
215/56: deck
215/57:
train_data = np.array([986,  999, 1098, 1066,  985,  997, 1117, 1005,  942, 1001, 1039,
        1004, 1005,  997, 1039,  993, 1011,  994, 1085,  978,  978, 1012,
        1171, 1032,  989,  998,  950,  980, 1049, 1006, 1062, 1015, 1028,
        1042, 1062, 1022, 1075,  971, 1013, 1014, 1014, 1034, 1037,  938,
        1050, 1071, 1046,  995, 1046,  955, 1034,  973])
test_data = np.array([138, 171, 170, 165, 143, 140, 129, 136, 153, 174, 170, 169,
        150, 138, 168, 149, 156, 162, 155, 168, 136, 141, 131, 152, 156,
        124, 140, 129, 108, 152, 136, 147, 159, 155, 140, 146, 131, 155,
        147, 148, 115, 167, 145, 121, 128, 159, 138, 119, 157, 154, 156,
        144])

length = list(np.add(train_data,test_data))
215/58:
d = 0
for i in range(len(deck)):
    print(i)
#     deck[i] = length[i]
215/59:
d = 0
for i in range(len(deck)):
    deck[i] = length[i]
deck
215/60: length
215/61:
labels = ['10c', '10d', '10h', '10s', '2c', '2d', '2h', '2s', '3c', '3d', '3h', '3s', '4c', '4d', '4h', '4s', '5c', '5d', '5h', '5s', '6c', '6d', '6h', '6s', '7c', '7d', '7h', '7s', '8c', '8d', '8h', '8s', '9c', '9d', '9h', '9s', 'Ac', 'Ad', 'Ah', 'As', 'Jc', 'Jd', 'Jh', 'Js', 'Kc', 'Kd', 'Kh', 'Ks', 'Qc', 'Qd', 'Qh', 'Qs']

dictionary = {}

for l in labels:
    for k in length:
        dictionary[l] = k
dictionary
215/62:
labels = ['10c', '10d', '10h', '10s', '2c', '2d', '2h', '2s', '3c', '3d', '3h', '3s', '4c', '4d', '4h', '4s', '5c', '5d', '5h', '5s', '6c', '6d', '6h', '6s', '7c', '7d', '7h', '7s', '8c', '8d', '8h', '8s', '9c', '9d', '9h', '9s', 'Ac', 'Ad', 'Ah', 'As', 'Jc', 'Jd', 'Jh', 'Js', 'Kc', 'Kd', 'Kh', 'Ks', 'Qc', 'Qd', 'Qh', 'Qs']

dictionary = {}

for key in labels:
    for value in length:
        dictionary[key] = value
        length.remove(value)
        break

dictionary
215/63:
import os
import numpy as np

val = []

for i in os.listdir('C:/Users/Dell/Downloads/archive (1)/valid/labels'):
    with open(os.path.join('C:/Users/Dell/Downloads/archive (1)/valid/labels', i)) as f:
        
        val.append(f.read())
val
215/64:
iklm = []
for z in val:
    if '\n' in z:
        for k in z.split('\n'):
            iklm.append(k)
215/65:
lab3 = []
for r in iklm:
    lab3.append(r.split(' ')[0])
215/66:
print(len(lab3))
lab3
215/67: np.unique(np.array(lab3), return_counts=True)
215/68:
train_data = np.array([986,  999, 1098, 1066,  985,  997, 1117, 1005,  942, 1001, 1039,
        1004, 1005,  997, 1039,  993, 1011,  994, 1085,  978,  978, 1012,
        1171, 1032,  989,  998,  950,  980, 1049, 1006, 1062, 1015, 1028,
        1042, 1062, 1022, 1075,  971, 1013, 1014, 1014, 1034, 1037,  938,
        1050, 1071, 1046,  995, 1046,  955, 1034,  973])

valid_data = np.array([282, 290, 313, 358, 276, 282, 312, 295, 290, 283, 253, 281, 322,
        284, 286, 329, 289, 304, 311, 303, 304, 285, 261, 250, 255, 339,
        309, 275, 272, 263, 288, 294, 283, 322, 295, 265, 285, 278, 282,
        307, 304, 297, 264, 300, 317, 259, 295, 317, 294, 299, 308, 250])

test_data = np.array([138, 171, 170, 165, 143, 140, 129, 136, 153, 174, 170, 169,
        150, 138, 168, 149, 156, 162, 155, 168, 136, 141, 131, 152, 156,
        124, 140, 129, 108, 152, 136, 147, 159, 155, 140, 146, 131, 155,
        147, 148, 115, 167, 145, 121, 128, 159, 138, 119, 157, 154, 156,
        144])

length = list(np.add(train_data,valid_data))
215/69: dictionary.clear()
215/70:
labels = ['10c', '10d', '10h', '10s', '2c', '2d', '2h', '2s', '3c', '3d', '3h', '3s', '4c', '4d', '4h', '4s', '5c', '5d', '5h', '5s', '6c', '6d', '6h', '6s', '7c', '7d', '7h', '7s', '8c', '8d', '8h', '8s', '9c', '9d', '9h', '9s', 'Ac', 'Ad', 'Ah', 'As', 'Jc', 'Jd', 'Jh', 'Js', 'Kc', 'Kd', 'Kh', 'Ks', 'Qc', 'Qd', 'Qh', 'Qs']

dictionary = {}

for key in labels:
    for value in length:
        dictionary[key] = value
        length.remove(value)
        break

dictionary
215/71: length
215/72:
train_data = np.array([986,  999, 1098, 1066,  985,  997, 1117, 1005,  942, 1001, 1039,
        1004, 1005,  997, 1039,  993, 1011,  994, 1085,  978,  978, 1012,
        1171, 1032,  989,  998,  950,  980, 1049, 1006, 1062, 1015, 1028,
        1042, 1062, 1022, 1075,  971, 1013, 1014, 1014, 1034, 1037,  938,
        1050, 1071, 1046,  995, 1046,  955, 1034,  973])

valid_data = np.array([282, 290, 313, 358, 276, 282, 312, 295, 290, 283, 253, 281, 322,
        284, 286, 329, 289, 304, 311, 303, 304, 285, 261, 250, 255, 339,
        309, 275, 272, 263, 288, 294, 283, 322, 295, 265, 285, 278, 282,
        307, 304, 297, 264, 300, 317, 259, 295, 317, 294, 299, 308, 250])

test_data = np.array([138, 171, 170, 165, 143, 140, 129, 136, 153, 174, 170, 169,
        150, 138, 168, 149, 156, 162, 155, 168, 136, 141, 131, 152, 156,
        124, 140, 129, 108, 152, 136, 147, 159, 155, 140, 146, 131, 155,
        147, 148, 115, 167, 145, 121, 128, 159, 138, 119, 157, 154, 156,
        144])

length = list(np.add(train_data,valid_data))
215/73: length
215/74:
train_data = np.array([986,  999, 1098, 1066,  985,  997, 1117, 1005,  942, 1001, 1039,
        1004, 1005,  997, 1039,  993, 1011,  994, 1085,  978,  978, 1012,
        1171, 1032,  989,  998,  950,  980, 1049, 1006, 1062, 1015, 1028,
        1042, 1062, 1022, 1075,  971, 1013, 1014, 1014, 1034, 1037,  938,
        1050, 1071, 1046,  995, 1046,  955, 1034,  973])

valid_data = np.array([282, 290, 313, 358, 276, 282, 312, 295, 290, 283, 253, 281, 322,
        284, 286, 329, 289, 304, 311, 303, 304, 285, 261, 250, 255, 339,
        309, 275, 272, 263, 288, 294, 283, 322, 295, 265, 285, 278, 282,
        307, 304, 297, 264, 300, 317, 259, 295, 317, 294, 299, 308, 250])

test_data = np.array([138, 171, 170, 165, 143, 140, 129, 136, 153, 174, 170, 169,
        150, 138, 168, 149, 156, 162, 155, 168, 136, 141, 131, 152, 156,
        124, 140, 129, 108, 152, 136, 147, 159, 155, 140, 146, 131, 155,
        147, 148, 115, 167, 145, 121, 128, 159, 138, 119, 157, 154, 156,
        144])

length = list(np.add(train_data,test_data))
215/75: length
215/76: dictionary.clear()
215/77:
labels = ['10c', '10d', '10h', '10s', '2c', '2d', '2h', '2s', '3c', '3d', '3h', '3s', '4c', '4d', '4h', '4s', '5c', '5d', '5h', '5s', '6c', '6d', '6h', '6s', '7c', '7d', '7h', '7s', '8c', '8d', '8h', '8s', '9c', '9d', '9h', '9s', 'Ac', 'Ad', 'Ah', 'As', 'Jc', 'Jd', 'Jh', 'Js', 'Kc', 'Kd', 'Kh', 'Ks', 'Qc', 'Qd', 'Qh', 'Qs']

dictionary = {}

for key in labels:
    for value in length:
        dictionary[key] = value
        length.remove(value)
        break

dictionary
215/78:
dictionary2 = {}

for key in labels:
    for value in valid_data:
        dictionary2[key] = value
        break

dictionary2
215/79:
train_data = np.array([986,  999, 1098, 1066,  985,  997, 1117, 1005,  942, 1001, 1039,
        1004, 1005,  997, 1039,  993, 1011,  994, 1085,  978,  978, 1012,
        1171, 1032,  989,  998,  950,  980, 1049, 1006, 1062, 1015, 1028,
        1042, 1062, 1022, 1075,  971, 1013, 1014, 1014, 1034, 1037,  938,
        1050, 1071, 1046,  995, 1046,  955, 1034,  973])

valid_data = np.array([282, 290, 313, 358, 276, 282, 312, 295, 290, 283, 253, 281, 322,
        284, 286, 329, 289, 304, 311, 303, 304, 285, 261, 250, 255, 339,
        309, 275, 272, 263, 288, 294, 283, 322, 295, 265, 285, 278, 282,
        307, 304, 297, 264, 300, 317, 259, 295, 317, 294, 299, 308, 250])

test_data = np.array([138, 171, 170, 165, 143, 140, 129, 136, 153, 174, 170, 169,
        150, 138, 168, 149, 156, 162, 155, 168, 136, 141, 131, 152, 156,
        124, 140, 129, 108, 152, 136, 147, 159, 155, 140, 146, 131, 155,
        147, 148, 115, 167, 145, 121, 128, 159, 138, 119, 157, 154, 156,
        144])

length = list(np.add(train_data,test_data))
215/80:
dictionary2 = {}

for key in labels:
    for value in valid_data:
        dictionary2[key] = value
        valid_data.remove(value)
        break

dictionary2
215/81:
train_data = np.array([986,  999, 1098, 1066,  985,  997, 1117, 1005,  942, 1001, 1039,
        1004, 1005,  997, 1039,  993, 1011,  994, 1085,  978,  978, 1012,
        1171, 1032,  989,  998,  950,  980, 1049, 1006, 1062, 1015, 1028,
        1042, 1062, 1022, 1075,  971, 1013, 1014, 1014, 1034, 1037,  938,
        1050, 1071, 1046,  995, 1046,  955, 1034,  973])

valid_data = np.array([282, 290, 313, 358, 276, 282, 312, 295, 290, 283, 253, 281, 322,
        284, 286, 329, 289, 304, 311, 303, 304, 285, 261, 250, 255, 339,
        309, 275, 272, 263, 288, 294, 283, 322, 295, 265, 285, 278, 282,
        307, 304, 297, 264, 300, 317, 259, 295, 317, 294, 299, 308, 250])

test_data = np.array([138, 171, 170, 165, 143, 140, 129, 136, 153, 174, 170, 169,
        150, 138, 168, 149, 156, 162, 155, 168, 136, 141, 131, 152, 156,
        124, 140, 129, 108, 152, 136, 147, 159, 155, 140, 146, 131, 155,
        147, 148, 115, 167, 145, 121, 128, 159, 138, 119, 157, 154, 156,
        144])

length = list(np.add(train_data,test_data))
test_length = list(valid_data)
215/82: dictionary2.clear()
215/83:
dictionary2 = {}

for key in labels:
    for value in test_length:
        dictionary2[key] = value
        test_length.remove(value)
        break

dictionary2
216/1:
import pandas as pd

train = {'10c': 1124,
 '10d': 1170,
 '10h': 1268,
 '10s': 1231,
 '2c': 1128,
 '2d': 1137,
 '2h': 1246,
 '2s': 1141,
 '3c': 1095,
 '3d': 1175,
 '3h': 1209,
 '3s': 1173,
 '4c': 1155,
 '4d': 1135,
 '4h': 1207,
 '4s': 1142,
 '5c': 1167,
 '5d': 1156,
 '5h': 1240,
 '5s': 1146,
 '6c': 1114,
 '6d': 1153,
 '6h': 1302,
 '6s': 1184,
 '7c': 1145,
 '7d': 1122,
 '7h': 1090,
 '7s': 1109,
 '8c': 1157,
 '8d': 1158,
 '8h': 1198,
 '8s': 1162,
 '9c': 1187,
 '9d': 1197,
 '9h': 1202,
 '9s': 1168,
 'Ac': 1206,
 'Ad': 1126,
 'Ah': 1160,
 'As': 1162,
 'Jc': 1129,
 'Jd': 1201,
 'Jh': 1182,
 'Js': 1059,
 'Kc': 1178,
 'Kd': 1230,
 'Kh': 1184,
 'Ks': 1114,
 'Qc': 1203,
 'Qd': 1109,
 'Qh': 1190,
 'Qs': 1117}
data = pd.DataFrame(sales_dict)
216/2:
import pandas as pd

train = {'10c': 1124,
 '10d': 1170,
 '10h': 1268,
 '10s': 1231,
 '2c': 1128,
 '2d': 1137,
 '2h': 1246,
 '2s': 1141,
 '3c': 1095,
 '3d': 1175,
 '3h': 1209,
 '3s': 1173,
 '4c': 1155,
 '4d': 1135,
 '4h': 1207,
 '4s': 1142,
 '5c': 1167,
 '5d': 1156,
 '5h': 1240,
 '5s': 1146,
 '6c': 1114,
 '6d': 1153,
 '6h': 1302,
 '6s': 1184,
 '7c': 1145,
 '7d': 1122,
 '7h': 1090,
 '7s': 1109,
 '8c': 1157,
 '8d': 1158,
 '8h': 1198,
 '8s': 1162,
 '9c': 1187,
 '9d': 1197,
 '9h': 1202,
 '9s': 1168,
 'Ac': 1206,
 'Ad': 1126,
 'Ah': 1160,
 'As': 1162,
 'Jc': 1129,
 'Jd': 1201,
 'Jh': 1182,
 'Js': 1059,
 'Kc': 1178,
 'Kd': 1230,
 'Kh': 1184,
 'Ks': 1114,
 'Qc': 1203,
 'Qd': 1109,
 'Qh': 1190,
 'Qs': 1117}
data = pd.DataFrame(train)
217/1:
import cv2
import time
cpt = 0
maxFrames = 70 # if you want 5 frames only.

cap=cv2.VideoCapture(0)

while cpt < maxFrames:
    ret, frame = cap.read()
    frame=cv2.resize(frame,(640,480))
    cv2.imshow("test window", frame) # show image in window
    cv2.imwrite("/home/pi/images/Car_%d.jpg" %cpt, frame)
    time.sleep(0.5)
    cpt += 1
    if cv2.waitKey(1)&0xFF==27:
        break
cap.release()
cv2.destroyAllWindows()
218/1:
import cv2
import time
cpt = 0
maxFrames = 70 # if you want 5 frames only.

cap=cv2.VideoCapture(0)

while cpt < maxFrames:
    ret, frame = cap.read()
    frame=cv2.resize(frame,(640,480))
    cv2.imshow("test window", frame) # show image in window
    cv2.imwrite("/home/pi/images/Car_%d.jpg" %cpt, frame)
    time.sleep(0.5)
    cpt += 1
    if cv2.waitKey(1)&0xFF==27:
        break
cap.release()
cv2.destroyAllWindows()
219/1:
import cv2
import time
cpt = 0
maxFrames = 70 # if you want 5 frames only.

cap=cv2.VideoCapture(0)

while cpt < maxFrames:
    ret, frame = cap.read()
    frame=cv2.resize(frame,(640,480))
    cv2.imshow("test window", frame) # show image in window
    cv2.imwrite("/home/pi/images/Car_%d.jpg" %cpt, frame)
    time.sleep(0.5)
    cpt += 1
    if cv2.waitKey(1)&0xFF==27:
        break
cap.release()
cv2.destroyAllWindows()
219/2: 614//32
219/3:
import cv2
import time
cpt = 0
maxFrames = 70 # if you want 5 frames only.

cap=cv2.VideoCapture(0)

while cpt < maxFrames:
    ret, frame = cap.read()
    frame=cv2.resize(frame,(614,614))
    cv2.imshow("test window", frame) # show image in window
    cv2.imwrite("/home/pi/images/Car_%d.jpg" %cpt, frame)
    time.sleep(0.5)
    cpt += 1
    if cv2.waitKey(1)&0xFF==27:
        break
cap.release()
cv2.destroyAllWindows()
219/4:
import cv2
import time
cpt = 0
maxFrames = 70 # if you want 5 frames only.

cap=cv2.VideoCapture(0)

while cpt < maxFrames:
    ret, frame = cap.read()
    frame=cv2.resize(frame,(614,614))
    cv2.imshow("test window", frame) # show image in window
    cv2.imwrite("/home/pi/images/Car_%d.jpg" %cpt, frame)
#     time.sleep(0.5)
    cpt += 1
    if cv2.waitKey(1)&0xFF==27:
        break
cap.release()
cv2.destroyAllWindows()
220/1:
import cv2
import time
cpt = 0
maxFrames = 70 # if you want 5 frames only.

cap=cv2.VideoCapture(0)

while cpt < maxFrames:
    ret, frame = cap.read()
    frame=cv2.resize(frame,(614,614))
    cv2.imshow("test window", frame) # show image in window
    cv2.imwrite("/home/pi/images/Car_%d.jpg" %cpt, frame)
#     time.sleep(0.5)
    cpt += 1
    if cv2.waitKey(1)&0xFF==27:
        break
cap.release()
cv2.destroyAllWindows()
220/2:
import cv2
import time
cpt = 0
maxFrames = 70 # if you want 5 frames only.

cap=cv2.VideoCapture(0)

while cpt < maxFrames:
    ret, frame = cap.read()
    frame=cv2.resize(frame,(614,614))
    cv2.imshow("test window", frame) # show image in window
    cv2.imwrite("/home/pi/images/Car_%d.jpg" %cpt, frame)
#     time.sleep(0.5)
    cpt += 1
    if cv2.waitKey(1)&0xFF==27:
        break
cap.release()
cv2.destroyAllWindows()
227/1:
raw_val = {'10c': 282,
 '10d': 290,
 '10h': 313,
 '10s': 358,
 '2c': 276,
 '2d': 282,
 '2h': 312,
 '2s': 295,
 '3c': 290,
 '3d': 283,
 '3h': 253,
 '3s': 281,
 '4c': 322,
 '4d': 284,
 '4h': 286,
 '4s': 329,
 '5c': 289,
 '5d': 304,
 '5h': 311,
 '5s': 303,
 '6c': 304,
 '6d': 285,
 '6h': 261,
 '6s': 250,
 '7c': 255,
 '7d': 339,
 '7h': 309,
 '7s': 275,
 '8c': 272,
 '8d': 263,
 '8h': 288,
 '8s': 294,
 '9c': 283,
 '9d': 322,
 '9h': 295,
 '9s': 265,
 'Ac': 285,
 'Ad': 278,
 'Ah': 282,
 'As': 307,
 'Jc': 304,
 'Jd': 297,
 'Jh': 264,
 'Js': 300,
 'Kc': 317,
 'Kd': 259,
 'Kh': 295,
 'Ks': 317,
 'Qc': 294,
 'Qd': 299,
 'Qh': 308,
 'Qs': 250}
raw_val
227/2:
raw_val = {'10c': 282,
 '10d': 290,
 '10h': 313,
 '10s': 358,
 '2c': 276,
 '2d': 282,
 '2h': 312,
 '2s': 295,
 '3c': 290,
 '3d': 283,
 '3h': 253,
 '3s': 281,
 '4c': 322,
 '4d': 284,
 '4h': 286,
 '4s': 329,
 '5c': 289,
 '5d': 304,
 '5h': 311,
 '5s': 303,
 '6c': 304,
 '6d': 285,
 '6h': 261,
 '6s': 250,
 '7c': 255,
 '7d': 339,
 '7h': 309,
 '7s': 275,
 '8c': 272,
 '8d': 263,
 '8h': 288,
 '8s': 294,
 '9c': 283,
 '9d': 322,
 '9h': 295,
 '9s': 265,
 'Ac': 285,
 'Ad': 278,
 'Ah': 282,
 'As': 307,
 'Jc': 304,
 'Jd': 297,
 'Jh': 264,
 'Js': 300,
 'Kc': 317,
 'Kd': 259,
 'Kh': 295,
 'Ks': 317,
 'Qc': 294,
 'Qd': 299,
 'Qh': 308,
 'Qs': 250}
227/3:
raw_val['Jc'] += 6
raw_val
227/4:
raw_val['Jh'] += 5
raw_val
227/5:
raw_val['4c'] += 2
raw_val
227/6:
raw_val['Qs'] += 8
raw_val
227/7:
raw_val['Qc'] += 7
raw_val
227/8:
raw_val['9h'] += 3
raw_val
227/9:
raw_val['8h'] += 2
raw_val
227/10:
raw_val['9c'] += 3
raw_val
227/11:
raw_val['Qd'] += 5
raw_val
227/12:
raw_val['Ad'] += 2
raw_val
227/13:
raw_val['Qh'] += 3
raw_val
227/14:
raw_val['Jd'] += 5
raw_val
227/15:
raw_val['9d'] += 2
raw_val
227/16:
raw_val['8d'] += 2
raw_val
227/17:
raw_val['7h'] += 1
raw_val
227/18:
raw_val['5c'] += 3
raw_val
227/19:
raw_val['Qd'] += 5
raw_val
227/20:
raw_val['3d'] += 2
raw_val
227/21:
raw_val['6d'] += 3
raw_val
227/22:
raw_val['3s'] += 3
raw_val
227/23:
raw_val['5d'] += 3
raw_val
227/24:
raw_val['2h'] += 2
raw_val
227/25:
raw_val['4s'] += 2
raw_val
227/26:
raw_val['8s'] += 3
raw_val
227/27:
raw_val['6s'] += 1
raw_val
227/28:
raw_val['2d'] += 2
raw_val
227/29:
raw_val['Kc'] += 4
raw_val
227/30:
raw_val['As'] += 3
raw_val
227/31:
raw_val['10h'] += 2
raw_val
227/32:
raw_val['2s'] += 1
raw_val
227/33:
raw_val['Js'] += 3
raw_val
227/34:
raw_val['9s'] += 1
raw_val
227/35:
raw_val['6h'] += 1
raw_val
227/36:
raw_val['Ks'] += 1
raw_val
227/37:
raw_val['2c'] += 1
raw_val
227/38:
raw_val['5s'] += 1
raw_val
227/39:
raw_val['Ac'] += 3
raw_val
227/40:
raw_val['8h'] += 1
raw_val
227/41:
raw_val['10c'] += 2
raw_val
227/42:
raw_val['7c'] += 1
raw_val
227/43:
raw_val['7c'] -= 1
raw_val
227/44:
raw_val['7s'] += 1
raw_val
227/45:
raw_val['7d'] += 1
raw_val
227/46:
raw_val['3h'] += 2
raw_val
227/47:
raw_val['8c'] += 1
raw_val
227/48:
raw_val['5d'] += 1
raw_val
227/49:
# raw_val['5d'] += 1
locked = raw_val
227/50: raw_val
227/51:
raw_val - 310
raw_val
227/52:
raw_val.values() -310
raw_val
227/53:
raw_val.items() -310
raw_val
227/54:
# raw_val.items() -310
raw_val[0]
227/55:
# raw_val.items() -310
for p in raw_val:
    print(p)
227/56:
# raw_val.items() -310
for p in raw_val.items():
    print(p)
227/57:
# raw_val.items() -310
for p in raw_val.values():
    print(p)
227/58:
# raw_val.items() -310
for p in raw_val.items():
    print(p[0])
    print(p[1]-310)
    print('\n')
229/1: from deepface import DeepFacec
229/2: from deepface import DeepFace
229/3: import deepface #import DeepFace
229/4: import deepface.DeepFace #import DeepFace
229/5: import tensorflow
229/6:
import tensorflow as tf

tf.__version__
229/7:
import tensorflow as tf

tf.__version__()
229/8:
import tensorflow as tf
print(tf.__version__)
229/9:
import tensorflow as tf
print(tf.version)
229/10:
import tensorflow.keras
print(keras.version)
229/11:
import tensorflow as tf
print(tf.version.VERSION)
229/12:
import tensorflow as tf
print(tf.__version__)
229/13:
import tensorflow as tf
print(tf.__version__)
230/1:
import tensorflow as tf
print(tf.__version__)
230/2: import deepface.DeepFace #import DeepFace
231/1:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/6.png'
img1_path = 'edhi/5.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'Facenet'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/2:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/6.png'
img2_path = 'edhi/5.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'Facenet'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/3: resp['distance']
231/4: 1 - resp['distance']
231/5: round(1 - resp['distance'],2)*100
231/6:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/1.png'
img2_path = 'edhi/3.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'Facenet'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/7: round(1 - resp['distance'],2)*100
231/8: plt.imshow(img1)
231/9: plt.imshow(img2)
231/10:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/1.png'
img2_path = 'edhi/3.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'Ensemble'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/11:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/1.png'
img2_path = 'edhi/3.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/12:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/6.png'
img2_path = 'edhi/8.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'Facenet'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/13:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/1.png'
img2_path = 'edhi/3.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/14: plt.imshow(img1)
231/15: plt.imshow(img2)
231/16:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/6.png'
img2_path = 'edhi/8.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/17: plt.imshow(img1)
231/18: plt.imshow(img2)
231/19:
df = DeepFace.find(img_path= 'ASattarEdhi.png', db_path= 'edhi/')
df
231/20:
for i in os.listdir('edhi/'):
    print(i)
231/21:
import os
for i in os.listdir('edhi/'):
    print(i)
231/22:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'edhi/6.png'

img1 = DeepFace.detectFace(img1_path)


model_name = 'VGG-Face'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1, img2_path= img2, model_name= model_name)
    print(resp)
    print('\n')
231/23:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'edhi/6.png'

img1 = DeepFace.detectFace(img1_path)


model_name = 'VGG-Face'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1, img2_path= img2, model_name= model_name)
    print(resp)
    print('\n')
231/24:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'edhi/6.png'

img1 = DeepFace.detectFace(img1_path)


model_name = 'VGG-Face'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1, img2_path= img2, model_name= model_name)
    print(resp)
    print('\n')
231/25:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'ASattarEdhi.png'

img1 = DeepFace.detectFace(img1_path)


model_name = 'VGG-Face'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1, img2_path= img2, model_name= model_name)
    print(resp)
    print('\n')
231/26:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/1.png'
img2_path = 'edhi/2.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/27:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/6.png'
img2_path = 'edhi/8.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/28:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'ASattarEdhi.png'

img1 = DeepFace.detectFace(img1_path)


model_name = 'VGG-Face'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1, img2_path= img2, model_name= model_name)
    print(resp)
    print('\n')
231/29:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'ASattarEdhi.png'

img1 = DeepFace.detectFace(img1_path)


model_name = 'VGG-Face'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1, img2_path= img2, model_name= model_name)
    print(resp)
    print('\n')
231/30:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'ASattarEdhi.png'

img1 = DeepFace.detectFace(img1_path)


model_name = 'VGG-Face'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1, img2_path= img2, model_name= model_name)
    print(resp)
    print('\n')
231/31:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'ASattarEdhi.png'

img1 = DeepFace.detectFace(img1_path)


model_name = 'VGG-Face'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1, img2_path= img2, model_name= model_name)
    print(resp)
    print('\n')
231/32:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/1.png'
img2_path = 'edhi/2.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/33:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/1.png'
img2_path = 'edhi/3.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/34:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/2.png'
img2_path = 'edhi/3.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/35:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/1.png'
img2_path = 'edhi/3.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/36:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/1.png'
img2_path = 'edhi/4.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/37:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/2.png'
img2_path = 'edhi/4.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/38:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/1.png'
img2_path = 'edhi/2.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/39:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/1.png'
img2_path = 'edhi/3.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/40:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/2.png'
img2_path = 'edhi/3.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/41:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/1.png'
img2_path = 'edhi/4.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/42:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/3.png'
img2_path = 'edhi/4.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/43:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/3.png'
img2_path = 'edhi/4.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/44:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/3.png'
img2_path = 'edhi/5.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/45:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/3.png'
img2_path = 'edhi/5.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1, img2_path= img2, model_name= model_name)

resp
231/46:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/1.png'
img2_path = 'edhi/3.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1, img2_path= img2, model_name= model_name)

resp
231/47:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/5.png'
img2_path = 'edhi/6.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1, img2_path= img2, model_name= model_name)

resp
231/48: plt.imshow(img2)
231/49: plt.imshow(img1)
231/50: type(img1)
231/51:
from deepface import DeepFace
import matplotlib.pyplot as plt
from PIL import Image

img1_path = 'edhi/5.png'
img2_path = 'edhi/6.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= Image.open(img1), img2_path= Image.open(img2), model_name= model_name)

resp
231/52:
from deepface import DeepFace
import matplotlib.pyplot as plt
from PIL import Image

img1_path = 'edhi/5.png'
img2_path = 'edhi/6.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= Image.fromarray(img1), img2_path= Image.fromarray(img2), model_name= model_name)

resp
231/53:
from deepface import DeepFace
import matplotlib.pyplot as plt
import cv2

img1_path = 'edhi/5.png'
img2_path = 'edhi/6.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= cv2.imread(img1), img2_path= cv2.imread(img2), model_name= model_name)

resp
231/54:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/6.png'
img2_path = 'edhi/7.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/55:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/7.png'
img2_path = 'edhi/8.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/56:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/8.png'
img2_path = 'edhi/9.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/57:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/6.png'
img2_path = 'old_man.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/58: plt.imshow(img1)
231/59: plt.imshow(img2)
231/60:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/6.png'
img2_path = 'edhi/8.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/61: plt.imshow(img2)
231/62:
from deepface import DeepFace
import matplotlib.pyplot as plt

img1_path = 'edhi/6.png'
img2_path = 'old_man.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)

model_name = 'VGG-Face'

resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)

resp
231/63: plt.imshow(img2)
231/64:
df = DeepFace.find(img_path= 'ASattarEdhi.png', db_path= 'edhi/')
df
231/65:
df = DeepFace.find(img_path= 'oldman.png', db_path= 'edhi/')
df
231/66:
df = DeepFace.find(img_path= 'old_man.png', db_path= 'edhi/')
df
232/1: !tar -xf lfw.tgz
232/2: !tar -xf lfw.tgz
232/3: !unzip -r FaceRecognition-main.zip
233/1:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'ASattarEdhi.png'

img1 = DeepFace.detectFace(img1_path)


model_name = 'VGG-Face'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1, img2_path= img2, model_name= model_name)
    print(resp)
    print('\n')
233/2:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'ASattarEdhi.png'
img2_path = 'old_man.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)
# img1_path = 'edhi/6.png'
# img2_path = 'old_man.png'


model_name = 'VGG-Face'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1, img2_path= img2, model_name= model_name)
    print(resp)
    print('\n')
233/3:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'edhi/6.png'
img2_path = 'old_man.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)



model_name = 'VGG-Face'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1, img2_path= img2, model_name= model_name)
    print(resp)
    print('\n')
233/4:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'edhi/6.png'
img2_path = 'edhi/8.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)



model_name = 'VGG-Face'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1, img2_path= img2, model_name= model_name)
    print(resp)
    print('\n')
233/5:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'edhi/6.png'
img2_path = 'edhi/8.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'VGG-Face'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
233/6:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'edhi/6.png'
img2_path = 'edhi/8.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensambled'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
233/7:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'edhi/6.png'
img2_path = 'edhi/8.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensamble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
233/8:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'edhi/6.png'
img2_path = 'edhi/8.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensemble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
234/1:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'edhi/6.png'
img2_path = 'edhi/8.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensemble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
234/2:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'edhi/6.png'
img2_path = 'edhi/8.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensemble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
234/3:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'ASattarEdhi.png'
img2_path = 'old_man.png'

# img1_path = 'edhi/6.png'
# img2_path = 'edhi/8.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensemble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
234/4:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

# img1_path = 'edhi/6.png'
# img2_path = 'edhi/8.png'

img1 = DeepFace.detectFace(img1_path)
img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensemble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
234/5:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'edhi/6.png'
img2_path = 'edhi/8.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensemble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
234/6:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'edhi/6.png'
img2_path = 'edhi/1.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensemble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
234/7:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'edhi/5.png'
img2_path = 'edhi/1.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensemble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
234/8:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'edhi/8.png'
img2_path = 'old_man2.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensemble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
234/9:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'edhi/9.png'
img2_path = 'old_man2.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensemble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
234/10:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'edhi/8.png'
img2_path = 'old_man2.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensemble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
234/11:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'edhi/8.png'
img2_path = 'edhi/1.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensemble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
234/12:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'edhi/1.png'
img2_path = 'old_man.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensemble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
234/13:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'chinki.png'
img2_path = 'minki.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensemble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
234/14:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'chinki.png'
img2_path = 'minki.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensemble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
234/15:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'chinki.png'
img2_path = 'minki.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensemble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
234/16:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

# img1_path = 'ASattarEdhi.png'
# img2_path = 'old_man.png'

img1_path = 'chinki.png'
img2_path = 'minki.png'

# img1 = DeepFace.detectFace(img1_path)
# img2 = DeepFace.detectFace(img2_path)



model_name = 'Ensemble'

for i in os.listdir('edhi/'):
    img2 = DeepFace.detectFace(img2_path)
    resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
    print(resp)
    print('\n')
234/17:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'chinki.png'
img2_path = 'other.png'

model_name = 'Ensemble'

# for i in os.listdir('edhi/'):
#     img2 = DeepFace.detectFace(img2_path)
resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
print(resp)
234/18:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'chinki.png'
img2_path = 'minki.png'

model_name = 'Ensemble'

# for i in os.listdir('edhi/'):
#     img2 = DeepFace.detectFace(img2_path)
resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
print(resp)
234/19:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'other.png'
img2_path = 'minki.png'

model_name = 'Ensemble'

# for i in os.listdir('edhi/'):
#     img2 = DeepFace.detectFace(img2_path)
resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
print(resp)
234/20:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'chinki.png'
img2_path = 'minki.png'

model_name = 'Ensemble'

# for i in os.listdir('edhi/'):
#     img2 = DeepFace.detectFace(img2_path)
resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
print(resp)
234/21: plt.imshow(img1_path)
234/22: plt.imshow(DeepFace.detectFace(img1_path))
234/23: plt.imshow(DeepFace.detectFace(img2_path))
234/24:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'other.png'
img2_path = 'minki.png'

model_name = 'Ensemble'

# for i in os.listdir('edhi/'):
#     img2 = DeepFace.detectFace(img2_path)
resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
print(resp)
234/25: plt.imshow(DeepFace.detectFace(img2_path))
234/26: plt.imshow(DeepFace.detectFace(img1_path))
235/1:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'ASattarEdhi.png'
img2_path = 'edhi/1.png'

model_name = 'Ensemble'

# for i in os.listdir('edhi/'):
#     img2 = DeepFace.detectFace(img2_path)
resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
print(resp)
235/2:
figure, axis = plt.subplots(2, 1)
axis[0, 0].imshow(DeepFace.detectFace(img1_path))
# axis[0, 0].set_title("Sine Function")
axis[1, 0].imshow(DeepFace.detectFace(img2_path))
axis[1, 0].set_title(resp['verified'])
235/3:
figure, axis = plt.subplots(2, 1)
axis[1, 1].imshow(DeepFace.detectFace(img1_path))
# axis[0, 0].set_title("Sine Function")
axis[1, 2].imshow(DeepFace.detectFace(img2_path))
axis[1, 2].set_title(resp['verified'])
235/4:
figure, axis = plt.subplots(2, 2)
axis[1, 1].imshow(DeepFace.detectFace(img1_path))
# axis[0, 0].set_title("Sine Function")
axis[1, 2].imshow(DeepFace.detectFace(img2_path))
axis[1, 2].set_title(resp['verified'])
235/5:
figure, axis = plt.subplots(1, 2)
axis[1, 1].imshow(DeepFace.detectFace(img1_path))
# axis[0, 0].set_title("Sine Function")
axis[1, 2].imshow(DeepFace.detectFace(img2_path))
axis[1, 2].set_title(resp['verified'])
235/6:
figure, axis = plt.subplots(1, 2)
axis[0, 0].imshow(DeepFace.detectFace(img1_path))
# axis[0, 0].set_title("Sine Function")
axis[0, 1].imshow(DeepFace.detectFace(img2_path))
axis[0, 1].set_title(resp['verified'])
235/7:
plt.imshow(DeepFace.detectFace(img1_path))
plt.imshow(DeepFace.detectFace(img2_path))
235/8:
plt.imshow(DeepFace.detectFace(img1_path))
print(plt.imshow(DeepFace.detectFace(img2_path)))
235/9: plt.imshow(DeepFace.detectFace(img1_path))
235/10: plt.imshow(DeepFace.detectFace(img2_path))
235/11:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'edhi/3.png'
img2_path = 'edhi/1.png'

model_name = 'Ensemble'

# for i in os.listdir('edhi/'):
#     img2 = DeepFace.detectFace(img2_path)
resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
print(resp)
235/12: plt.imshow(DeepFace.detectFace(img1_path))
235/13: plt.imshow(DeepFace.detectFace(img2_path))
235/14:
from deepface import DeepFace
import matplotlib.pyplot as plt
import os

img1_path = 'old_man.png'
img2_path = 'edhi/1.png'

model_name = 'Ensemble'

# for i in os.listdir('edhi/'):
#     img2 = DeepFace.detectFace(img2_path)
resp = DeepFace.verify(img1_path= img1_path, img2_path= img2_path, model_name= model_name)
print(resp)
235/15: plt.imshow(DeepFace.detectFace(img1_path))
236/1:
import openai

openai.api_key = "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"

model_engine = "text-davinci-003"
prompt = "Hi, what is good news today?"

completion = openai.Completion.create(
    engine = model_engine,
    prompt = prompt,
    max_tokens = 1024,
    n =1,
    stop = None,
    temprature = 0.5)

response = completion.choices[0].text
print(response)
236/2:
import openai

openai.api_key = "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"

model_engine = "text-davinci-003"
prompt = "Hi, what is good news today?"

completion = openai.Completion.create(
    engine = model_engine,
    prompt = prompt,
    max_tokens = 1024,
    n =1,
    stop = None,
    temperature = 0.5)

response = completion.choices[0].text
print(response)
236/3:
import openai

openai.api_key = "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"

model_engine = "text-davinci-003"
prompt = "from which direction moon rises?"

completion = openai.Completion.create(
    engine = model_engine,
    prompt = prompt,
    max_tokens = 1024,
    n =1,
    stop = None,
    temperature = 0.5)

response = completion.choices[0].text
print(response)
236/4:
import openai

openai.api_key = "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"

model_engine = "text-davinci-003"
prompt = "Hi chat-gpt, who do you love the most?"

completion = openai.Completion.create(
    engine = model_engine,
    prompt = prompt,
    max_tokens = 1024,
    n =1,
    stop = None,
    temperature = 0.5)

response = completion.choices[0].text
print(response.rstrip())
236/5: print('sgfvsagergerg')
236/6: response
236/7:
import openai

openai.api_key = "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"

model_engine = "text-davinci-003"
prompt = "Can you tell me about my friend Uzaif Ahmed Khatri?"

completion = openai.Completion.create(
    engine = model_engine,
    prompt = prompt,
    max_tokens = 1024,
    n =1,
    stop = None,
    temperature = 0.5)

response = completion.choices[0].text
print(response.rstrip())
236/8:
import openai

openai.api_key = "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"

model_engine = "text-davinci-003"
prompt = "When will google launch Bert"

completion = openai.Completion.create(
    engine = model_engine,
    prompt = prompt,
    max_tokens = 1024,
    n =1,
    stop = None,
    temperature = 0.5)

response = completion.choices[0].text
print(response.rstrip())
236/9:
import openai

openai.api_key = "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"

model_engine = "text-davinci-003"
prompt = "Thank you"

completion = openai.Completion.create(
    engine = model_engine,
    prompt = prompt,
    max_tokens = 1024,
    n =1,
    stop = None,
    temperature = 0.5)

response = completion.choices[0].text
print(response.rstrip())
237/1:
im_rgb = cv2.imread("C:/Users/Dell/Pictures/as.jpg")

h,w = im_rgb.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 45, .5)

rotate = cv2.wrapAffine(im_rgb, M, (w,h))
plt.imshow(rotate)
237/2:
import cv2

im_rgb = cv2.imread("C:/Users/Dell/Pictures/as.jpg")

h,w = im_rgb.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 45, .5)

rotate = cv2.wrapAffine(im_rgb, M, (w,h))
plt.imshow(rotate)
237/3:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [200, 50],
                [50, 200]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [100, 250]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
while(1):
    
    cv2.imshow('image', img)
    if cv2.waitKey(20) & 0xFF == 27:
        break
        
cv2.destroyAllWindows()
237/4:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [50, 50],
                [50, 50]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [100, 250]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
while(1):
    
    cv2.imshow('image', img)
    if cv2.waitKey(20) & 0xFF == 27:
        break
        
cv2.destroyAllWindows()
237/5:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[200, 200],
                [200, 200],
                [200, 200]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [100, 250]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
while(1):
    
    cv2.imshow('image', img)
    if cv2.waitKey(20) & 0xFF == 27:
        break
        
cv2.destroyAllWindows()
237/6:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0, 0],
                [0, 0],
                [0, 0]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [100, 250]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/7:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[100, 100],
                [100, 100],
                [100, 100]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [100, 250]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/8:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[100, 100],
                [100, 100],
                [100, 100]])

pts2 = np.float32([[100, 100],
                [100, 100],
                [100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/9:
import cv2

im_rgb = cv2.imread("C:/Users/Dell/Pictures/as.jpg")

h,w = im_rgb.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 45, .5)

rotate = cv2.wrapAffine(im_rgb, M, (w,h))
plt.imshow(rotate)
237/10: M
237/11: M*100
237/12:
import cv2

im_rgb = cv2.imread("C:/Users/Dell/Pictures/as.jpg")

h,w = im_rgb.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 45, .5)

rotate = cv2.wrapAffine(im_rgb, M*100, (w,h))
plt.imshow(rotate)
237/13:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [200, 50],
                [50, 200]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [100, 250]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
while(1):
    
    cv2.imshow('image', img)
    if cv2.waitKey(20) & 0xFF == 27:
        break
        
cv2.destroyAllWindows()
237/14:
plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()
237/15:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [50, 200],
                [200, 50]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [100, 250]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/16:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [50, 100],
                [100, 50]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [100, 250]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/17:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [50, 300],
                [300, 50]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [100, 250]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/18:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [50, 300],
                [300, 50]])

pts2 = np.float32([[60, 100],
                [200, 50],
                [100, 250]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/19:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [50, 300],
                [300, 50]])

pts2 = np.float32([[10, 100],
                [200, 80],
                [100, 250]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/20:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [50, 300],
                [300, 50]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [100, 250]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/21:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [50, 300],
                [300, 50]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [100, 200]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/22:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [50, 300],
                [300, 50]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [100, 230]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/23:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [50, 300],
                [300, 50]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [50, 230]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/24:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [50, 300],
                [300, 50]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [100, 230]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/25:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [100, 300],
                [300, 100]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [100, 230]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/26:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[70, 70],
                [50, 300],
                [300, 50]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [100, 230]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/27:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [50, 300],
                [300, 50]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [100, 230]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/28:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [50, 300],
                [300, 50]])

pts2 = np.float32([[50, 100],
                [200, 50],
                [100, 230]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/29:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [50, 300],
                [300, 50]])

pts2 = np.float32([[10, 10],
                [200, 50],
                [100, 230]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/30:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],
                [50, 300],
                [300, 50]])

pts2 = np.float32([[10, 100],
                [200, 50],
                [200, 230]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/31:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],[200, 300],[50, 200]])

pts2 = np.float32([[10, 100],[200, 50],[100, 250]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/32:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],[200, 300],[50, 200]])

pts2 = np.float32([[10, 100],[200, 50],[100, 350]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/33:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],[200, 300],[50, 200]])

pts2 = np.float32([[10, 100],[200, 50],[200, 250]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/34:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],[200, 300],[50, 200]])

pts2 = np.float32([[10, 100],[200, 150],[100, 250]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/35:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],[200, 300],[50, 200]])

pts2 = np.float32([[10, 100],[200, 200],[100, 250]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/36:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],[200, 300],[50, 200]])

pts2 = np.float32([[10, 100],[200, 200],[250, 250]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/37:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],[200, 300],[50, 200]])

pts2 = np.float32([[10, 100],[200, 200],[100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/38:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],[200, 300],[50, 200]])

pts2 = np.float32([[100, 100],[200, 200],[100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/39:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],[200, 300],[50, 200]])

pts2 = np.float32([[10, 80],[200, 200],[100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/40:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],[200, 300],[50, 200]])

pts2 = np.float32([[10, 200],[200, 200],[100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/41:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[50, 50],[200, 300],[200, 200]])

pts2 = np.float32([[10, 200],[200, 200],[100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/42:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[100, 150],[200, 300],[50, 200]])

pts2 = np.float32([[10, 200],[200, 200],[100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/43:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[100, 200],[200, 300],[50, 200]])

pts2 = np.float32([[10, 200],[200, 200],[100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/44:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[100, 150],[200, 300],[50, 200]])

pts2 = np.float32([[10, 200],[200, 200],[100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/45:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[100, 150],[200, 300],[150, 200]])

pts2 = np.float32([[10, 200],[200, 200],[100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/46:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[100, 150],[200, 300],[1, 200]])

pts2 = np.float32([[10, 200],[200, 200],[100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/47:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[100, 150],[100, 200],[1, 200]])

pts2 = np.float32([[10, 200],[200, 200],[100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/48:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[100, 150],[1, 200],[1, 200]])

pts2 = np.float32([[10, 200],[200, 200],[100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/49:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[100, 150],[200, 200],[1, 200]])

pts2 = np.float32([[10, 200],[200, 200],[100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/50:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[100, 150],[500, 500],[1, 200]])

pts2 = np.float32([[10, 200],[200, 200],[100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/51:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[100, 150],[50, 50],[1, 200]])

pts2 = np.float32([[10, 200],[200, 200],[100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/52:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[100, 150],[50, 150],[1, 200]])

pts2 = np.float32([[10, 200],[200, 200],[100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/53:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[1, 0, 100],[0, 1, 100]])

pts2 = np.float32([[10, 200],[200, 200],[100, 100]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, pts1, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/54:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(0.6*(cols-1)), 0], [int(0.4*(cols-1)), rows-1]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, pts1, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/55:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(0.6*(cols-1)), 0], [int(0.4*(cols-1)), rows-1]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/56:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(0.6*(cols-1)), 0], [int(0.2*(cols-1)), rows-1]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/57:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(0.2*(cols-1)), 0], [int(0.2*(cols-1)), rows-1]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/58:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(0.2*(cols-1)), 0], [int(0.9*(cols-1)), rows-1]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/59:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(0.9*(cols-1)), 0], [int(0.2*(cols-1)), rows-1]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/60:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0.2*(cols-1)), rows-1]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/61:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(1*(cols-1)), rows-1]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/62:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/63:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-10, 0], [0, rows-10]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/64:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 1], [1, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/65:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(1*(cols-1)), rows-1]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/66:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/67:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[1,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/68:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

M = cv2.getAffineTransform(pts1, pts2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/69:
image = cv2.imread("C:/Users/Dell/Pictures/as.jpg")

h,w = image.shape[:2]
center = (w//2, h//2)

M = cv2.getRotationMatrix2D(center, 90, .5)

rotate = cv2.wrapAffine(image, M, (w,h))
plt.imshow(rotate)
237/70:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-30,1)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
237/71:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-90,1)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/1:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-90,1)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/2:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-90,2)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/3:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-90,1.5)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/4:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/dh.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-90,1.5)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/5:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jd.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-90,1.5)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/6:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jd.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-90,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/7:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jd.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-65,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/8:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jd.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-75,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/9:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jd.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-90,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/10:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jd.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-60,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/11:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jd.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-40,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/12:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-40,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/13:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qd.jpg')
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-40,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/14:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread(cv2.cvtColor('C:/Users/Dell/Pictures/qd.jpg', cv2.COLOR_RGB2BGR))
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-40,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/15:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qd.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-40,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/16:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/ks.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-40,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/17:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-40,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/18:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-180,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/19:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/ks.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-180,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/20:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jc.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-180,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/21:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qd.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-180,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/22:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qd.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-240,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/23:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qd.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-90,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/24:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qd.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-270,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/25:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qS.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-270,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/26:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qS.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-90,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/27:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qS.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-180,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/28:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-180,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/29:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-180,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/30:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-111,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/31:
import numpy as np

np.arange(1,360)
238/32:
import numpy as np

np.arange(0,360)
238/33:
import numpy as np

np.arange(1,360)
238/34:
import numpy as np

np.arange(1,361)
238/35:
import numpy as np
gkh = list(np.arange(1,361))
gkh[::-1]
238/36:
import numpy as np
gkh = list(np.arange(1,361))
gkh[::2]
238/37:
import numpy as np
gkh = list(np.arange(1,361))
gkh[::4]
238/38:
import numpy as np
gkh = list(np.arange(1,361))
gkh[::5]
238/39:
import numpy as np
gkh = list(np.arange(1,361))
gkh[::10]
238/40:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-351,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/41:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-341,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/42:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-331,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/43:
import numpy as np
gkh = list(np.arange(1,361))
gkh[::14]
238/44:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-351,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/45:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-337,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/46:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-323,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/47:
import numpy as np
gkh = list(np.arange(1,361))
gkh[::18]
238/48:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-343,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/49:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-325,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/50:
import numpy as np
gkh = list(np.arange(1,361))
gkh[::20]
238/51:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-341,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/52:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-321,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/53:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-301,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/54:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-41,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/55:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-141,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/56:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-201,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/57:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-221,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/58:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-201,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/59:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-161,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/60:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-321,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/61:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-381,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/62:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-21,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/63:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-341,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/64:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-241,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/65:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

# M = cv2.getAffineTransform(pts1, pts2)
M = cv2.getRotationMatrix2D((cols/2,rows/2),-221,1.3)
dst = cv2.warpAffine(img, M, (cols, rows))

plt.subplot(121)
plt.imshow(img)
plt.title('Input')

plt.subplot(122)
plt.imshow(dst)
plt.title('Output')

plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/66:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))

    plt.subplot(121)
    plt.imshow(img)
    plt.title('Input')

    plt.subplot(122)
    plt.imshow(dst)
    plt.title('Output')

    plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/67:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))

    plt.subplot(121)
    plt.imshow(img)
    plt.title('Input')

    plt.subplot(122)
    plt.imshow(dst)
    plt.title('Output')

    plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/68:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/as'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/69:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jc.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/jc'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/70:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jd.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/jd'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/71:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/jh'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/72:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/kh.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/kh'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/73:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qs.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/qs'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/74:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qd.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/qd'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/75:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/ks.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/ks'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/76:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/kh.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/kh'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/77:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/jh'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/78:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jd.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/jd'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/79:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jc.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/jc'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
238/80:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/as'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
239/1:
import openai


model_engine = "text-davinci-003"
prompt = "python code to train model for deck cards detection"

completion = openai.Completion.create(
    engine = model_engine,
    prompt = prompt,
    max_tokens = 1024,
    n =1,
    stop = None,
    temperature = 0.5)

response = completion.choices[0].text
print(response)
239/2:
import openai

openai.api_key = "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"
model_engine = "text-davinci-003"
prompt = "python code to train model for deck cards detection"

completion = openai.Completion.create(
    engine = model_engine,
    prompt = prompt,
    max_tokens = 1024,
    n =1,
    stop = None,
    temperature = 0.5)

response = completion.choices[0].text
print(response)
239/3:
import openai

openai.api_key = "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"
model_engine = "text-davinci-003"
prompt = "pytorch code to train model for multi class classification"

completion = openai.Completion.create(
    engine = model_engine,
    prompt = prompt,
    max_tokens = 1024,
    n =1,
    stop = None,
    temperature = 0.5)

response = completion.choices[0].text
print(response)
239/4:
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# define the model
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(N_FEATURES, HIDDEN_SIZE) 
        self.fc2 = nn.Linear(HIDDEN_SIZE, N_CLASSES)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

# define the training loop
def train(model, device, train_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % LOG_INTERVAL == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))

# define the testing loop
def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)

    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))

# define the main function
def main():
    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")

    # define the model
    model = Net().to(device)

    # define the optimizer
    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)

    # define the data loaders
    train_loader = torch.utils.data.DataLoader(
        datasets.MNIST('data', train=True, download=True,
                       transform=transforms.Compose([
                           transforms.ToTensor(),
                           transforms.Normalize((0.1307,), (0.3081,))
                       ])),
        batch_size=BATCH_SIZE, shuffle=True)
    test_loader = torch.utils.data.DataLoader(
        datasets.MNIST('data', train=False, transform=transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])),
        batch_size=BATCH_SIZE, shuffle=True)

    # train the model
    for epoch in range(1, NUM_EPOCHS + 1):
        train(model, device, train_loader, optimizer, epoch)
        test(model, device, test_loader)


if __name__ == '__main__':
    main()
239/5:
import openai

openai.api_key = "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"
model_engine = "text-davinci-003"
prompt = "How is BBSUL to study"

completion = openai.Completion.create(
    engine = model_engine,
    prompt = prompt,
    max_tokens = 1024,
    n =1,
    stop = None,
    temperature = 0.5)

response = completion.choices[0].text
print(response)
239/6:
import openai

openai.api_key = "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"
model_engine = "text-davinci-003"
prompt = "write a poem for my friend simra in urdu"

completion = openai.Completion.create(
    engine = model_engine,
    prompt = prompt,
    max_tokens = 1024,
    n =1,
    stop = None,
    temperature = 0.5)

response = completion.choices[0].text
print(response)
239/7:
import openai

openai.api_key = "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"
model_engine = "text-davinci-003"
prompt = "about the ivy school"

completion = openai.Completion.create(
    engine = model_engine,
    prompt = prompt,
    max_tokens = 1024,
    n =1,
    stop = None,
    temperature = 0.5)

response = completion.choices[0].text
print(response)
239/8:
import openai

openai.api_key = "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"
model_engine = "text-davinci-003"
prompt = "thanks"

completion = openai.Completion.create(
    engine = model_engine,
    prompt = prompt,
    max_tokens = 1024,
    n =1,
    stop = None,
    temperature = 0.5)

response = completion.choices[0].text
print(response)
241/1:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/as.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/as'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
241/2:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/10s.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/10s'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
241/3:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jc.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/jc'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
241/4:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/kc.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/kc'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
241/5:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qc.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/qc'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
241/6:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qh.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

pts1 = np.float32([[0,0], [cols-1, 0], [0, rows-1]])

pts2 = np.float32([[0,0], [int(1*(cols-1)), 0], [int(0*(cols-1)), rows-1]])

dim = [1,21,41,61,81,101,121,141,161,181,201,221,241,261,281,301,321,341]
# M = cv2.getAffineTransform(pts1, pts2)
for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/qh'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
242/1:
import numpy as np
gkh = list(np.arange(1,361))
len(gkh[::20])
242/2:
import numpy as np
gkh = list(np.arange(1,361))
gkh[::20]
242/3:
import numpy as np
gkh = list(np.arange(1,361))
len(gkh[::10])
242/4: 36*5
242/5:
import numpy as np
gkh = list(np.arange(1,361))
gkh[::10]
242/6:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qh.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

gkh = list(np.arange(1,361))
dim = gkh[::10]

for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
#     cv2.imwrite('C:/Users/Dell/Downloads/dataset2/qh'+str(i)+'.jpg',dst)

    plt.subplot(121)
    plt.imshow(img)
    plt.title('Input')

    plt.subplot(122)
    plt.imshow(dst)
    plt.title('Output')

    plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
242/7:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jd.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

gkh = list(np.arange(1,361))
dim = gkh[::10]

for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
#     cv2.imwrite('C:/Users/Dell/Downloads/dataset2/qh'+str(i)+'.jpg',dst)

    plt.subplot(121)
    plt.imshow(img)
    plt.title('Input')

    plt.subplot(122)
    plt.imshow(dst)
    plt.title('Output')

    plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
242/8:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qc.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

gkh = list(np.arange(1,361))
dim = gkh[::10]

for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
#     cv2.imwrite('C:/Users/Dell/Downloads/dataset2/qh'+str(i)+'.jpg',dst)

    plt.subplot(121)
    plt.imshow(img)
    plt.title('Input')

    plt.subplot(122)
    plt.imshow(dst)
    plt.title('Output')

    plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
242/9:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jd.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

gkh = list(np.arange(1,361))
dim = gkh[::10]

for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/jd'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
242/10:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/jh.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

gkh = list(np.arange(1,361))
dim = gkh[::10]

for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/jh'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
242/11:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qc.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

gkh = list(np.arange(1,361))
dim = gkh[::10]

for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/qc'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
242/12:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qd.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

gkh = list(np.arange(1,361))
dim = gkh[::10]

for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/qd'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
242/13:
import cv2
import numpy as np
from matplotlib import pyplot as plt


img = cv2.imread('C:/Users/Dell/Pictures/qs.jpg')
# img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
rows, cols, ch = img.shape

gkh = list(np.arange(1,361))
dim = gkh[::10]

for i in dim:
    
    M = cv2.getRotationMatrix2D((cols/2,rows/2),-i,1.3)
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imwrite('C:/Users/Dell/Downloads/dataset2/qs'+str(i)+'.jpg',dst)

#     plt.subplot(121)
#     plt.imshow(img)
#     plt.title('Input')

#     plt.subplot(122)
#     plt.imshow(dst)
#     plt.title('Output')

#     plt.show()

# Displaying the image
# while(1):
    
#   cv2.imshow('image', img)
#   if cv2.waitKey(20) & 0xFF == 27:
#       break
        
# cv2.destroyAllWindows()
243/1:
# Python program to explain cv2.flip() method

# importing cv2
import cv2

# path
path = r'C:/Users/Dell/Pictures/jd.jpg'

# Reading an image in default mode
src = cv2.imread(path)

# Window name in which image is displayed
window_name = 'Image'

# Using cv2.flip() method
# Use Flip code 0 to flip vertically
image = cv2.flip(src, 0)

# Displaying the image
cv2.imshow(window_name, image)
cv2.waitKey(0)
243/2: print('\n\nt')
245/1:
from ultralytics import YOLO

model = YOLO("best_15.pt")
model.fuse()
245/2:
from ultralytics import YOLO

model = YOLO("best_15.pt", map_location='cpu')
model.fuse()
245/3:
from ultralytics import YOLO

model = YOLO("best_15.pt")
model.fuse()
247/1:
from ultralytics import YOLO

model = YOLO("best_15.pt")
model.fuse()
247/2:
from PIL import Image

type(Image.open('‪C:/Users/Dell/Pictures/qs.jpg'))
247/3:
from PIL import Image

Image.open('‪C:/Users/Dell/Pictures/qs.jpg')
247/4:
from PIL import Image

t = Image.open('‪C:/Users/Dell/Pictures/qs.jpg')
247/5:
from PIL import Image

Image.open(r'‪C:\Users\Dell\Pictures\qs.jpg')
248/1:
import ctypes
user32 = ctypes.windll.user32
user32.SetProcessDPIAware()
[w, h] = [user32.GetSystemMetrics(0), user32.GetSystemMetrics(1)]
248/2: w,h
249/1: 145/1366
249/2: (145/1366)*100
249/3: 1366*10.6%
249/4: (145/1366)
249/5: 1366*0.106
249/6: 1366**0.106
249/7: 1366*0.106
249/8: round(1366*0.106)
249/9: (145/1366), (50/768)
249/10: round(1366*0.106), round(768*0.065)
249/11: 823-145
249/12: type(w)
249/13:
import ctypes
user32 = ctypes.windll.user32
user32.SetProcessDPIAware()
ax, wae = [user32.GetSystemMetrics(0), user32.GetSystemMetrics(1)]
249/14: type(ax)
250/1:
import ctypes
user32 = ctypes.windll.user32
user32.SetProcessDPIAware()
ax, wae = [user32.GetSystemMetrics(0), user32.GetSystemMetrics(1)]
250/2: w,h
250/3: ax, wae
251/1:
import time
import numpy as np
import cv2



time.sleep(0.1)
hog = cv2.HOGDescriptor()
hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    flipped = cv2.flip(frame, flipCode = -1)
    frame1 = cv2.resize(flipped, (640, 480))
    font = cv2.FONT_HERSHEY_SIMPLEX

    gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
    boxes, weights = hog.detectMultiScale(frame1, winStride=(8,8) )
    boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])

    for (xA, yA, xB, yB) in boxes:
        # display the detected boxes in the colour picture
        cv2.rectangle(frame1, (xA, yA), (xB, yB),(0, 255, 0), 2)

        b=len(boxes)
        cv2.putText(frame1,"peoplecount:"+str(b),(20,50),0,2,(255,0,0),3)
    img = cv2.resize(frame1,(640,480))
    cv2.imshow("Frame", frame1);
    key = cv2.waitKey(1) & 0xFF
    if key == ord("q"):
        break
252/1:
import time
import numpy as np
import cv2



time.sleep(0.1)
hog = cv2.HOGDescriptor()
hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    frame1 = cv2.resize(frame, (640, 480))
    font = cv2.FONT_HERSHEY_SIMPLEX

    gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
    boxes, weights = hog.detectMultiScale(frame1, winStride=(8,8) )
    boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])

    for (xA, yA, xB, yB) in boxes:
        # display the detected boxes in the colour picture
        cv2.rectangle(frame1, (xA, yA), (xB, yB),(0, 255, 0), 2)

        b=len(boxes)
        cv2.putText(frame1,"peoplecount:"+str(b),(20,50),0,2,(255,0,0),3)
    img = cv2.resize(frame1,(640,480))
    cv2.imshow("Frame", frame1);
    key = cv2.waitKey(1) & 0xFF
    if key == ord("q"):
        break
254/1:
import time
import numpy as np
import cv2



time.sleep(0.1)
hog = cv2.HOGDescriptor()
hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    frame1 = cv2.resize(frame, (640, 480))
    font = cv2.FONT_HERSHEY_SIMPLEX

    gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
    boxes, weights = hog.detectMultiScale(frame1, winStride=(8,8) )
    boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])

    for (xA, yA, xB, yB) in boxes:
        # display the detected boxes in the colour picture
        cv2.rectangle(frame1, (xA, yA), (xB, yB),(0, 255, 0), 2)

        b=len(boxes)
        cv2.putText(frame1,"peoplecount:"+str(b),(20,50),0,2,(255,0,0),3)
    img = cv2.resize(frame1,(640,480))
    
    cv2.imshow("Frame", frame1)
    key = cv2.waitKey(1)
    if key == ord("q"):
        break
255/1:
import time
import numpy as np
import cv2



time.sleep(0.1)
hog = cv2.HOGDescriptor()
hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    font = cv2.FONT_HERSHEY_SIMPLEX
    
    boxes, weights = hog.detectMultiScale(frame, winStride=(8,8) )
    boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])

    for (xA, yA, xB, yB) in boxes:
        # display the detected boxes in the colour picture
        cv2.rectangle(frame, (xA, yA), (xB, yB),(0, 255, 0), 2)

        b=len(boxes)
        cv2.putText(frame,"peoplecount:"+str(b),(20,50),0,2,(255,0,0),3)
    img = cv2.resize(frame,(640,480))
    
    cv2.imshow("Frame", frame)
#     key = cv2.waitKey(1)
#     if key == ord("q"):
#         break
    k = cv2.waitKey(10)
    if k == 13:# or k == ord('Q'):
        break
256/1:
import os

for i in os.listdir('C:/Users/Dell/Downloads/sceneries'):
    print(i)
256/2:
import os

hog = cv2.HOGDescriptor()
hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())

for i in os.listdir('C:/Users/Dell/Downloads/sceneries'):
    frame = cv2.imread('C:/Users\\Dell\\Downloads\\sceneries\\'+i)
    
    font = cv2.FONT_HERSHEY_SIMPLEX
    
    boxes, weights = hog.detectMultiScale(frame, winStride=(8,8) )
    boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])

    if len(boxes) > 0:
        print(i)
256/3:
import os
import cv2

hog = cv2.HOGDescriptor()
hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())

for i in os.listdir('C:/Users/Dell/Downloads/sceneries'):
    frame = cv2.imread('C:/Users\\Dell\\Downloads\\sceneries\\'+i)
    
    font = cv2.FONT_HERSHEY_SIMPLEX
    
    boxes, weights = hog.detectMultiScale(frame, winStride=(8,8) )
    boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])

    if len(boxes) > 0:
        print(i)
256/4:
import os
import cv2
import numpy as np

hog = cv2.HOGDescriptor()
hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())

for i in os.listdir('C:/Users/Dell/Downloads/sceneries'):
    frame = cv2.imread('C:/Users\\Dell\\Downloads\\sceneries\\'+i)
    
    font = cv2.FONT_HERSHEY_SIMPLEX
    
    boxes, weights = hog.detectMultiScale(frame, winStride=(8,8) )
    boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])

    if len(boxes) > 0:
        print(i)
256/5:
import os
import cv2
import numpy as np

body_cascade = cv2.CascadeClassifier("haarcascade_fullbody.xml")

for i in os.listdir('C:/Users/Dell/Downloads/sceneries'):
    frame = cv2.imread('C:/Users\\Dell\\Downloads\\sceneries\\'+i)
    gray_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    
    people = body_cascade.detectMultiScale(gray_img, 1.2, 1)
    
#     font = cv2.FONT_HERSHEY_SIMPLEX
    
#     boxes, weights = hog.detectMultiScale(frame, winStride=(8,8) )
#     boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])

    if len(people) > 0:
        print(i)
257/1:
def buttonz(a):
    list = []
    add = 60 *
    for i in range(a):
        x = 20
        y = 190
        add = 60 * a
        list.append([x+add,y+add])
    return list
257/2:
def buttonz(a):
    list = []
    for i in range(a):
        x = 20
        y = 190
        add = 60 * a
        list.append([x+add,y+add])
    return list
257/3:
def buttonz(a):
    list = []
    for i in range(a):
        x = 20
        y = 190
        add = 60 * a
        list.append([x+add,y+add])
    return list

buttons(5)
257/4:
def buttonz(a):
    list = []
    for i in range(a):
        x = 20
        y = 190
        add = 60 * a
        list.append([x+add,y+add])
    return list

buttonz(5)
257/5:
def buttonz(a):
    list = []
    x = 20
    y = 190
    for i in range(a):
        
        add = 60 * a
        list.append([x+add,y+add])
        add = 0
        
    return list

buttonz(5)
257/6:
for i in range(5):
    print(i)
257/7:
def buttonz(a):
    list = []
    x = 20
    y = 190
    for i in range(a):
        
        add = 60 * a
        print(add)
        list.append([x+add,y+add])
        add = 0
        
    return list

buttonz(5)
257/8:
def buttonz(a):
    list = []
    x = 20
    y = 190
    for i in range(a):
        
        add = 60 * i
        print(add)
        list.append([x+add,y+add])
        add = 0
        
    return list

buttonz(5)
257/9:
def buttonz(a):
    list = []
    x = 20
    y = 190
    for i in range(a):
        
        add = 60 * i
        list.append([x+add,y+add])
        add = 0
        
    return list

buttonz(5)
257/10:
def buttonz(a):
    list = []
    x = 20
    y = 190
    for i in range(a):
        
        add = 60 * i
        list.append([x+add,y])
        add = 0
        
    return list

buttonz(5)
257/11:
if a%5==0:
    print(True)
257/12:
if 10%5==0:
    print(True)
257/13:
if 7%5==0:
    print(True)
257/14:
if 5%5==0:
    print(True)
257/15:
def buttonz(a):
    list = []
#     count = 0
    x = 20
    y = 190
    for i in range(a):
        
        add = 60 * i
#         if 
        list.append([x+add,y])
#         add = 0
        
    return list

buttonz(5)
257/16:
def buttonz(a):
    list = []
    count = 0
    x = 20
    y = 190
    for i in range(a):
        
        add = 60 * i
        if count%5:
            y += 60
        list.append([x+add,y])
        
    return list

buttonz(5)
257/17:
def buttonz(a):
    list = []
    count = 0
    x = 20
    y = 190
    for i in range(a):
        
        add = 60 * i
        if count%5:
            y += 60
        list.append([x+add,y])
        
    return list

buttonz(8)
257/18:
def buttonz(a):
    list = []
    count = 0
    x = 20
    y = 190
    for i in range(a):
        
        add = 60 * i
        if count%5:
            y += 60
        list.append([x+add,y])
        count+=1
        
    return list

buttonz(8)
257/19:
def buttonz(a):
    list = []
    count = 0
    x = 20
    y = 190
    for i in range(a):
        
        add = 60 * i
        if count%5==0:
            y += 60
        list.append([x+add,y])
        count+=1
        
    return list

buttonz(8)
257/20:
def buttonz(a):
    list = []
    count = 0
    x = 20
    y = 190
    for i in range(a):
        
        add = 60 * i
        if count%5==0:
            print(i)
            y += 60
        list.append([x+add,y])
        count+=1
        
    return list

buttonz(8)
257/21:
if 0%5==0:
    print(True)
257/22:
def buttonz(a):
    list = []
    count = 1
    x = 20
    y = 190
    for i in range(a):
        
        add = 60 * i
        if count%5==0:
            print(i)
            y += 60
        list.append([x+add,y])
        count+=1
        
    return list

buttonz(8)
257/23:
def buttonz(a):
    list = []
    count = 0
    x = 20
    y = 190
    for i in range(a):
        
        add = 60 * i
        if count%5==0:
            print(i)
            y += 60
        list.append([x+add,y])
        count+=1
        
    return list

buttonz(8)
257/24:
def buttonz(a):
    list = []
    count = 0
    x = 20
    y = 190
    for i in range(a):
        
        add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
        list.append([x+add,y])
        count+=1
        
    return list

buttonz(8)
257/25: x
257/26:
def buttonz(a):
    list = []
    count = 0
    x = [20, 80, 140, 200, 260]
#     xx =
    y = 190
    for i in range(a):
        
        add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
#         if 
        list.append([x[i],y])
        count+=1
        
    return list

buttonz(8)
257/27:
def buttonz(a):
    list = []
    count = 0
    x = [20, 80, 140, 200, 260]
    y = 190
    for i in range(a):
        
        add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
        list.append([x+add,y])
        count+=1
        
    return list

buttonz(8)
257/28:
def buttonz(a):
    list = []
    count = 0
    x = 20
    y = 190
    for i in range(a):
        
        add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
        list.append([x+add,y])
        count+=1
        
    return list

buttonz(8)
257/29:
def buttonz(a):
    list = []
    count = 0
    x = 20
    y = 190
    for i in range(a):
        
        add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
            x = 20
        list.append([x+add,y])
        count+=1
        
    return list

buttonz(8)
257/30:
def buttonz(a):
    list = []
    count = 0
    x = 20
    y = 190
    for i in range(a):
        
        if count%5==0 and count!=0:
            print(i)
            y += 60
            x = 20
        
        else:
            add = 60 * i
        
        list.append([x+add,y])
        count+=1
        
    return list

buttonz(8)
257/31:
def buttonz(a):
    list = []
    count = 0
    x = 20
    y = 190
    for i in range(a):
        
        add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
            
            if len(list)%5==0 and count!=0:
                x = 20
        list.append([x+add,y])
        count+=1
        
    return list

buttonz(8)
257/32:
x = [20, 80, 140, 200, 260]

for i in range(8):
    print(x)
257/33:
x = [20, 80, 140, 200, 260]

for i in range(x):
    print(x)
257/34:
x = [20, 80, 140, 200, 260]

for i in x:
    print(i)
257/35:
while True:
    for i in x:
        print(i)
257/36:
def buttonz(a):
    list = []
    count = 0
    x = []
    y = 190
    
    while len(x) == a:
        for i in x:
            x.append(i)
    
    for i in range(a):
#         add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
            
        list.append([x[i],y])
        count+=1
        
    return list

buttonz(8)
257/37: x
258/1:
def buttonz(a):
    list = []
    count = 0
    x = []
    y = 190
    
    while len(x) == a:
        for i in x:
            x.append(i)
    
    for i in range(a):
#         add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
            
        list.append([x[i],y])
        count+=1
        
    return list

buttonz(8)
258/2: x
258/3: x
258/4: a
258/5:
def buttonz(a):
    list = []
    count = 0
    x = []
    y = 190
    print(x, a)
    while len(x) == a:
        for i in x:
            x.append(i)
    
    for i in range(a):
#         add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
            
        list.append([x[i],y])
        count+=1
        
    return list

buttonz(8)
258/6:
ascd = []
len(ascd)
258/7:
def buttonz(a):
    list = []
    count = 0
    xx = [20, 80, 140, 200, 260]
    x = []
    y = 190
    
    while len(x) == a:
        for i in xx:
            x.append(i)
    
    for i in range(a):
#         add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
            
        list.append([x[i],y])
        count+=1
        
    return list

buttonz(8)
258/8: x
258/9:
def buttonz(a):
    list = []
    count = 0
    xx = [20, 80, 140, 200, 260]
    x = []
    y = 190
    
    while len(x) == a:
        for i in xx:
            x.append(i)
            
    print(x)
    
    for i in range(a):
#         add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
            
        list.append([x[i],y])
        count+=1
        
    return list

buttonz(8)
258/10:
def buttonz(a):
    list = []
    count = 0
    xx = [20, 80, 140, 200, 260]
    x = []
    y = 190
    
    while len(x) == a:
        for i in xx:
            x.append(i)
            print(i)
    
    for i in range(a):
#         add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
            
        list.append([x[i],y])
        count+=1
        
    return list

buttonz(8)
258/11:
def buttonz(a):
    list = []
    count = 0
    xx = [20, 80, 140, 200, 260]
    x = []
    y = 190
    
    while len(x) != a:
        for i in xx:
            x.append(i)
    
    for i in range(a):
#         add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
            
        list.append([x[i],y])
        count+=1
        
    return list

buttonz(8)
258/12:
def buttonz(a):
    list = []
    count = 0
    xx = [20, 80, 140, 200, 260]
    x = []
    y = 190
    
    while True:
        for i in xx:
            x.append(i)
            if len(x) == a:
                break
            
    
    for i in range(a):
#         add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
            
        list.append([x[i],y])
        count+=1
        
    return list

buttonz(8)
258/13:
def buttonz(a):
    list = []
    count = 0
    xx = [20, 80, 140, 200, 260]
    x = []
    y = 190
    
    while True:
        for i in xx:
            x.append(i)
            if len(x) == a:
                break
                break
            
    
    for i in range(a):
#         add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
            
        list.append([x[i],y])
        count+=1
        
    return list

buttonz(8)
258/14:
def buttonz(a):
    list = []
    count = 0
    xx = [20, 80, 140, 200, 260]
    x = []
    y = 190
    
    while True:
        for i in xx:
            x.append(i)
            if len(x) == a:
                print(True)
                break
                break
            
    
    for i in range(a):
#         add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
            
        list.append([x[i],y])
        count+=1
        
    return list

buttonz(8)
258/15:
def buttonz(a):
    list = []
    count = 0
    xx = [20, 80, 140, 200, 260]
    x = []
    y = 190
    
    while True:
        for i in xx:
            x.append(i)
            if len(x) == a:
                print(x)
                break
                break
            
    
    for i in range(a):
#         add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
            
        list.append([x[i],y])
        count+=1
        
    return list

buttonz(8)
258/16:
def buttonz(a):
    list = []
    count = 0
    xx = [20, 80, 140, 200, 260]
    x = []
    y = 190
    
    while True:
        for i in xx:
            x.append(i)
            if len(x) == a:
                break
                break
                break
            
    
    for i in range(a):
#         add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
            
        list.append([x[i],y])
        count+=1
        
    return list

buttonz(8)
258/17:
def buttonz(a):
    Flag = True
    list = []
    count = 0
    xx = [20, 80, 140, 200, 260]
    x = []
    y = 190
    
    while Flag:
        for i in xx:
            x.append(i)
            if len(x) == a:
                Flag = False
            
    
    for i in range(a):
#         add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
            
        list.append([x[i],y])
        count+=1
        
    return list

buttonz(8)
258/18:
def buttonz(a):
    Flag = True
    list = []
    count = 0
    xx = [20, 80, 140, 200, 260]
    x = []
    y = 190
    
    while Flag:
        for i in xx:
            x.append(i)
            if len(x) == a:
                Flag = False
            
    
    for i in range(a):
#         add = 60 * i
        if count%5==0 and count!=0:
            print(i)
            y += 60
            
        list.append([x[i],y])
        count+=1
        
    return list

buttonz(25)
258/19: x
258/20:
x = [20, 80, 140, 200, 260]
y = ['dg','sdsg','sgws','kg','hrg']

for a,b in x,y:
    print(a,b)
258/21:
x = [20, 80, 140, 200, 260]
y = ['dg','sdsg','sgws','kg','hrg']

for a,b in enumrate(x,y):
    print(a,b)
258/22:
x = [20, 80, 140, 200, 260]
y = ['dg','sdsg','sgws','kg','hrg']

for a,b in enumirate(x,y):
    print(a,b)
258/23:
x = [20, 80, 140, 200, 260]
y = ['dg','sdsg','sgws','kg','hrg']

for a,b in zip(x,y):
    print(a,b)
259/1:
def buttonz(a):
    Flag = True
    list = []
    count = 0
    xx = [20, 80, 140, 200, 260]
    x = []
    y = 190
    
    # Number of x
    while Flag:
        for i in xx:
            x.append(i)
            if len(x) == a:
                Flag = False
            
    # y distancing
    for i in range(a):
        # After 5 counts increase y by 60
        if count%5==0 and count!=0:
            print(i)
            y += 60
            
        list.append([x[i],y])
        count+=1
        
    return list

buttonz(25)
259/2: 'I:/QT NoMad Project/Icons/drivers.png'.split('/')
259/3: 'I:/QT NoMad Project/Icons/drivers.png'.split('/')[-2:]
259/4: 'I:/QT NoMad Project/Icons/drivers.png'.split('/')[-2:].join('/')
259/5: 'I:/QT NoMad Project/Icons/drivers.png'.split('/')[-2:].merge('/')
259/6: str('I:/QT NoMad Project/Icons/drivers.png'.split('/')[-2:]).join('/')
259/7: str('I:/QT NoMad Project/Icons/drivers.png'.split('/')[-2:])
259/8:
flan = 'I:/QT NoMad Project/Icons/drivers.png'.split('/')[-2:]
flan[0]+'/'+flan
259/9:
flan = 'I:/QT NoMad Project/Icons/drivers.png'.split('/')[-2:]
flan[0]+'/'+flan[1]
261/1:
import face_recognition
image = face_recognition.load_image_file("F:/F/daraz/Watches/DSC_8042.jpg")

# Find all the faces in the image using the default HOG-based model.
face_locations = face_recognition.face_locations(image)

print("Found {} faces in this photograph.".format(len(face_locations)))
261/2:
from PIL import Image
import cv2

face_image = cv2.imread('F:/F/daraz/Watches/DSC_8042.jpg')
for i, face_location in enumerate(face_locations):
    # Print the location of each face in this image
    top, right, bottom, left = face_location
    
    # We can extract face using these 4 points
    face_image = image[top:bottom, left:right]
    pil_image = Image.fromarray(face_image)
    
    # Show extracted face
    pil_image.show()
261/3:
from PIL import Image
import cv2

face_image = cv2.imread('F:/F/daraz/Watches/DSC_8042.jpg')
for face_location in face_locations:
    # Print the location of each face in this image
    top, right, bottom, left = face_location
    
    # We can draw rectangle using OpenCV rectangle method
    cv2.rectangle(face_image, (left, top), (right, bottom), (255, 255, 1), 4)
    
    cv2.imshow('Image',face_image)
263/1:
import face_recognition
image = face_recognition.load_image_file("F:/F/daraz/Watches/DSC_8042.jpg")

# Find all the faces in the image using the default HOG-based model.
face_locations = face_recognition.face_locations(image)

# print("Found {} faces in this photograph.".format(len(face_locations)))
263/2:
from PIL import Image
import cv2

face_image = cv2.imread('F:/F/daraz/Watches/DSC_8042.jpg')
for face_location in face_locations:
    # Print the location of each face in this image
    top, right, bottom, left = face_location
    
    # We can draw rectangle using OpenCV rectangle method
    cv2.rectangle(face_image, (left, top), (right, bottom), (255, 255, 1), 4)
    
cv2.imshow('Image',face_image)
# (this is necessary to avoid Python kernel form crashing)
cv2.waitKey(0)
  
# closing all open windows
cv2.destroyAllWindows()
263/3:
from PIL import Image
import cv2

face_image = cv2.imread('F:/F/daraz/Watches/DSC_8042.jpg')
for face_location in face_locations:
    # Print the location of each face in this image
    top, right, bottom, left = face_location
    
    # We can draw rectangle using OpenCV rectangle method
    cv2.rectangle(face_image, (left, top), (right, bottom), (255, 255, 1), 4)
    
cv2.imwrite("out.jpg", face_image)
263/4:
cap = cv2.VideoCapture(0)

while True:
    image = cv2.read(cap)[1]
    
    faces = face_recognition.load_image_file(image)
    face_locations = face_recognition.face_locations(faces)
    
    for face_location in face_locations:
        # Print the location of each face in this image
        top, right, bottom, left = face_location

        # We can draw rectangle using OpenCV rectangle method
        cv2.rectangle(image, (left, top), (right, bottom), (255, 255, 1), 4)
        
    cv2.imshow("Output",image)
    # fps.update()
    k = cv2.waitKey(10)
    if k == 13:# or k == ord('Q'):
        break
263/5:
cap = cv2.VideoCapture(0)

while True:
    image = cap.read()[1]
    
    faces = face_recognition.load_image_file(image)
    face_locations = face_recognition.face_locations(faces)
    
    for face_location in face_locations:
        # Print the location of each face in this image
        top, right, bottom, left = face_location

        # We can draw rectangle using OpenCV rectangle method
        cv2.rectangle(image, (left, top), (right, bottom), (255, 255, 1), 4)
        
    cv2.imshow("Output",image)
    # fps.update()
    k = cv2.waitKey(10)
    if k == 13:# or k == ord('Q'):
        break
264/1:
cap = cv2.VideoCapture(0)

while True:
    image = cap.read()[1]
    
    faces = face_recognition.load_image_file(Image.fromarray(image))
    face_locations = face_recognition.face_locations(faces)
    
    for face_location in face_locations:
        # Print the location of each face in this image
        top, right, bottom, left = face_location

        # We can draw rectangle using OpenCV rectangle method
        cv2.rectangle(image, (left, top), (right, bottom), (255, 255, 1), 4)
        
    cv2.imshow("Output",image)
    # fps.update()
    k = cv2.waitKey(10)
    if k == 13:# or k == ord('Q'):
        break
264/2:
from PIL import Image
import cv2

cap = cv2.VideoCapture(0)

while True:
    image = cap.read()[1]
    
    faces = face_recognition.load_image_file(Image.fromarray(image))
    face_locations = face_recognition.face_locations(faces)
    
    for face_location in face_locations:
        # Print the location of each face in this image
        top, right, bottom, left = face_location

        # We can draw rectangle using OpenCV rectangle method
        cv2.rectangle(image, (left, top), (right, bottom), (255, 255, 1), 4)
        
    cv2.imshow("Output",image)
    # fps.update()
    k = cv2.waitKey(10)
    if k == 13:# or k == ord('Q'):
        break
265/1:
from PIL import Image
import face_recognition
import cv2

cap = cv2.VideoCapture(0)

while True:
    image = cap.read()[1]
    
    faces = face_recognition.load_image_file(Image.fromarray(image))
    face_locations = face_recognition.face_locations(faces)
    
    for face_location in face_locations:
        # Print the location of each face in this image
        top, right, bottom, left = face_location

        # We can draw rectangle using OpenCV rectangle method
        cv2.rectangle(image, (left, top), (right, bottom), (255, 255, 1), 4)
        
    cv2.imshow("Output",image)
    # fps.update()
    k = cv2.waitKey(10)
    if k == 13:# or k == ord('Q'):
        break
266/1:
from PIL import Image
import face_recognition
import cv2

cap = cv2.VideoCapture(0)

while True:
    image = cap.read()[1]
    cv2.imwrite("out.jpg", image)
    faces = face_recognition.load_image_file("out.jpg")
    face_locations = face_recognition.face_locations(faces)
    
    for face_location in face_locations:
        # Print the location of each face in this image
        top, right, bottom, left = face_location

        # We can draw rectangle using OpenCV rectangle method
        cv2.rectangle(image, (left, top), (right, bottom), (255, 255, 1), 4)
        
    cv2.imshow("Output",image)
    # fps.update()
    k = cv2.waitKey(10)
    if k == 13:# or k == ord('Q'):
        break
268/1:
from PIL import Image
import face_recognition
import cv2

cap = cv2.VideoCapture(0)

while True:
    image = cap.read()[1]
    cv2.imwrite("out.jpg", image)
    faces = face_recognition.load_image_file("out.jpg")
    face_locations = face_recognition.face_locations(faces)
    
    for face_location in face_locations:
        # Print the location of each face in this image
        top, right, bottom, left = face_location

        # We can draw rectangle using OpenCV rectangle method
        cv2.rectangle(image, (left, top), (right, bottom), (255, 255, 1), 4)
        
    cv2.imshow("Output",image)
    # fps.update()
    k = cv2.waitKey(1)
    if k == 13:# or k == ord('Q'):
        break
270/1:
from PIL import Image
import face_recognition
import cv2

cap = cv2.VideoCapture(0)

while True:
    image = cap.read()[1]
    cv2.imwrite("out.jpg", image)
    faces = face_recognition.load_image_file("out.jpg")
    face_locations = face_recognition.face_locations(faces)
    
    for face_location in face_locations:
        # Print the location of each face in this image
        top, right, bottom, left = face_location

        # We can draw rectangle using OpenCV rectangle method
        cv2.rectangle(image, (left, top), (right, bottom), (0, 0, 255), 4)
        
    cv2.imshow("Output",image)
    # fps.update()
    if cv2.waitKey(0) & 0xFF == 13:
        break
272/1:
from PIL import Image
import face_recognition
import cv2

cap = cv2.VideoCapture(0)

while True:
    image = cap.read()[1]
    cv2.imwrite("out.jpg", image)
    faces = face_recognition.load_image_file("out.jpg")
    face_locations = face_recognition.face_locations(faces)
    
    for face_location in face_locations:
        # Print the location of each face in this image
        top, right, bottom, left = face_location

        # We can draw rectangle using OpenCV rectangle method
        cv2.rectangle(image, (left, top), (right, bottom), (0, 0, 255), 4)
        
    cv2.imshow("Output",image)
    # fps.update()
    if cv2.waitKey(10) & 0xFF == 13:
        break
274/1:
from PIL import Image
import face_recognition
import cv2

cap = cv2.VideoCapture(0)

while True:
    image = cap.read()[1]
    cv2.imwrite("out.jpg", image)
    faces = face_recognition.load_image_file("out.jpg")
    face_locations = face_recognition.face_locations(faces)
    
    for face_location in face_locations:
        # Print the location of each face in this image
        top, right, bottom, left = face_location

        # We can draw rectangle using OpenCV rectangle method
        cv2.rectangle(image, (left, top), (right, bottom), (0, 0, 255), 4)
        
    cv2.imshow("Output",image)
    # fps.update()
    k = cv2.waitKey(0)
    if k == 13:# or k == ord('Q'):
        break
276/1:
from PIL import Image
import face_recognition
import cv2

cap = cv2.VideoCapture(0)

while True:
    image = cap.read()[1]
    cv2.imwrite("out.jpg", image)
    faces = face_recognition.load_image_file("out.jpg")
    face_locations = face_recognition.face_locations(faces)
    
    for face_location in face_locations:
        # Print the location of each face in this image
        top, right, bottom, left = face_location

        # We can draw rectangle using OpenCV rectangle method
        cv2.rectangle(image, (left, top), (right, bottom), (0, 0, 255), 4)
        
    cv2.imshow("Output",image)
    
    if cv2.waitKey(1)  == 13 & 0xFF == 13:
        break
278/1:
from PIL import Image
import face_recognition
import cv2

cap = cv2.VideoCapture(0)

while True:
    image = cap.read()[1]
    cv2.imwrite("out.jpg", image)
    faces = face_recognition.load_image_file("out.jpg")
    face_locations = face_recognition.face_locations(faces)
    
    for face_location in face_locations:
        # Print the location of each face in this image
        top, right, bottom, left = face_location

        # We can draw rectangle using OpenCV rectangle method
        cv2.rectangle(image, (left, top), (right, bottom), (0, 0, 255), 4)
        
    cv2.imshow("Output",image)
    
    if cv2.waitKey(1)  == 13 & 0xFF == 13:
        break

cap.release()
cv2.destroyAllWindows()
278/2:
from PIL import Image
import face_recognition
import cv2

cap = cv2.VideoCapture(0)

while True:
    image = cap.read()[1]
    cv2.imwrite("out.jpg", image)
    faces = face_recognition.load_image_file("out.jpg")
    face_locations = face_recognition.face_locations(faces)
    
    for face_location in face_locations:
        # Print the location of each face in this image
        top, right, bottom, left = face_location

        # We can draw rectangle using OpenCV rectangle method
        cv2.rectangle(image, (left, top), (right, bottom), (0, 0, 255), 4)
        
    cv2.imshow("Output",image)
    
    if cv2.waitKey(1) & 0xFF == 13:
        break

cap.release()
cv2.destroyAllWindows()
283/1: round(58.325387440270234)
283/2: round(59.83908737242202)
288/1:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('try'):
        
        img = cv2.imread('try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                print(x)
                total.append(boxes[i])
#                 box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
        
        print(total)
        total = []
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
288/2:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('try'):
        
        img = cv2.imread('try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
#                 box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
        print(x)
        print(total)
        total = []
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
288/3:
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

plt.imshow(c)
289/1:
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

plt.imshow(c[111:111+241,149:149+353])
289/2:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

plt.imshow(c[111:111+241,149:149+353])
289/3:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

#y:y:h, x:x+w
plt.imshow(c[111:111+353,149:149+241])
289/4:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

#y:y:h, x:x+w
plt.imshow(c[91:111+353,149:149+241])
289/5:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

x, y, w, h = y, x, h, w

#y:y:h, x:x+w
plt.imshow(c[149:149+241,111:111+353])
289/6:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
plt.imshow(c[149:149+241,111:111+353])
289/7:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
plt.imshow(c[149:149+221,111:111+353])
289/8:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
plt.imshow(c[149:149+221,111:111+253])
289/9:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
plt.imshow(c[149:149+221,111:111])
289/10:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
plt.imshow(c[149:149+221,111:111+153])
289/11:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
plt.imshow(c[149:149+221,111:111+123])
289/12:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
plt.imshow(c[149:149+221,111:111+143])
289/13:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
plt.imshow(c[149:149+221,111:111+133])
289/14:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
plt.imshow(c[149:149+221,111:111+128])
289/15:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
plt.imshow(c[149:149+221,111:111+130])
289/16:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
plt.imshow(c[149:149+201,111:111+130])
289/17:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
plt.imshow(c[149:149+204,111:111+130])
289/18:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[149:353,111:241])
289/19:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[129:353,111:241])
289/20:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[149:353,111:241])
289/21:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[149:373,111:241])
289/22:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[149:353,111:241])
289/23:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[149:353,91:241])
289/24:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[149:353,111:241])
289/25:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[149:353,111:221])
289/26:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# x, y, w, h = y, x, h, w

#y:y:h, x:x+w
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[149:353,111:241])
289/27:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# up, down, left, right

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[149:353,283:438])
289/28:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# up, down, left, right

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[189:353,283:438])
289/29:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# up, down, left, right

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[119:353,283:438])
289/30:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# up, down, left, right

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[149:353,111:241])
289/31: plt.imshow(c)
289/32:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# up, down, left, right

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[149:353,111:441])
289/33:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# up, down, left, right

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[149:353,111:439])
289/34:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# up, down, left, right

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[149:353,111:438])
289/35:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[582:25,1581:2291])
289/36:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[582:25,1581:1416])
289/37:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[582:1105,1581:2291])
289/38:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[582:1105,1525:2291])
289/39: plt.imshow(c)
289/40:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[582:650,1525:2291])
289/41:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[582:1105,1525:2291])
289/42:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[582:1105,25:2291])
289/43:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[1159:1105,25:2291])
289/44:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[25:1105,25:2291])
289/45:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[582:1105,25:2291])
289/46:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[582:1705,25:2291])
289/47:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[582:1690,25:2291])
289/48:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[582:1670,25:2291])
289/49:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[582:1640,25:2291])
289/50:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[582:1600,25:2291])
289/51:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[582:1400,25:2291])
289/52:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[582:1500,25:2291])
289/53:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[582:1570,25:2291])
289/54:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest

# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[582:1581,25:2291])
289/55:
d = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest
149, 111, 353,
# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(d[149:353,111:438])
289/56:
d = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest
149, 111, 353,
# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(d[149:353,111:238])
289/57:
d = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

# up: smallest, left: smallest, down: largest, right: largest
149, 111, 353,
# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(d[149:111,353:438])
289/58:
d = cv2.imread('try2/00e92fb32a8e990e8d97549b1cb03a24fac062c3.jpg')

# up: smallest, left: smallest, down: largest, right: largest
149, 111, 353,
# 
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(d[149:353,111:438])
289/59:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

up, down, left, right = 149, 111, 353, 241
# up: smallest, left: smallest, down: largest, right: largest

# 149:353,111:438 582:1581,25:2291
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[up:left, down:right])
289/60:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

up, down, left, right = 582, 25, 1581, 2291
# up: smallest, left: smallest, down: largest, right: largest

# 582:1581,25:2291
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[up:left, down:right])
289/61: total
289/62:
t = [(149, 111, 353, 241), (172, 350, 290, 438), (176, 329, 283, 400)]

t[::1]
289/63:
t = [(149, 111, 353, 241), (172, 350, 290, 438), (176, 329, 283, 400)]

t[:1:]
289/64:
t = [(149, 111, 353, 241), (172, 350, 290, 438), (176, 329, 283, 400)]

t[1::]
289/65:
t = [(149, 111, 353, 241), (172, 350, 290, 438), (176, 329, 283, 400)]

t[::1]
289/66:
t = [(149, 111, 353, 241), (172, 350, 290, 438), (176, 329, 283, 400)]

t[::2]
289/67:
t = [(149, 111, 353, 241), (172, 350, 290, 438), (176, 329, 283, 400)]

t[::-1]
289/68:
t = [(149, 111, 353, 241), (172, 350, 290, 438), (176, 329, 283, 400)]

t[:-1:-1]
289/69:
t = [(149, 111, 353, 241), (172, 350, 290, 438), (176, 329, 283, 400)]

t[:-1:]
289/70:
t = [(149, 111, 353, 241), (172, 350, 290, 438), (176, 329, 283, 400)]

for i in range(3):
    print(t[i][0])
289/71:
t = [(149, 111, 353, 241), (172, 350, 290, 438), (176, 329, 283, 400)]

u,d,l,r = [],[],[],[]
for i in range(3):
    u.append(t[i][0])
    print(min(u))
    d.append(t[i][1])
    print(min(d))
    l.append(t[i][2])
    print(max(l))
    r.append(t[i][3])
    print(max(r))
289/72:
t = [(149, 111, 353, 241), (172, 350, 290, 438), (176, 329, 283, 400)]

u,d,l,r = [],[],[],[]
# for i in range(3):
u.append(t[i][0])
print(min(u))
d.append(t[i][1])
print(min(d))
l.append(t[i][2])
print(max(l))
r.append(t[i][3])
print(max(r))
289/73:
t = [(149, 111, 353, 241), (172, 350, 290, 438), (176, 329, 283, 400)]

u,d,l,r = [],[],[],[]
for i in range(1):
    u.append(t[i][0])
    print(min(u))
    d.append(t[i][1])
    print(min(d))
    l.append(t[i][2])
    print(max(l))
    r.append(t[i][3])
    print(max(r))
289/74:
t = [(582, 25, 1545, 2291), (1159, 1105, 1525, 1416), (872, 650, 1581, 1007)]

u,d,l,r = [],[],[],[]
for i in range(1):
    u.append(t[i][0])
    print(min(u))
    d.append(t[i][1])
    print(min(d))
    l.append(t[i][2])
    print(max(l))
    r.append(t[i][3])
    print(max(r))
289/75:
t = [(582, 25, 1545, 2291), (1159, 1105, 1525, 1416), (872, 650, 1581, 1007)]

u,d,l,r = [],[],[],[]
for i in range(3):
    u.append(t[i][0])
    print(min(u))
    d.append(t[i][1])
    print(min(d))
    l.append(t[i][2])
    print(max(l))
    r.append(t[i][3])
    print(max(r))
289/76:
t = [(582, 25, 1545, 2291), (1159, 1105, 1525, 1416), (872, 650, 1581, 1007)]

u,d,l,r = [],[],[],[]
for i in range(3):
    u.append(t[i][0])
    d.append(t[i][1])
    l.append(t[i][2])
    r.append(t[i][3])
    

print(min(u))
print(min(d))
print(max(l))
print(max(r))
289/77:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('try'):
        
        img = cv2.imread('try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
#                 box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(3):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        total = []
        
        
        
        
        cv2.imwrite('try2/'+x,img[min(u): max(l), min(d):max(r)])
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/78:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('try'):
        
        img = cv2.imread('try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
#                 box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(3):
            if len(total[i]) != 0:
                u.append(total[i][0])
                d.append(total[i][1])
                l.append(total[i][2])
                r.append(total[i][3])
        total = []
        
        
        
        
        cv2.imwrite('try2/'+x,img[min(u): max(l), min(d):max(r)])
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/79:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('try'):
        
        img = cv2.imread('try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
#                 box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            if len(total[i]) != 0:
                u.append(total[i][0])
                d.append(total[i][1])
                l.append(total[i][2])
                r.append(total[i][3])
        total = []
        
        
        
        
        cv2.imwrite('try2/'+x,img[min(u): max(l), min(d):max(r)])
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/80:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('try'):
        
        img = cv2.imread('try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
#                 box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        total = []
        
        
        
        
        cv2.imwrite('try2/'+x,img[min(u): max(l), min(d):max(r)])
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/81:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('try'):
        
        img = cv2.imread('try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
#                 box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
        print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        total = []
        
        
        
        
        cv2.imwrite('try2/'+x,img[min(u): max(l), min(d):max(r)])
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/82:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('try'):
        
        img = cv2.imread('try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
#                 box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
        print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        total = []
        
        
        
        
        cv2.imwrite('try2/'+x,np.hstack(img,img[min(u): max(l), min(d):max(r)]))
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/83:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('try'):
        
        img = cv2.imread('try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
#                 box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
        print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        total = []
        
        
        
        
        cv2.imwrite('try2/'+x,np.hstack((img,img[min(u): max(l), min(d):max(r)])))
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/84:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('try'):
        
        img = cv2.imread('try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
#                 box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
        print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        total = []
        
        
        
        
        cv2.imwrite('try2/'+x,np.hconcat([img,img[min(u): max(l), min(d):max(r)]]))
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/85:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('try'):
        
        img = cv2.imread('try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
#                 box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
        print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        total = []
        
        
        
        
        cv2.imwrite('try2/'+x,cv2.hconcat([img,img[min(u): max(l), min(d):max(r)]]))
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/86:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('try'):
        
        img = cv2.imread('try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
#                 box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
        print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        total = []
        
        
        
        
        cv2.imwrite('try2/'+x,img[min(u): max(l), min(d):max(r)])
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/87:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('try'):
        
        img = cv2.imread('try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
                cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
        print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        total = []
        
        
        
        
        cv2.imwrite('try2/'+x,img[min(u): max(l), min(d):max(r)])
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/88:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('try'):
        
        img = cv2.imread('try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
                cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
        print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        total = []
        
        
        
        print(min(u), max(l), min(d), max(r))
        cv2.imwrite('try2/'+x,img[min(u): max(l), min(d):max(r)])
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/89:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

up, down, left, right = 149, 353, 111, 438
# up: smallest, down: smallest, left: largest, right: largest

# 582:1581,25:2291
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[up:left, down:right])
289/90:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

up, down, left, right = 149, 353, 111, 438
# up: smallest, down: smallest, left: largest, right: largest

# 582:1581,25:2291
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[up:down, left:right])
289/91:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

up, down, left, right = 582, 25, 1581, 2291
# up: smallest, down: smallest, left: largest, right: largest

# 582:1581,25:2291
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[up:left, down:right])
289/92:
import cv2
import matplotlib.pyplot as plt 
c = cv2.imread('try2/00ee1708354d642ee49809f6117d031abb8691af.jpg')

up, down, left, right = 582, 25, 1581, 2291
# up: smallest, down: smallest, left: largest, right: largest

# 582:1581,25:2291
# plt.imshow(c[149:149+204,111:111+130])
plt.imshow(c[up:left, down:right])
289/93:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('try'):
        
        img = cv2.imread('try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
                cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
        print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        cv2.imwrite('try2/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/94:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('try'):
        
        img = cv2.imread('try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
        print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        cv2.imwrite('try2/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/95:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('picnic'):
        
        img = cv2.imread('picnic/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        cv2.imwrite('picnic 50/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/96:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('picnic'):
        
        img = cv2.imread('picnic/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
        print(min(u), max(l), min(d), max(r))
        if len(min(u)) or len(max(l)) or len(min(d)) or len(max(r)) != 0:
            cv2.imwrite('picnic 50/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/97:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('picnic'):
        
        img = cv2.imread('picnic/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('picnic 50/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/98:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('try'):
        
        img = cv2.imread('try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('try2/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
289/99:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
 
    for x in os.listdir('picnic'):
        
        img = cv2.imread('picnic/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('picnic 50/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
290/1:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
    
#     os.makedirs(folder_path, exist_ok=True)
 
    for x in os.listdir('Example'):
        
        img = cv2.imread('Example/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Example2/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
290/2:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.5
    total = []
    # cap = cv2.VideoCapture(0)
    
#     os.makedirs(folder_path, exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples2/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
291/1:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.4
    total = []
    # cap = cv2.VideoCapture(0)
    
#     os.makedirs(folder_path, exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples2/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
291/2:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = 'object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.25
    total = []
    # cap = cv2.VideoCapture(0)
    
#     os.makedirs(folder_path, exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples2/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
291/3:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017\faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.7
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples3', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples3/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
293/1:

# for j in os.listdir('Examples'):
#     for k in :
#         if j not in k:
#             print()
os.listdir('Examples2')
293/2:
import os
# for j in os.listdir('Examples'):
#     for k in :
#         if j not in k:
#             print()
os.listdir('Examples2')
293/3:
import os

list = os.listdir('Examples2')

for j in os.listdir('Examples'):
    if j not in list:
        print(j)
293/4:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017\faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.7
    total = []
    # cap = cv2.VideoCapture(0)
    
#     os.makedirs('Examples3', exist_ok=True)
 
    for x in os.listdir('Try'):
        
        img = cv2.imread('Try/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Try2/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
293/5:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017\faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.7
    total = []
    # cap = cv2.VideoCapture(0)
    
#     os.makedirs('Examples3', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples2/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
293/6:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.7
    total = []
    # cap = cv2.VideoCapture(0)
    
#     os.makedirs('Examples3', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples3/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
293/7:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.4
    total = []
    # cap = cv2.VideoCapture(0)
    
#     os.makedirs('Examples3', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples3/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
293/8:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.3
    total = []
    # cap = cv2.VideoCapture(0)
    
#     os.makedirs('Examples3', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples3/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
293/9:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.25
    total = []
    # cap = cv2.VideoCapture(0)
    
#     os.makedirs('Examples3', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples3/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
293/10:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\rfcn_resnet101_coco_11_06_2017\rfcn_resnet101_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.7
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples4', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples4/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
293/11:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\rfcn_resnet101_coco_11_06_2017\rfcn_resnet101_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.4
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples4', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples4/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
294/1:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import time
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
start_time = time.time()
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\rfcn_resnet101_coco_11_06_2017\rfcn_resnet101_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.4
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples4', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples4/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
end_time = time.time()

print("Elapsed Time:", end_time-start_time)
294/2:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import time
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
start_time = time.time()
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.25
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples4', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples3/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
end_time = time.time()

print("Elapsed Time:", end_time-start_time)

# 8 minutes
294/3:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import time
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
start_time = time.time()
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.25
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples5', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples5/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
end_time = time.time()

print("Elapsed Time:", end_time-start_time)

# 8 minutes
294/4:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import time
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
start_time = time.time()
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.2
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples5', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples5/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
end_time = time.time()

print("Elapsed Time:", end_time-start_time)

# 8 minutes
294/5:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import time
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
start_time = time.time()
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.1
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples5', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples5/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
end_time = time.time()

print("Elapsed Time:", end_time-start_time)

# 8 minutes
294/6:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import time
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
start_time = time.time()
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.05
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples5', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples5/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
end_time = time.time()

print("Elapsed Time:", end_time-start_time)

# 8 minutes
294/7:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import time
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
start_time = time.time()
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.08
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples5', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples5/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
end_time = time.time()

print("Elapsed Time:", end_time-start_time)

# 8 minutes
294/8:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import time
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
start_time = time.time()
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.15
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples5', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples5/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
end_time = time.time()

print("Elapsed Time:", end_time-start_time)

# 8 minutes
294/9:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import time
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
start_time = time.time()
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.1
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples5', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples5/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
end_time = time.time()

print("Elapsed Time:", end_time-start_time)

# 8 minutes
294/10:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import time
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
start_time = time.time()
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.09
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples5', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples5/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
end_time = time.time()

print("Elapsed Time:", end_time-start_time)

# 8 minutes
294/11:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import time
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
start_time = time.time()
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.08
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples5', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples5/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
end_time = time.time()

print("Elapsed Time:", end_time-start_time)

# 8 minutes
294/12:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import time
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
start_time = time.time()
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.08
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples5', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples5/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
end_time = time.time()

print("Elapsed Time:", end_time-start_time)

# 8 minutes
294/13:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import time
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
start_time = time.time()
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.07
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples5', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples5/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
end_time = time.time()

print("Elapsed Time:", end_time-start_time)

# 8 minutes
294/14:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import time
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
start_time = time.time()
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.2
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples5', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples5/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
end_time = time.time()

print("Elapsed Time:", end_time-start_time)

# 8 minutes
294/15:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import time
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
start_time = time.time()
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.15
    total = []
    # cap = cv2.VideoCapture(0)
    
    os.makedirs('Examples5', exist_ok=True)
 
    for x in os.listdir('Examples'):
        
        img = cv2.imread('Examples/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('Examples5/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
end_time = time.time()

print("Elapsed Time:", end_time-start_time)

# 8 minutes
294/16:
# Code adapted from Tensorflow Object Detection Framework
# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
# Tensorflow Object Detection Detector

import numpy as np
import tensorflow as tf
import time
import cv2
import os


class DetectorAPI:
    def __init__(self, path_to_ckpt):
        self.path_to_ckpt = path_to_ckpt

        self.detection_graph = tf.Graph()
        with self.detection_graph.as_default():
            od_graph_def = tf.compat.v1.GraphDef()
            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

        self.default_graph = self.detection_graph.as_default()
        self.sess = tf.compat.v1.Session(graph=self.detection_graph)

        # Definite input and output Tensors for detection_graph
        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')
        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')
        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')

    def processFrame(self, image):
        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(image, axis=0)
        # Actual detection.
        
        (boxes, scores, classes, num) = self.sess.run(
            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],
            feed_dict={self.image_tensor: image_np_expanded})

        im_height, im_width,_ = image.shape
        boxes_list = [None for i in range(boxes.shape[1])]
        for i in range(boxes.shape[1]):
            boxes_list[i] = (int(boxes[0,i,0] * im_height),
                        int(boxes[0,i,1]*im_width),
                        int(boxes[0,i,2] * im_height),
                        int(boxes[0,i,3]*im_width))

        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])

    def close(self):
        self.sess.close()
        self.default_graph.close()

u,d,l,r = [],[],[],[]
start_time = time.time()
        
if __name__ == "__main__":
    model_path = r'C:\Users\Dell\Downloads\ssd_inception_v2_coco_11_06_2017\ssd_inception_v2_coco_11_06_2017\frozen_inference_graph.pb'
    odapi = DetectorAPI(path_to_ckpt=model_path)
    threshold = 0.15
    total = []
    # cap = cv2.VideoCapture(0)
    
#     os.makedirs('Examples5', exist_ok=True)
 
    for x in os.listdir('picnic'):
        
        img = cv2.imread('picnic/'+x)
        # img = cv2.resize(img, (1280, 720))

        boxes, scores, classes, num = odapi.processFrame(img)
        

        # Visualization of the results of a detection.

        for i in range(len(boxes)):
            # Class 1 represents human
            if classes[i] == 1 and scores[i] > threshold:
                
                total.append(boxes[i])
                box = boxes[i]
                
                
#                 # print(len(box))
#                 # if len(box) == 0:
#                     # print(x)
#                     # print(box)
#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)
#         print(x)
#         print(total)
        
        for i in range(len(total)):
            u.append(total[i][0])
            d.append(total[i][1])
            l.append(total[i][2])
            r.append(total[i][3])
        
        
        
        
#         print(min(u), max(l), min(d), max(r))
        if len(u) or len(l) or len(d) or len(r) != 0:
            cv2.imwrite('picnic 50/'+x,img[min(u): max(l), min(d):max(r)])
        
        total,u,d,l,r = [],[],[],[],[]
            

#         cv2.imwrite('try2/'+x, img)
        # key = cv2.waitKey(1)
        # if key & 0xFF == ord('q'):
        #     break
end_time = time.time()

print("Elapsed Time:", end_time-start_time)

# 8 minutes
295/1:
import cv2
import numpy as np
import os

# Function to read all images in a folder
def read_images_from_folder(folder):
    images = []
    for filename in os.listdir(folder):
        img = cv2.imread(os.path.join(folder,filename))
        if img is not None:
            images.append(img)
    return images

# Function to create a mask of a specific shape
def create_mask(shape, size):
    if shape == 'circle':
        mask = np.zeros(size, dtype=np.uint8)
        cv2.circle(mask, (size[1]//2, size[0]//2), int(min(size[0], size[1])/2), (255,255,255), -1)
    elif shape == 'heart':
        mask = np.zeros(size, dtype=np.uint8)
        cv2.ellipse(mask, (size[1]//2, size[0]//3), (size[1]//3, size[0]//3), 0, 0, 180, (255, 255, 255), -1)
    return mask

# Get user input for images and shape
folder = "Examples2" #input("Enter folder name containing images: ")
shape = "heart" #input("Enter shape (circle/heart): ")
output_path = "Examples6" #input("Enter output file name: ")

# Read images from folder
images = read_images_from_folder(folder)

# Create mask for the selected shape
mask = create_mask(shape, images[0].shape[:2])

# Initialize output image
output = np.zeros_like(images[0])

# Loop over all images and place them inside the mask
for img in images:
    img_resized = cv2.resize(img, (mask.shape[1], mask.shape[0]))
    output = cv2.bitwise_or(output, cv2.bitwise_and(img_resized, img_resized, mask=mask))

# Save output image
cv2.imwrite(output_path, output)
296/1:
import cv2
import matplotlib.pyplot as plt
import os

for i in os.listdir('Examples5'):
    
    img = cv2.imread(i)
    h,w,_ = img.shape
    img = cv2.resize(img, (w//3,h//3), interpolation = cv2.INTER_AREA)
    plt.imshow('Examples5/'+img)
296/2:
import cv2
import matplotlib.pyplot as plt
import os

for i in os.listdir('Examples5'):
    
    img = cv2.imread(i)
    h,w,_ = img.shape()
    img = cv2.resize(img, (w//3,h//3), interpolation = cv2.INTER_AREA)
    plt.imshow('Examples5/'+img)
296/3:
import cv2
import matplotlib.pyplot as plt
import os

for i in os.listdir('Examples5'):
    
    img = cv2.imread(i)
    h,w,_ = img.shape
    img = cv2.resize(img, (w//3,h//3), interpolation = cv2.INTER_AREA)
    plt.imshow('Examples5/'+img)
296/4:
import cv2
import matplotlib.pyplot as plt
import os

for i in os.listdir('Examples5'):
    
    img = cv2.imread(i)
    h,w,_ = lambda img: tuple(img.shape[1::-1])
    img = cv2.resize(img, (w//3,h//3), interpolation = cv2.INTER_AREA)
    plt.imshow('Examples5/'+img)
296/5: h
296/6: w
296/7:
import cv2
import matplotlib.pyplot as plt
import os

for i in os.listdir('Examples5'):
    
    img = cv2.imread('Examples5/'+i)
    h,w,_ = img.shape
    img = cv2.resize(img, (w//3,h//3), interpolation = cv2.INTER_AREA)
    plt.imshow('Examples5/'+img)
296/8:
import cv2
import matplotlib.pyplot as plt
import os

for i in os.listdir('Examples5'):
    
    img = cv2.imread('Examples5/'+i)
    h,w,_ = img.shape
    img = cv2.resize(img, (w//3,h//3), interpolation = cv2.INTER_AREA)
    
    plt.imshow(img)
296/9:
import cv2
import matplotlib.pyplot as plt
import os

for i in os.listdir('Examples5'):
    
    img = cv2.imread('Examples5/'+i)
    h,w,_ = img.shape
    img = cv2.resize(img, (w//3,h//3), interpolation = cv2.INTER_AREA)
    
plt.imshow(img)
296/10:
import cv2
import matplotlib.pyplot as plt
import os

for i in os.listdir('Examples5'):
    
    img = cv2.imread('Examples5/'+i)
    h,w,_ = img.shape
    img = cv2.resize(img, (w//3,h//3), interpolation = cv2.INTER_AREA)
    
    cv2.imshow(img)
296/11:
import cv2
import matplotlib.pyplot as plt
import os

for i in os.listdir('Examples5'):
    
    img = cv2.imread('Examples5/'+i)
    h,w,_ = img.shape
    img = cv2.resize(img, (w//3,h//3), interpolation = cv2.INTER_AREA)
    
    cv2.imShow(img)
296/12:
import cv2
import matplotlib.pyplot as plt
import os

for i in os.listdir('Examples5'):
    
    img = cv2.imread('Examples5/'+i)
    h,w,_ = img.shape
    img = cv2.resize(img, (w//3,h//3), interpolation = cv2.INTER_AREA)
    
    cv2.imshow('image',img)
297/1:
import cv2
import time
import os

for i in os.listdir('Examples5'):
    
    img = cv2.imread('Examples5/'+i)
    h,w,_ = img.shape
    img = cv2.resize(img, (w//3,h//3), interpolation = cv2.INTER_AREA)
    
    cv2.imshow('image',img)
    time.sleep(2)
299/1:
import cv2
import time
import os

for i in os.listdir('Examples5'):
    
    img = cv2.imread('Examples5/'+i)
    h,w,_ = img.shape
    img = cv2.resize(img, (w//3,h//3), interpolation = cv2.INTER_AREA)
    
    cv2.imshow('image',img)
    
    cv2.waitKey(0)
cv2.destroyAllWindows()
299/2:
import cv2
import numpy as np

img = np.zeros((600, 900, 3), dtype=np. uint8) #background

cv2.rectangle(img, (0,0), (900,500), (255,225,85), -1) 
cv2.rectangle(img, (0,500), (900,600), (75,180,70), -1)
#sun
cv2.circle(img, (200,150), 60, (0,255,255), -1) 
cv2.circle(img, (200,150), 75, (220,255,255), 10)
cv2.imshow("tree", img)
cv2.destroyAllWindows()
299/3:
import cv2
import numpy as np

img = np.zeros((600, 900, 3), dtype=np. uint8) #background

cv2.rectangle(img, (0,0), (900,500), (255,225,85), -1) 
cv2.rectangle(img, (0,500), (900,600), (75,180,70), -1)
#sun
cv2.circle(img, (200,150), 60, (0,255,255), -1) 
cv2.circle(img, (200,150), 75, (220,255,255), 10)
cv2.imshow("tree", img)

cv2.waitKey(0)
cv2.destroyAllWindows()
299/4:
import cv2
import numpy as np

img = np.zeros((600, 900, 3), dtype=np. uint8) #background

cv2.rectangle(img, (0,0), (900,500), (255,225,85), -1) 
cv2.rectangle(img, (0,500), (900,600), (75,180,70), -1)
#sun
cv2.circle(img, (200,150), 60, (0,255,255), -1) 
cv2.circle(img, (200,150), 75, (22,255,255), 10)
cv2.imshow("tree", img)

cv2.waitKey(0)
cv2.destroyAllWindows()
299/5:
import cv2
import numpy as np

img = np.zeros((600, 900, 3), dtype=np. uint8) #background

cv2.rectangle(img, (0,0), (900,500), (255,225,85), -1) 
cv2.rectangle(img, (0,500), (900,600), (75,180,70), -1)
#sun
cv2.circle(img, (200,150), 60, (0,255,255), -1) 
cv2.circle(img, (200,150), 75, (155,255,255), 10)
cv2.imshow("tree", img)

cv2.waitKey(0)
cv2.destroyAllWindows()
299/6:
import cv2
import numpy as np

img = np.zeros((600, 900, 3), dtype=np. uint8) #background

cv2.rectangle(img, (0,0), (900,500), (255,225,85), -1) 
cv2.rectangle(img, (0,500), (900,600), (75,180,70), -1)
#sun
cv2.circle(img, (200,150), 60, (0,255,255), -1) 
cv2.circle(img, (200,150), 75, (100,255,255), 10)
cv2.imshow("tree", img)

cv2.waitKey(0)
cv2.destroyAllWindows()
299/7:
import cv2
import numpy as np

img = np.zeros((600, 900, 3), dtype=np. uint8) #background

cv2.rectangle(img, (0,0), (900,500), (255,225,85), -1) 
cv2.rectangle(img, (0,500), (900,600), (75,180,70), -1)
#sun
cv2.circle(img, (200,150), 60, (0,255,255), -1) 
cv2.circle(img, (200,150), 60, (100,255,255), 10)
cv2.imshow("tree", img)

cv2.waitKey(0)
cv2.destroyAllWindows()
299/8:
import cv2
import numpy as np

img = np.zeros((700, 1300, 3), dtype=np. uint8) #background

cv2.rectangle(img, (0,0), (900,500), (255,225,85), -1) 
cv2.rectangle(img, (0,500), (900,600), (75,180,70), -1)
#sun
cv2.circle(img, (200,150), 60, (0,255,255), -1) 
cv2.circle(img, (200,150), 60, (100,255,255), 10)
cv2.imshow("tree", img)

cv2.waitKey(0)
cv2.destroyAllWindows()
299/9:
import cv2
import numpy as np

img = np.zeros((700, 1350, 3), dtype=np. uint8) #background 600 900

cv2.rectangle(img, (0,0), (1350,600), (255,225,85), -1) 
cv2.rectangle(img, (0,500), (1350,700), (75,180,70), -1)
#sun
cv2.circle(img, (250,350), 60, (0,255,255), -1) 
cv2.circle(img, (250,350), 60, (100,255,255), 10)
cv2.imshow("tree", img)

cv2.waitKey(0)
cv2.destroyAllWindows()
299/10:
import cv2
import numpy as np

img = np.zeros((700, 1350, 3), dtype=np. uint8) #background 600 900

cv2.rectangle(img, (0,0), (1350,600), (255,225,85), -1) 
cv2.rectangle(img, (0,500), (1350,700), (75,180,70), -1)
#sun
cv2.circle(img, (250,250), 60, (0,255,255), -1) 
cv2.circle(img, (250,250), 60, (100,255,255), 10)
cv2.imshow("tree", img)

cv2.waitKey(0)
cv2.destroyAllWindows()
299/11:
import cv2
import numpy as np

img = np.zeros((700, 1350, 3), dtype=np. uint8) #background 600 900

cv2.rectangle(img, (0,0), (1350,600), (255,225,85), -1) 
cv2.rectangle(img, (0,600), (1350,700), (75,180,70), -1)
#sun
cv2.circle(img, (250,250), 60, (0,255,255), -1) 
cv2.circle(img, (250,250), 60, (100,255,255), 10)
cv2.imshow("tree", img)

cv2.waitKey(0)
cv2.destroyAllWindows()
299/12:
import cv2
import numpy as np

img = np.zeros((700, 1350, 3), dtype=np. uint8) #background 600 900

cv2.rectangle(img, (0,0), (1350,600), (255,225,85), -1) 
cv2.rectangle(img, (0,600), (1350,700), (75,180,70), -1)
#sun
cv2.circle(img, (250,250), 60, (0,255,255), -1) 
cv2.circle(img, (250,250), 60, (100,255,255), 10)
cv2.imshow("tree", img)

cv2.waitKey(0)
cv2.destroyAllWindows()
299/13:
import cv2
import numpy as np

img = np.zeros((700, 1350, 3), dtype=np. uint8) #background 600 900

cv2.rectangle(img, (0,0), (1350,600), (255,225,85), -1) 
cv2.rectangle(img, (0,600), (1350,700), (75,180,70), -1)
#sun
cv2.circle(img, (250,250), 60, (0,255,255), -1) 
cv2.circle(img, (250,250), 60, (100,255,255), 10)
cv2.imshow("tree", img)

cv2.waitKey(0)
cv2.destroyAllWindows()
299/14:
arr = []
for i in os.listdir('Examples5'):
    arr.append(cv2.imread('Examples5/'+i))
    
len(arr)
299/15: np.concatinate((arr[0],arr[1]))
299/16: np.concatenate((arr[0],arr[1]))
299/17:
arr,shape = [],[]
for i in os.listdir('Examples5'):
    img = cv2.imread('Examples5/'+i)
    arr.append(img)
    shape.append(img.shape[:2])
    
len(arr)
len(shape)
299/18:
# np.concatenate((arr[0],arr[1]))
np.hstack([arr[0],arr[1]])
299/19: arr[0].shape
299/20:
# np.concatenate((arr[0],arr[1]))
np.hstack([arr[0],arr[0]])
299/21:
# np.concatenate((arr[0],arr[1]))
cv2.imshow("tree", np.hstack([arr[0],arr[0]]))

cv2.waitKey(0)
cv2.destroyAllWindows()
299/22:
# np.concatenate((arr[0],arr[1]))
row1 = np.hstack([arr[0],arr[0]])
row2 = np.hstack([arr[1],arr[1]])
cv2.imshow("tree", np.vstack(row1, row2))

cv2.waitKey(0)
cv2.destroyAllWindows()
299/23:
# np.concatenate((arr[0],arr[1]))
row1 = np.hstack([arr[0],arr[0]])
# row2 = np.hstack([arr[1],arr[1]])
cv2.imshow("tree", np.vstack(row1, row2))
cv2.imshow("tree", row1)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/24:
# np.concatenate((arr[0],arr[1]))
row1 = np.hstack([arr[0],arr[0]])
# row2 = np.hstack([arr[1],arr[1]])
# cv2.imshow("tree", np.vstack(row1, row2))
cv2.imshow("tree", row1)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/25:
# np.concatenate((arr[0],arr[1]))
row1 = np.hstack([arr[1],arr[1]])
# row2 = np.hstack([arr[1],arr[1]])
# cv2.imshow("tree", np.vstack(row1, row2))
cv2.imshow("tree", row1)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/26: arr[1].shape
299/27: np.arange(4,5)
299/28: np.arange(1,5)
299/29: np.zeros(1,5)
299/30: np.zeros((1,5))
299/31: arr[0].shape
299/32:
# np.concatenate((arr[0],arr[1]))
row1 = np.vstack([arr[1], np.zeros((382, 583)), arr[1]])
# row2 = np.hstack([arr[1],arr[1]])
# cv2.imshow("tree", np.vstack(row1, row2))
cv2.imshow("tree", row1)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/33:
# np.concatenate((arr[0],arr[1]))
row1 = np.vstack(([arr[1], np.zeros((382, 583)), arr[1]]))
# row2 = np.hstack([arr[1],arr[1]])
# cv2.imshow("tree", np.vstack(row1, row2))
cv2.imshow("tree", row1)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/34:
# np.concatenate((arr[0],arr[1]))
row1 = np.vstack([[arr[1]], [np.zeros((382, 583))], [arr[1]]])
# row2 = np.hstack([arr[1],arr[1]])
# cv2.imshow("tree", np.vstack(row1, row2))
cv2.imshow("tree", row1)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/35:
# np.concatenate((arr[0],arr[1]))
row1 = np.hstack([arr[1], np.zeros((382, 583)), arr[1]])
# row2 = np.hstack([arr[1],arr[1]])
# cv2.imshow("tree", np.vstack(row1, row2))
cv2.imshow("tree", row1)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/36:
# np.concatenate((arr[0],arr[1]))
row1 = np.hstack([arr[1], arr[1]])
# row2 = np.hstack([arr[1],arr[1]])
# cv2.imshow("tree", np.vstack(row1, row2))
cv2.imshow("tree", row1)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/37:
# np.concatenate((arr[0],arr[1]))
row1 = np.vstack([arr[1], arr[1]])
# row2 = np.hstack([arr[1],arr[1]])
# cv2.imshow("tree", np.vstack(row1, row2))
cv2.imshow("tree", row1)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/38:
# np.concatenate((arr[0],arr[1]))
row1 = np.vstack([arr[1]//3, arr[1]//3])
# row2 = np.hstack([arr[1],arr[1]])
# cv2.imshow("tree", np.vstack(row1, row2))
cv2.imshow("tree", row1)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/39:
# np.concatenate((arr[0],arr[1]))
row1 = np.vstack([arr[1]*3, arr[1]*3])
# row2 = np.hstack([arr[1],arr[1]])
# cv2.imshow("tree", np.vstack(row1, row2))
cv2.imshow("tree", row1)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/40:
# np.concatenate((arr[0],arr[1]))
row1 = np.vstack([arr[1]+3, arr[1]+3])
# row2 = np.hstack([arr[1],arr[1]])
# cv2.imshow("tree", np.vstack(row1, row2))
cv2.imshow("tree", row1)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/41:
# np.concatenate((arr[0],arr[1]))
row1 = np.vstack([arr[1]//3, arr[1]//3])
# row2 = np.hstack([arr[1],arr[1]])
# cv2.imshow("tree", np.vstack(row1, row2))
cv2.imshow("tree", row1)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/42:
# np.concatenate((arr[0],arr[1]))
row1 = np.vstack([arr[1], arr[1]])
# row2 = np.hstack([arr[1],arr[1]])
# cv2.imshow("tree", np.vstack(row1, row2))
cv2.imshow("tree", row1)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/43:
# np.concatenate((arr[0],arr[1]))
row1 = np.vstack([arr[1], arr[1]])
# row2 = np.hstack([arr[1],arr[1]])
# cv2.imshow("tree", np.vstack(row1, row2))
cv2.imwrite("tree.jpg", row1)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/44: max(shape)
299/45: max(shape//3)
299/46: max(shape)
299/47:

cv2.imwrite("tree.jpg", np.zeros((max(shape))))
cv2.waitKey(0)
cv2.destroyAllWindows()
299/48: min(shape)
299/49: max(shape)
299/50: max(shape[0])
299/51: max(shape[1])
299/52: max(shape)
299/53:

cv2.imshow("tree.jpg", np.zeros((max(shape))))
cv2.waitKey(0)
cv2.destroyAllWindows()
299/54: np.zeros((1,5))+s55
299/55: np.zeros((1,5))+255
299/56:

cv2.imshow("tree.jpg", np.zeros((max(shape))+255))
cv2.waitKey(0)
cv2.destroyAllWindows()
299/57: np.zeros((max(shape))+255)
299/58:

cv2.imshow("tree.jpg", np.zeros((max(shape)))+255)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/59:

cv2.imshow("tree.jpg", row1)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/60:

cv2.imwrite("tree.jpg", row1)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/61:

cv2.imwrite("tree1.jpg", row2)
cv2.waitKey(0)
cv2.destroyAllWindows()
299/62:
# np.concatenate((arr[0],arr[1]))
row1 = np.vstack([arr[1], arr[1]])
row2 = np.hstack([arr[0],arr[0]])
# cv2.imshow("tree", np.vstack(row1, row2))
cv2.imwrite("tree1.jpg", row2)
cv2.waitKey(0)
cv2.destroyAllWindows()
300/1:
import cv2
import matplotlib.pyplot as plt
300/2:
img = cv2.imread('frame.jpg')
plt.imshow(img)
300/3:
img = cv2.imread('frame.jpg',1)
plt.imshow(img)
300/4:
img = cv2.imread('frame.jpg',0)
plt.imshow(img)
300/5:
img = cv2.imread('frame.jpg',2)
plt.imshow(img)
300/6:
img = cv2.imread('frame.jpg',3)
plt.imshow(img)
300/7:
img = cv2.imread('frame.jpg',cv2.COLOR_BGR2RGB)
plt.imshow(img)
300/8:
img = cv2.imread('frame.jpg',cv2.COLOR_RGB2BGR)
plt.imshow(img)
300/9:
img = cv2.imread('frame.jpg')
img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)
plt.imshow(img)
300/10:
img = cv2.imread('frame.jpg')
img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
plt.imshow(img)
300/11:
img = cv2.imread('frame.jpg')
img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
img = cv2.medianBlur(img, 5)
plt.imshow(img)
300/12:
img = cv2.imread('frame.jpg')
img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
img = cv2.medianBlur(img, 25)
plt.imshow(img)
300/13:
img = cv2.imread('frame.jpg')
img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
img = cv2.medianBlur(img, 35)
plt.imshow(img)
300/14:
img = cv2.imread('frame.jpg')
img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
img = cv2.medianBlur(img, 55)
plt.imshow(img)
300/15:
img = cv2.imread('frame.jpg')
img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
img = cv2.medianBlur(img, 75)
plt.imshow(img)
300/16:
img = cv2.imread('frame.jpg')
img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
img = cv2.blur(img, (5,5))
plt.imshow(img)
300/17:
img = cv2.imread('frame.jpg')
img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
img = cv2.blur(img, (50,50))
plt.imshow(img)
300/18:
img = cv2.imread('frame.jpg')
img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
img = cv2.blur(img, (30,30))
plt.imshow(img)
300/19:
img = cv2.imread('frame.jpg')
img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
img = cv2.blur(img, (40,40))
plt.imshow(img)
300/20:
img = cv2.imread('frame.jpg')
img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
img = cv2.blur(img, (50,50))
plt.imshow(img)
300/21:
img = cv2.imread(r'C:\Users\Dell\Downloads\sceneries\00000004_(3).jpg')
img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
# img = cv2.medianBlur(img, 75)
plt.imshow(img)
300/22:
img = cv2.imread(r'C:\Users\Dell\Downloads\sceneries\00000004_(3).jpg')
img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
img = cv2.blur(img, (50,50))
plt.imshow(img)
300/23:
img = cv2.imread(r'C:\Users\Dell\Downloads\sceneries\00000004_(3).jpg')
img = cv2.cvtColor(img//3,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
img = cv2.blur(img, (50,50))
plt.imshow(img)
300/24:
img = cv2.imread(r'C:\Users\Dell\Downloads\sceneries\00000004_(3).jpg')
img = cv2.cvtColor(img//2,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
img = cv2.blur(img, (50,50))
plt.imshow(img)
300/25:
img = cv2.imread(r'C:\Users\Dell\Downloads\sceneries\00000004_(3).jpg')
img = cv2.cvtColor(img//2,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
img = cv2.blur(img, (40,40))
plt.imshow(img)
300/26:
img = cv2.imread(r'C:\Users\Dell\Downloads\sceneries\00000004_(3).jpg')
img = cv2.cvtColor(img//2,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
img = cv2.blur(img, (30,30))
plt.imshow(img)
300/27:
img = cv2.imread(r'C:\Users\Dell\Downloads\sceneries\00000003_(7).jpg')
img = cv2.cvtColor(img//2,cv2.COLOR_RGB2BGR)
img = cv2.resize(img, (1000,550))
img = cv2.blur(img, (30,30))
plt.imshow(img)
300/28:
from PIL import Image

# open images
img1 = Image.open(r"C:\Users\Dell\Downloads\sceneries\00000003_(7).jpg")
img2 = Image.open(r"C:\Users\Dell\Downloads\sceneries\00000002.jpg")

# determine the size of the new image
new_width = img1.width + img2.width
new_height = max(img1.height, img2.height)

# create a new image with the desired dimensions
merged_image = Image.new('RGB', (new_width, new_height))

# paste the images onto the new image
merged_image.paste(img1, (0, 0))
merged_image.paste(img2, (img1.width, 0))

# save the merged image
merged_image.save("merged_image.png")
301/1: os.listdir('Examples5')
301/2:
import os
os.listdir('Examples5')
301/3:
import os
'Examples5'+os.listdir('Examples5')
301/4:
from PIL import Image

lst = []
new_width, new_height = 0, 0

for f in os.listdir('Examples5'):
    lst.append('Examples5/'+f)


for i in lst:
    
    img1 = Image.open(i)
    # determine the size of the new image
    new_width = img1.width + new_width
    
    if img1.height > new_height:
        new_height = img1.height

    # create a new image with the desired dimensions
    merged_image = Image.new('RGB', (new_width, new_height))


    # paste the images onto the new image
    merged_image.paste(img2, (img1.width, 0))

# save the merged image
merged_image.save("merged_image.png")
301/5:
from PIL import Image

lst = []
new_width, new_height = 0, 0

for f in os.listdir('Examples5'):
    lst.append('Examples5/'+f)


for i in lst:
    
    img1 = Image.open(i)
    # determine the size of the new image
    new_width = img1.width + new_width
    
    if img1.height > new_height:
        new_height = img1.height

    # create a new image with the desired dimensions
    merged_image = Image.new('RGB', (new_width, new_height))


    # paste the images onto the new image
    merged_image.paste(img1, (img1.width, 0))

# save the merged image
merged_image.save("merged_image.png")
301/6:
from PIL import Image

lst = []
new_width, new_height = 0, 0

for f in os.listdir('Examples5'):
    lst.append('Examples5/'+f)


for i in lst:
    
    img1 = Image.open(i)
    # determine the size of the new image
    new_width = img1.width + new_width
    new_width2 = new_width - img1.width
    
    if img1.height > new_height:
        new_height = img1.height

    # create a new image with the desired dimensions
    merged_image = Image.new('RGB', (new_width, new_height))


    # paste the images onto the new image
    merged_image.paste(img1, (new_width2, 0))

# save the merged image
merged_image.save("merged_image.png")
301/7:
from PIL import Image

lst = []
new_width, new_height = 0, 0
img2 = np.zeros((255,255))

for f in os.listdir('Examples5'):
    lst.append('Examples5/'+f)


for i in lst:
    
    img1 = Image.open(i)
    # determine the size of the new image
    new_width = img1.width + new_width
    new_width2 = new_width - img1.width
    
    if img1.height > new_height:
        new_height = img1.height

    # create a new image with the desired dimensions
    merged_image = Image.new('RGB', (new_width, new_height))



    # paste the images onto the new image
    merged_image.paste(img2, (img1.width, 0))
    img2 = img1

# save the merged image
merged_image.save("merged_image.png")
301/8:
from PIL import Image
import numpy as np

lst = []
new_width, new_height = 0, 0
img2 = np.zeros((255,255))

for f in os.listdir('Examples5'):
    lst.append('Examples5/'+f)


for i in lst:
    
    img1 = Image.open(i)
    # determine the size of the new image
    new_width = img1.width + new_width
    new_width2 = new_width - img1.width
    
    if img1.height > new_height:
        new_height = img1.height

    # create a new image with the desired dimensions
    merged_image = Image.new('RGB', (new_width, new_height))



    # paste the images onto the new image
    merged_image.paste(img2, (img1.width, 0))
    img2 = img1

# save the merged image
merged_image.save("merged_image.png")
301/9:
from PIL import Image
import numpy as np

lst = []
new_width, new_height = 0, 0
img2 = Image.fromarray(np.zeros((255,255)))

for f in os.listdir('Examples5'):
    lst.append('Examples5/'+f)


for i in lst:
    
    img1 = Image.open(i)
    # determine the size of the new image
    new_width = img1.width + new_width
    new_width2 = new_width - img1.width
    
    if img1.height > new_height:
        new_height = img1.height

    # create a new image with the desired dimensions
    merged_image = Image.new('RGB', (new_width, new_height))



    # paste the images onto the new image
    merged_image.paste(img2, (img1.width, 0))
    img2 = img1

# save the merged image
merged_image.save("merged_image.png")
301/10:
from PIL import Image

# open images
img1 = Image.open(r"Examples5/pexels-photo-6762298.jpeg")
img2 = Image.open(r"Examples5/pexels-photo-1181422.jpeg")

# determine the size of the new image
new_width = img1.width + img2.width
new_height = max(img1.height, img2.height)

# create a new image with the desired dimensions
merged_image = Image.new('RGB', (new_width, new_height))

# paste the images onto the new image
merged_image.paste(img1, (0, 0))
merged_image.paste(img2, (img1.width, 0))

# save the merged image
merged_image.save("merged_image2.png")
301/11:
from PIL import Image
import numpy as np

lst = []
new_width, new_height = 0, 0
img2 = Image.fromarray(np.zeros((255,255)))

for f in os.listdir('Examples5'):
    lst.append('Examples5/'+f)


for i in lst:
    
    img1 = Image.open(i)
    # determine the size of the new image
    new_width = img1.width + new_width
    new_width2 = new_width - img1.width
    
    if img1.height > new_height:
        new_height = img1.height

    # create a new image with the desired dimensions
    merged_image = Image.new('RGB', (new_width, new_height))



    # paste the images onto the new image
    merged_image.paste(img2, (new_width, 0))
    img2 = img1

# save the merged image
merged_image.save("merged_image.png")
301/12:
from PIL import Image
import numpy as np

lst = []
new_width, new_height = 0, 0
img2 = Image.fromarray(np.zeros((255,255)))

for f in os.listdir('Examples5'):
    lst.append('Examples5/'+f)


for i in lst:
    
    img1 = Image.open(i)
    # determine the size of the new image
    new_width = img1.width + new_width
    new_width2 = new_width - img1.width
    
    if img1.height > new_height:
        new_height = img1.height

    # create a new image with the desired dimensions
    merged_image = Image.new('RGB', (new_width, new_height))



    # paste the images onto the new image
    merged_image.paste(img2, (new_width2, 0))
    img2 = img1

# save the merged image
merged_image.save("merged_image.png")
301/13:
from PIL import Image
import numpy as np

lst = []
new_width, new_height = 0, 0
img2 = Image.fromarray(np.zeros((255,255)))

for f in os.listdir('Examples5'):
    lst.append('Examples5/'+f)


for i in lst:
    
    img1 = Image.open(i)
    # determine the size of the new image
    new_width = img1.width + new_width
    new_width2 = new_width - img1.width
    
    if img1.height > new_height:
        new_height = img1.height

    # create a new image with the desired dimensions
    merged_image = Image.new('RGB', (new_width, new_height))



    # paste the images onto the new image
    merged_image.paste(img1, (new_width2, 0))
    img2 = img1

# save the merged image
merged_image.save("merged_image.png")
302/1:
with open('chat-number.txt') as f:
    print(f)
302/2:
with open('chat-number.txt') as f:
    print(f.read())
302/3:
f = open('chat-number.txt')
print(f.read())
302/4:
f = open('chat-number.txt')
print(type(f.read()))
302/5:
f = open('chat-number.txt')
print(type(int(f.read())))
302/6:
with open('chat-number.txt', 'rw') as f:
    print(f.read())
302/7:
with open('chat-number.txt', 'w') as f:
    print(f.read())
302/8:
with open('chat-number.txt', 'a') as f:
    print(f.read())
302/9:
with open('chat-number.txt', 'r') as f:
    print(f.read())
302/10:
f = open("chat-number.txt", "a")

f.read()
302/11:
f = open("chat-number.txt", "a")

print(f.read())
302/12:
f = open("chat-number.txt", "a+")

print(f.read())
302/13:
with open('chat-number.txt', 'r') as f:
    print(f.read())
302/14:
f = open("chat-number.txt", "a+")

print(f.read())
302/15:
with open('chat-number.txt', 'r') as f:
    print(f.read())
302/16:
f = open("chat-number.txt", "r+")

print(f.read())
302/17:
f = open("chat-number.txt", "r+")

print(f.read())

f.write(7)

print(f.read())
302/18:
f = open("chat-number.txt", "r+")

print(f.read())

f.write("7")

print(f.read())
302/19:
f = open("chat-number.txt", "r+")

print(f.read())

f.write("7")

print(f.read())
302/20:
f = open("chat-number.txt", "r+")

print(f.read())

f.write("7")

print(f.read())
302/21:
f = open("chat-number.txt", "r+")

print(f.read())

f.write("7")

print(f.readline())
302/22:
f = open("chat-number.txt", "r+")

# print(f.read())

# f.write("7")

print(f.read())
302/23:
f = open("chat-number.txt", "r+")

# print(f.read())

# f.write("7")

print(f.read())
302/24:
f = open("chat-number.txt", "r+")

print(f.read())

f.write("")
f.write("7")

print(f.read())
302/25: print(f.read())
302/26:
f = open("chat-number.txt", "r+r")

print(f.read())

f.write("")
f.write("7")

print(f.read())
302/27:
f = open("chat-number.txt", "w+")

print(f.read())

f.write("")
f.write("7")

print(f.read())
302/28:
f = open("chat-number.txt", "r")

print(f.read())

f.write("")
f.write("7")

print(f.read())
302/29:
f = open("chat-number.txt")

print(f.read())

f.write("")
f.write("7")

print(f.read())
302/30:
f = open("chat-number.txt", "w+")

print(f.read())

f.write("")
f.write("7")

print(f.read())
302/31:
f = open("chat-number.txt", "r+")

print(f.read())

f.write("")
f.write("7")

print(f.read())
302/32:
f = open("chat-number.txt", "r+")

print(f.read())

f.write("")
f.write("7")

return f.read()
302/33:
f = open("chat-number.txt", "r+")

print(f.read())

f.write("")
f.write("7")

print(f.read())
302/34: print(f.read())
302/35:
f = open("chat-number.txt", "r+")

print(f.read())

f.write("")
f.write("7")

print(f.read())
302/36:
f = open("chat-number.txt", "r+")

print(f.read())

f.truncate(0)
f.write("7")

print(f.read())
302/37:
f = open("chat-number.txt", "r+")

print(f.read())

f.truncate(1)
f.write("7")

print(f.read())
302/38:
f = open("chat-number.txt", "r+")

print(f.read())

f.truncate(1)
f.write("7")

print(f.read())
302/39: print(f.read())
302/40: print(f.read())
302/41: print(f.read())
302/42: print(f.readlines())
302/43:
f = open("chat-number.txt", "r")
g = open("chat-number.txt", "w")

print(f.read())

g.truncate(1)
g.write("7")

print(f.read())
302/44:
f = open("chat-number.txt", "r")
g = open("chat-number.txt", "w")

print(f.read())

g.truncate(1)
g.write("7")

print(f.read())
302/45:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

g.truncate(1)
g.write("7")

print(f.read())
302/46:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

f.truncate(1)
f.write("7")

print(f.read())
302/47:
f.truncate(1)
f.write("7")

print(f.read())
302/48:
f = open("chat-number.txt", "r+")
f.truncate(1)
f.write("7")

print(f.read())
302/49:
f = open("chat-number.txt", "w+")
f.truncate(1)
f.write("7")

print(f.read())
302/50:
f = open("chat-number.txt", "w+")
# f.truncate(1)
f.write("7")

print(f.read())
302/51:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())
302/52:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write("7")

print(g.read())
302/53:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write("7")

print(g.read())
302/54:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write("7")

print(g.read())
302/55:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write("7")

print(g.read())
302/56:
f = open("chat-number.txt", "r+")
current = int(f.read()) +1

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write(current)

print(g.read())
302/57:
f = open("chat-number.txt", "r+")
current = int(f.read()) +1

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write(str(current))

print(g.read())
302/58:
f = open("chat-number.txt", "r+")
current = int(str(f.read())) +1

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write(str(current))

print(g.read())
302/59:
f = open("chat-number.txt", "r+", )
current = float(f.read()) +1

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write(str(current))

print(g.read())
302/60:
f = open("chat-number.txt", "r+", )
current = f.read()

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write(str(current))

print(g.read())
302/61:
f = open("chat-number.txt", "r+", )
current = f.read()

print(current)

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write(str(current))

print(g.read())
302/62:
f = open("chat-number.txt", "r+", )
current = f.read()

print(current)

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write(str(current))

print(g.read())
302/63:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write("7")

print(g.read())
302/64:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write("7")

print(g.read())
302/65:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write("7")

print(g.read())
302/66:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write("7")

print(g.read())
302/67:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write("7")

print(f.read())
302/68:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write("7")

print(f.read())
302/69:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write("7")

print(f.read())
302/70:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write("7")

print(f.read())
302/71:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

g = open("chat-number.txt", "w+")
# f.truncate(1)
g.write("7")

print(f.read())
302/72:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
# f.truncate(1)
f.write("7")

print(f.read())
302/73:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
# f.truncate(1)
f.write("7")

# print(f.read())
302/74:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.truncate(1)
f.write("7")

# print(f.read())
302/75:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.truncate(1)
f.write("7")

# print(f.read())
302/76:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.truncate(1)
f.write("7")

# print(f.read())
302/77:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.truncate(1)
f.write("7")

# print(f.read())
302/78:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.truncate(2)
f.write("7")

# print(f.read())
302/79:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.truncate(2)
f.write("7")

# print(f.read())
302/80:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.truncate(0)
f.write("7")

# print(f.read())
302/81:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.truncate(1)
f.write("7")

# print(f.read())
302/82:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.truncate(1)
f.write("7")

print(f.read())
302/83:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.truncate(1)
f.write("7")

print(f.read())
302/84:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.truncate(1)
f.write("7")

print(f.read())
302/85:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.truncate(1)
f.write("7")

print(f.read())

f.close()
302/86:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.truncate(1)
f.write("7")

print(f.read())

f.close()
302/87:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.truncate(1)
f.write("7")

print(f.read())

f.close()
302/88:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.truncate(1)
f.write("7")

print(f.read())

f.close()
302/89:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write("7")

print(f.read())

f.close()
302/90:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write("7")

whatisit = f.read()
print(whatisit)

f.close()
302/91:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write("7")

whatisit = f.read()
print(whatisit)

f.close()
302/92:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write("7")

whatisit = f.read()
print(whatisit)

f.close()
302/93:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(str(f.read()))

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write("7")

whatisit = f.read()
print(whatisit)

f.close()
302/94:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(int(f.read()))

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write("7")

whatisit = f.read()
print(whatisit)

f.close()
302/95:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())
inc = str(int(f.read()) +1)

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(inc)

whatisit = f.read()
print(whatisit)

f.close()
302/96:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())
inc = str(int(f.read()))

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(inc)

whatisit = f.read()
print(whatisit)

f.close()
302/97:
f = open("chat-number.txt", "r+", encoding='utf-8')
# g = open("chat-number.txt", "w")

print(f.read())
inc = str(int(f.read()))

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(inc)

whatisit = f.read()
print(whatisit)

f.close()
302/98:
f = open("chat-number.txt", "r+", encoding='base 10')
# g = open("chat-number.txt", "w")

print(f.read())
inc = str(int(f.read()))

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(inc)

whatisit = f.read()
print(whatisit)

f.close()
302/99:
f = open("chat-number.txt", "r+", encoding='utf-8')
# g = open("chat-number.txt", "w")

print(f.read())
inc = str(int(f.read()))

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(inc)

whatisit = f.read()
print(whatisit)

f.close()
302/100:
f = open("chat-number.txt", "r+", encoding='utf-8')
# g = open("chat-number.txt", "w")

print(f.read())
inc = str(float(f.read()))

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(inc)

whatisit = f.read()
print(whatisit)

f.close()
302/101:
f = open("chat-number.txt", "r+", encoding='utf-8')
# g = open("chat-number.txt", "w")

print(f.read())
inc = str(float(f.read()))

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(inc)

whatisit = f.read()
print(whatisit)

f.close()
302/102:
f = open("chat-number.txt", "r+", encoding='utf-8')
# g = open("chat-number.txt", "w")

print(f.read())
inc = str(float(f.read()))

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(inc)

whatisit = f.read()
print(whatisit)

f.close()
302/103:
f = open("chat-number.txt", "r+", encoding='utf-8')
# g = open("chat-number.txt", "w")

print(f.read())
inc = str(float(f.read()))

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(inc)

whatisit = f.read()
print(whatisit)

f.close()
302/104:
f = open("chat-number.txt", "r+", encoding='utf-8')
# g = open("chat-number.txt", "w")

print(f.read())
inc = str(float(f.read()))

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(inc)

whatisit = f.read()
print(whatisit)

f.close()
302/105:
f = open("chat-number.txt", "r+", encoding='utf-8')
# g = open("chat-number.txt", "w")

print(f.read())
inc = float(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(inc)

whatisit = f.read()
print(whatisit)

f.close()
302/106:
f = open("chat-number.txt", "r+", encoding='utf-8')
# g = open("chat-number.txt", "w")

print(f.read())
# inc = float(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(inc)

whatisit = f.read()
print(whatisit)

f.close()
302/107:
f = open("chat-number.txt", "r+", encoding='utf-8')
# g = open("chat-number.txt", "w")

print(f.read())
# inc = float(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write("6")

whatisit = f.read()
print(whatisit)

f.close()
302/108:
f = open("chat-number.txt", "r+", encoding='utf-8')
# g = open("chat-number.txt", "w")

print(f.read())
# inc = float(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write("7")

whatisit = f.read()
print(whatisit)

f.close()
302/109: f.read()
302/110:
f = open("chat-number.txt", "r+", encoding='utf-8')
# g = open("chat-number.txt", "w")

print(f.read())
# inc = float(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write("6")

whatisit = f.read()
print(whatisit)

# f.close()
302/111:
f = open("chat-number.txt", "r+", encoding='utf-8')
# g = open("chat-number.txt", "w")

print(f.read())
# inc = float(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write("7")

whatisit = f.read()
print(whatisit)

# f.close()
302/112: f.read()
302/113: f.read()
302/114:
f = open("chat-number.txt", "r+")
# g = open("chat-number.txt", "w")

print(f.read())
# inc = float(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write("7")

whatisit = f.read()
print(whatisit)

# f.close()
302/115: f.read()
302/116: f.read()
302/117:
f = open("chat-number.txt", "r+")
g = open("chat-number.txt", "r+")

print(f.read())
# inc = float(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write("7")

# whatisit = f.read()
print(f.read())

# f.close()
302/118:
f = open("chat-number.txt", "r+")
g = open("chat-number.txt", "r+")

print(f.read())
# inc = float(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write("6")

# whatisit = f.read()
print(f.read())

# f.close()
302/119:
f = open("chat-number.txt", "r+")


print(f.read())
# inc = float(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write("7")
f.close()

g = open("chat-number.txt", "r+")
# whatisit = f.read()
print(g.read())
302/120:
f = open("chat-number.txt", "r+")


print(type(f.read()))
# inc = float(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write("7")
f.close()

g = open("chat-number.txt", "r+")
# whatisit = f.read()
print(g.read())
302/121: f.read()
302/122: g.read()
302/123:
f = open("chat-number.txt", "r+")


print(next(f))
# inc = float(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write("7")
f.close()

g = open("chat-number.txt", "r+")
# whatisit = f.read()
print(g.read())
302/124:
f = open("chat-number.txt", "r+")


print(int(next(f)))
# inc = float(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write("7")
f.close()

g = open("chat-number.txt", "r+")
# whatisit = f.read()
print(g.read())
302/125:
f = open("chat-number.txt", "r+")

inc = int(next(f)) +1
print(inc)
# inc = float(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(inc)
f.close()

g = open("chat-number.txt", "r+")
# whatisit = f.read()
print(g.read())
302/126:
f = open("chat-number.txt", "r+")

inc = int(next(f)) +1
print(inc)
# inc = float(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)

f.close()

g = open("chat-number.txt", "r+")
g.write(inc)
print(next(g))
302/127:
f = open("chat-number.txt", "r+")

inc = int(next(f)) +1
print(inc)
# inc = float(f.read())

# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(str(inc))
f.close()

g = open("chat-number.txt", "r+")
# whatisit = f.read()
print(g.read())
302/128:
f = open("chat-number.txt", "r+")

print(f.read())

inc = int(next(f)) +1


# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(str(inc))
f.close()

g = open("chat-number.txt", "r+")
# whatisit = f.read()
print(g.read())
302/129:
f = open("chat-number.txt", "r+")


inc = int(next(f)) +1
print(f.read())


# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(str(inc))
f.close()

g = open("chat-number.txt", "r+")
# whatisit = f.read()
print(g.read())
302/130:
f = open("chat-number.txt", "r+")


inc = int(next(f)) +1
print(next(f))


# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(str(inc))
f.close()

g = open("chat-number.txt", "r+")
# whatisit = f.read()
print(g.read())
302/131:
f = open("chat-number.txt", "r+")


inc = int(next(f)) +1
print(f)


# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(str(inc))
f.close()

g = open("chat-number.txt", "r+")
# whatisit = f.read()
print(g.read())
302/132:
f = open("chat-number.txt", "r+")


inc = int(next(f)) +1
print(f[-1])


# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(str(inc))
f.close()

g = open("chat-number.txt", "r+")
# whatisit = f.read()
print(g.read())
302/133:
f = open("chat-number.txt", "r+")


inc = int(next(f)) +1
print(str(f)[-1])


# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(str(inc))
f.close()

g = open("chat-number.txt", "r+")
# whatisit = f.read()
print(g.read())
302/134:
f = open("chat-number.txt", "r+")


inc = int(next(f)) +1
print(next(f))


# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(str(inc))
f.close()

g = open("chat-number.txt", "r+")
# whatisit = f.read()
print(g.read())
302/135:
f = open("chat-number.txt", "r+")


inc = int(next(f)) +1
print((f.read()))


# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(str(inc))
f.close()

g = open("chat-number.txt", "r+")
# whatisit = f.read()
print(g.read())
302/136:
f = open("chat-number.txt", "r+")


inc = int(next(f)) +1
print((f.read()))


# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(str(inc))
f.close()

g = open("chat-number.txt", "r+")
# whatisit = f.read()
print(g.read())
302/137: g.read()
302/138:
f = open("chat-number.txt", "r+")


inc = int(next(f)) +1
print((f.read()))


# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(str(inc))
f.close()

g = open("chat-number.txt", "r+")
# whatisit = f.read()
print(g.read())
302/139:
f = open("chat-number.txt", "r+")


inc = int(next(f)) +1
print(inc)


# g = open("chat-number.txt", "w+")
f.seek(0)
f.write(str(inc))
f.close()

g = open("chat-number.txt", "r+")
# whatisit = f.read()
print(g.read())
303/1:
from PIL import Image

# Define the images you want to use in the collage
image_paths = []
for f in os.listdir('Examples5'):
    image_paths.append('Examples5/'+f)

# Open each image and store it in a list
images = []
for path in image_paths:
    image = Image.open(path)
    images.append(image)

# Calculate the total width and height of the collage
total_width = 0
max_height = 0
for image in images:
    width, height = image.size
    total_width += width
    max_height = max(max_height, height)

# Create a new blank image for the collage
collage = Image.new('RGB', (total_width, max_height), (255, 255, 255))

# Paste each image into the collage
x_offset = 0
for image in images:
    width, height = image.size
    collage.paste(image, (x_offset, 0))
    x_offset += width

# Save the collage
collage.save('collage.jpg')
303/2:
from PIL import Image
import os

# Define the images you want to use in the collage
image_paths = []
for f in os.listdir('Examples5'):
    image_paths.append('Examples5/'+f)

# Open each image and store it in a list
images = []
for path in image_paths:
    image = Image.open(path)
    images.append(image)

# Calculate the total width and height of the collage
total_width = 0
max_height = 0
for image in images:
    width, height = image.size
    total_width += width
    max_height = max(max_height, height)

# Create a new blank image for the collage
collage = Image.new('RGB', (total_width, max_height), (255, 255, 255))

# Paste each image into the collage
x_offset = 0
for image in images:
    width, height = image.size
    collage.paste(image, (x_offset, 0))
    x_offset += width

# Save the collage
collage.save('collage.jpg')
303/3: type(collage)
303/4:
import cv2
cv2.imwrite('saved.png', cv2.imread(collage))
303/5:
import cv2
import numpy as np
cv2.imwrite('saved.png', np.array(collage))
303/6:
from PIL import Image
import os

# Define the images you want to use in the collage
image_paths = []
for f in os.listdir('Examples5'):
    image_paths.append('Examples5/'+f)

# Open each image and store it in a list
images = []
for path in image_paths:
    image = Image.open(path).convert('RGB')
    images.append(image)

# Calculate the total width and height of the collage
total_width = 0
max_height = 0
for image in images:
    width, height = image.size
    total_width += width
    max_height = max(max_height, height)

# Create a new blank image for the collage
collage = Image.new('RGB', (total_width, max_height), (255, 255, 255))

# Paste each image into the collage
x_offset = 0
for image in images:
    width, height = image.size
    collage.paste(image, (x_offset, 0))
    x_offset += width

# Save the collage
# collage.save('collage.jpg')
303/7:
import cv2
import numpy as np
cv2.imwrite('saved.png', np.array(collage))
303/8:
from PIL import Image
import os

# Define the images you want to use in the collage
image_paths = []
for f in os.listdir('Examples5'):
    image_paths.append('Examples5/'+f)

# Open each image and store it in a list
images = []
for path in image_paths:
    image = Image.open(path).convert('BGR')
    images.append(image)

# Calculate the total width and height of the collage
total_width = 0
max_height = 0
for image in images:
    width, height = image.size
    total_width += width
    max_height = max(max_height, height)

# Create a new blank image for the collage
collage = Image.new('BGR', (total_width, max_height), (255, 255, 255))

# Paste each image into the collage
x_offset = 0
for image in images:
    width, height = image.size
    collage.paste(image, (x_offset, 0))
    x_offset += width

# Save the collage
# collage.save('collage.jpg')
303/9:
from PIL import Image
import os

# Define the images you want to use in the collage
image_paths = []
for f in os.listdir('Examples5'):
    image_paths.append('Examples5/'+f)

# Open each image and store it in a list
images = []
for path in image_paths:
    image = Image.open(path).convert('BGR')
    images.append(image)

# Calculate the total width and height of the collage
total_width = 0
max_height = 0
for image in images:
    width, height = image.size
    total_width += width
    max_height = max(max_height, height)

# Create a new blank image for the collage
collage = Image.new('RGB', (total_width, max_height), (255, 255, 255))

# Paste each image into the collage
x_offset = 0
for image in images:
    width, height = image.size
    collage.paste(image, (x_offset, 0))
    x_offset += width

# Save the collage
# collage.save('collage.jpg')
303/10:
from PIL import Image
import os

# Define the images you want to use in the collage
image_paths = []
for f in os.listdir('Examples5'):
    image_paths.append('Examples5/'+f)

# Open each image and store it in a list
images = []
for path in image_paths:
    image = Image.open(path).convert('RGB')
    images.append(image)

# Calculate the total width and height of the collage
total_width = 0
max_height = 0
for image in images:
    width, height = image.size
    total_width += width
    max_height = max(max_height, height)

# Create a new blank image for the collage
collage = Image.new('RGB', (total_width, max_height), (255, 255, 255))

# Paste each image into the collage
x_offset = 0
for image in images:
    width, height = image.size
    collage.paste(image, (x_offset, 0))
    x_offset += width

# Save the collage
# collage.save('collage.jpg')
303/11:
import cv2
import numpy as np
cv2.imwrite('saved.png', np.array(collage))
303/12:
from PIL import Image
import os

# Define the images you want to use in the collage
image_paths = []
for f in os.listdir('Examples5'):
    image_paths.append('Examples5/'+f)

# Open each image and store it in a list
images = []
for path in image_paths:
    image = Image.open(path).convert('RGB')
    images.append(image)

# Calculate the total width and height of the collage
total_width = 0
max_height = 0
for image in images:
    width, height = image.size
    total_width += width
    max_height = max(max_height, height)

# Create a new blank image for the collage
collage = Image.new((total_width, max_height), (255, 255, 255))

# Paste each image into the collage
x_offset = 0
for image in images:
    width, height = image.size
    collage.paste(image, (x_offset, 0))
    x_offset += width

# Save the collage
# collage.save('collage.jpg')
303/13:
from PIL import Image
import os

# Define the images you want to use in the collage
image_paths = []
for f in os.listdir('Examples5'):
    image_paths.append('Examples5/'+f)

# Open each image and store it in a list
images = []
for path in image_paths:
    image = Image.open(path).convert('BGR')
    images.append(image)

# Calculate the total width and height of the collage
total_width = 0
max_height = 0
for image in images:
    width, height = image.size
    total_width += width
    max_height = max(max_height, height)

# Create a new blank image for the collage
collage = Image.new('BGR', (total_width, max_height), (255, 255, 255))

# Paste each image into the collage
x_offset = 0
for image in images:
    width, height = image.size
    collage.paste(image, (x_offset, 0))
    x_offset += width

# Save the collage
# collage.save('collage.jpg')
303/14:
from PIL import Image
import os

# Define the images you want to use in the collage
image_paths = []
for f in os.listdir('Examples5'):
    image_paths.append('Examples5/'+f)

# Open each image and store it in a list
images = []
for path in image_paths:
    image = Image.open(path) #.convert('BGR')
    images.append(image)

# Calculate the total width and height of the collage
total_width = 0
max_height = 0
for image in images:
    width, height = image.size
    total_width += width
    max_height = max(max_height, height)

# Create a new blank image for the collage
collage = Image.new('BGR', (total_width, max_height), (255, 255, 255))

# Paste each image into the collage
x_offset = 0
for image in images:
    width, height = image.size
    collage.paste(image, (x_offset, 0))
    x_offset += width

# Save the collage
# collage.save('collage.jpg')
303/15:
from PIL import Image
import os

# Define the images you want to use in the collage
image_paths = []
for f in os.listdir('Examples5'):
    image_paths.append('Examples5/'+f)

# Open each image and store it in a list
images = []
for path in image_paths:
    image = Image.open(path) #.convert('BGR')
    images.append(image)

# Calculate the total width and height of the collage
total_width = 0
max_height = 0
for image in images:
    width, height = image.size
    total_width += width
    max_height = max(max_height, height)

# Create a new blank image for the collage
collage = Image.new('RGB', (total_width, max_height), (255, 255, 255))

# Paste each image into the collage
x_offset = 0
for image in images:
    width, height = image.size
    collage.paste(image, (x_offset, 0))
    x_offset += width

# Save the collage
# collage.save('collage.jpg')
303/16:
from PIL import Image
import os

# Define the images you want to use in the collage
image_paths = []
for f in os.listdir('Examples5'):
    image_paths.append('Examples5/'+f)

# Open each image and store it in a list
images = []
for path in image_paths:
    image = Image.open(path) #.convert('BGR')
    images.append(image[:,:,::-1])

# Calculate the total width and height of the collage
total_width = 0
max_height = 0
for image in images:
    width, height = image.size
    total_width += width
    max_height = max(max_height, height)

# Create a new blank image for the collage
collage = Image.new('RGB', (total_width, max_height), (255, 255, 255))

# Paste each image into the collage
x_offset = 0
for image in images:
    width, height = image.size
    collage.paste(image, (x_offset, 0))
    x_offset += width

# Save the collage
# collage.save('collage.jpg')
303/17:
from PIL import Image
import os

# Define the images you want to use in the collage
image_paths = []
for f in os.listdir('Examples5'):
    image_paths.append('Examples5/'+f)

# Open each image and store it in a list
images = []
for path in image_paths:
    image = Image.open(path)[:,:,::-1] #.convert('BGR')
    images.append(image)

# Calculate the total width and height of the collage
total_width = 0
max_height = 0
for image in images:
    width, height = image.size
    total_width += width
    max_height = max(max_height, height)

# Create a new blank image for the collage
collage = Image.new('RGB', (total_width, max_height), (255, 255, 255))

# Paste each image into the collage
x_offset = 0
for image in images:
    width, height = image.size
    collage.paste(image, (x_offset, 0))
    x_offset += width

# Save the collage
# collage.save('collage.jpg')
303/18:
from PIL import Image
import os

# Define the images you want to use in the collage
image_paths = []
for f in os.listdir('Examples5'):
    image_paths.append('Examples5/'+f)

# Open each image and store it in a list
images = []
for path in image_paths:
    image = Image.open(path) #.convert('BGR')
    b, g, r = image.split()
    im = Image.merge("RGB", (r, g, b))
    images.append(im)

# Calculate the total width and height of the collage
total_width = 0
max_height = 0
for image in images:
    width, height = image.size
    total_width += width
    max_height = max(max_height, height)

# Create a new blank image for the collage
collage = Image.new('RGB', (total_width, max_height), (255, 255, 255))

# Paste each image into the collage
x_offset = 0
for image in images:
    width, height = image.size
    collage.paste(image, (x_offset, 0))
    x_offset += width

# Save the collage
# collage.save('collage.jpg')
303/19:
import cv2
import numpy as np
cv2.imwrite('saved.png', np.array(collage))
304/1:
import sys, os
from PyQt5.QtWidgets import QApplication, QWidget, QLabel, QVBoxLayout
from PyQt5.QtCore import Qt
from PyQt5.QtGui import QPixmap

class ImageLabel(QLabel):
    def __init__(self):
        super().__init__()

        self.setAlignment(Qt.AlignCenter)
        self.setText('\n\n Drop Image Here \n\n')
        self.setStyleSheet('''
            QLabel{
                border: 4px dashed #aaa
        }
        ''')

    def setPixmap(self, image):
        super().setPixmap(image)
        
class AppDemo(QWidget):
    def __init__(self):
        super().__init__()
        self.resize(400, 400)
        self.setAcceptDrops(True)
        
        mainLayout = QVBoxLayout()

    self.photoViewer = ImageLabel()
    mainLayout.addWidget(self.photoViewer)

        self.setLayout(mainLayout)

    def dragEnterEvent(self, event):
    if event.mimeData().hasImage:
        event.accept()
    else:
        event.ignore()

    def dragMoveEvent(self, event):
    if event.mimeData().hasImage:
        event.accept()
    else:
        event.ignore()

    def dropEvent(self, event):
    if event.mimeData().hasImage:
        event.setDropAction(Qt.CopyAction)
        file_path = event.mimeData().urls()[0].toLocalFile()
        self.set_image(file_path)

        event.accept()
    else:
        event.ignore()

    def set_image(self, file_path):
        self.photoViewer.setPixmap(QPixmap(file_path))



app = QApplication(sys.argv)
demo = AppDemo()
demo.show()
sys.exit(app.exec_())
304/2:
import sys, os
from PyQt5.QtWidgets import QApplication, QWidget, QLabel, QVBoxLayout
from PyQt5.QtCore import Qt
from PyQt5.QtGui import QPixmap

class ImageLabel(QLabel):
    def __init__(self):
        super().__init__()

        self.setAlignment(Qt.AlignCenter)
        self.setText('\n\n Drop Image Here \n\n')
        self.setStyleSheet('''
            QLabel{
                border: 4px dashed #aaa
        }
        ''')

    def setPixmap(self, image):
        super().setPixmap(image)
        
class AppDemo(QWidget):
    def __init__(self):
        super().__init__()
        self.resize(400, 400)
        self.setAcceptDrops(True)
        
        mainLayout = QVBoxLayout()

    self.photoViewer = ImageLabel()
    mainLayout.addWidget(self.photoViewer)

        self.setLayout(mainLayout)

    def dragEnterEvent(self, event):
    if event.mimeData().hasImage:
        event.accept()
    else:
        event.ignore()

    def dragMoveEvent(self, event):
    if event.mimeData().hasImage:
        event.accept()
    else:
        event.ignore()

    def dropEvent(self, event):
        if event.mimeData().hasImage:
            event.setDropAction(Qt.CopyAction)
            file_path = event.mimeData().urls()[0].toLocalFile()
            self.set_image(file_path)

            event.accept()
        else:
            event.ignore()

    def set_image(self, file_path):
        self.photoViewer.setPixmap(QPixmap(file_path))



app = QApplication(sys.argv)
demo = AppDemo()
demo.show()
sys.exit(app.exec_())
304/3:
import sys, os
from PyQt5.QtWidgets import QApplication, QWidget, QLabel, QVBoxLayout
from PyQt5.QtCore import Qt
from PyQt5.QtGui import QPixmap

class ImageLabel(QLabel):
    def __init__(self):
        super().__init__()

        self.setAlignment(Qt.AlignCenter)
        self.setText('\n\n Drop Image Here \n\n')
        self.setStyleSheet('''
            QLabel{
                border: 4px dashed #aaa
        }
        ''')

    def setPixmap(self, image):
        super().setPixmap(image)
        
class AppDemo(QWidget):
    def __init__(self):
        super().__init__()
        self.resize(400, 400)
        self.setAcceptDrops(True)
        
        mainLayout = QVBoxLayout()

        self.photoViewer = ImageLabel()
        mainLayout.addWidget(self.photoViewer)

        self.setLayout(mainLayout)

    def dragEnterEvent(self, event):
        if event.mimeData().hasImage:
            event.accept()
        else:
            event.ignore()

    def dragMoveEvent(self, event):
        if event.mimeData().hasImage:
            event.accept()
        else:
            event.ignore()

    def dropEvent(self, event):
        if event.mimeData().hasImage:
            event.setDropAction(Qt.CopyAction)
            file_path = event.mimeData().urls()[0].toLocalFile()
            self.set_image(file_path)

            event.accept()
        else:
            event.ignore()

    def set_image(self, file_path):
        self.photoViewer.setPixmap(QPixmap(file_path))



app = QApplication(sys.argv)
demo = AppDemo()
demo.show()
sys.exit(app.exec_())
304/4:
import sys, os
from PyQt5.QtWidgets import QApplication, QWidget, QLabel, QVBoxLayout
from PyQt5.QtCore import Qt
from PyQt5.QtGui import QPixmap

class ImageLabel(QLabel):
    def __init__(self):
        super().__init__()

        self.setAlignment(Qt.AlignCenter)
        self.setText('\n\n Drop Image Here \n\n')
        self.setStyleSheet('''
            QLabel{
                border: 4px dashed #aaa
        }
        ''')

    def setPixmap(self, image):
        super().setPixmap(image)
        
class AppDemo(QWidget):
    def __init__(self):
        super().__init__()
        self.resize(400, 400)
        self.setAcceptDrops(True)
        
        mainLayout = QVBoxLayout()

        self.photoViewer = ImageLabel()
        mainLayout.addWidget(self.photoViewer)

        self.setLayout(mainLayout)

    def dragEnterEvent(self, event):
        if event.mimeData().hasImage:
            event.accept()
        else:
            event.ignore()

    def dragMoveEvent(self, event):
        if event.mimeData().hasImage:
            event.accept()
        else:
            event.ignore()

    def dropEvent(self, event):
        if event.mimeData().hasImage:
            event.setDropAction(Qt.CopyAction)
            file_path = event.mimeData().urls()[0].toLocalFile()
            self.set_image(file_path)

            event.accept()
        else:
            event.ignore()

    def set_image(self, file_path):
        self.photoViewer.setPixmap(QPixmap(file_path))



app = QApplication(sys.argv)
demo = AppDemo()
demo.show()
sys.exit(app.exec_())
306/1:
import sys, os
from PyQt5.QtWidgets import QApplication, QWidget, QLabel, QVBoxLayout
from PyQt5.QtCore import Qt
from PyQt5.QtGui import QPixmap

class ImageLabel(QLabel):
    def __init__(self):
        super().__init__()

        self.setAlignment(Qt.AlignCenter)
        self.setText('\n\n Drop Image Here \n\n')
        self.setStyleSheet('''
            QLabel{
                border: 4px dashed #aaa
        }
        ''')

    def setPixmap(self, image):
        super().setPixmap(image)
        
class AppDemo(QWidget):
    def __init__(self):
        super().__init__()
        self.resize(400, 400)
        self.setAcceptDrops(True)
        
        mainLayout = QVBoxLayout()

        self.photoViewer = ImageLabel()
        mainLayout.addWidget(self.photoViewer)

        self.setLayout(mainLayout)

    def dragEnterEvent(self, event):
        if event.mimeData().hasImage:
            event.accept()
        else:
            event.ignore()

    def dragMoveEvent(self, event):
        if event.mimeData().hasImage:
            event.accept()
        else:
            event.ignore()

    def dropEvent(self, event):
        if event.mimeData().hasImage:
            event.setDropAction(Qt.CopyAction)
            file_path = event.mimeData().urls()[0].toLocalFile()
            self.set_image(file_path)

            event.accept()
        else:
            event.ignore()

    def set_image(self, file_path):
        self.photoViewer.setPixmap(QPixmap(file_path))



app = QApplication(sys.argv)
demo = AppDemo()
demo.show()
sys.exit(app.exec_())
306/2:
import sys, os
from PyQt5.QtWidgets import QApplication, QWidget, QLabel, QVBoxLayout
from PyQt5.QtCore import Qt
from PyQt5.QtGui import QPixmap

class ImageLabel(QLabel):
    def __init__(self):
        super().__init__()

        self.setAlignment(Qt.AlignCenter)
        self.setText('\n\n Drop Image Here \n\n')
        self.setStyleSheet('''
            QLabel{
                border: 4px dashed #aaa
        }
        ''')

    def setPixmap(self, image):
        super().setPixmap(image)
        
class AppDemo(QWidget):
    def __init__(self):
        super().__init__()
        self.resize(400, 400)
        self.setAcceptDrops(True)
        
        mainLayout = QVBoxLayout()

        self.photoViewer = ImageLabel()
        mainLayout.addWidget(self.photoViewer)

        self.setLayout(mainLayout)

    def dragEnterEvent(self, event):
        if event.mimeData().hasImage:
            event.accept()
        else:
            event.ignore()

    def dragMoveEvent(self, event):
        if event.mimeData().hasImage:
            event.accept()
        else:
            event.ignore()

    def dropEvent(self, event):
        if event.mimeData().hasImage:
            event.setDropAction(Qt.CopyAction)
            file_path = event.mimeData().urls()[0].toLocalFile()
            self.set_image(file_path)

            event.accept()
        else:
            event.ignore()

    def set_image(self, file_path):
        self.photoViewer.setPixmap(QPixmap(file_path))



app = QApplication(sys.argv)
demo = AppDemo()
demo.show()
sys.exit(app.exec_())
308/1:
import sys, os
from PyQt5.QtWidgets import QApplication, QWidget, QLabel, QVBoxLayout
from PyQt5.QtCore import Qt
from PyQt5.QtGui import QPixmap

class ImageLabel(QLabel):
    def __init__(self):
        super().__init__()

        self.setAlignment(Qt.AlignCenter)
        self.setText('\n\n Drop Image Here \n\n')
        self.setStyleSheet('''
            QLabel{
                border: 4px dashed #aaa
        }
        ''')

    def setPixmap(self, image):
        super().setPixmap(image)
        
class AppDemo(QWidget):
    def __init__(self):
        super().__init__()
        self.resize(400, 400)
        self.setAcceptDrops(True)
        
        mainLayout = QVBoxLayout()

        self.photoViewer = ImageLabel()
        mainLayout.addWidget(self.photoViewer)

        self.setLayout(mainLayout)

    def dragEnterEvent(self, event):
        if event.mimeData().hasImage:
            event.accept()
        else:
            event.ignore()

    def dragMoveEvent(self, event):
        if event.mimeData().hasImage:
            event.accept()
        else:
            event.ignore()

    def dropEvent(self, event):
        if event.mimeData().hasImage:
            event.setDropAction(Qt.CopyAction)
            file_path = event.mimeData().urls()[0].toLocalFile()
            self.set_image(file_path)

            event.accept()
        else:
            event.ignore()

    def set_image(self, file_path):
        self.photoViewer.setPixmap(QPixmap(file_path))



app = QApplication(sys.argv)
demo = AppDemo()
demo.show()
sys.exit(app.exec_())
309/1:
import cv2
import numpy as np

img = np.zeros((700, 1350, 3), dtype=np. uint8) #background 600 900

cv2.rectangle(img, (0,0), (1350,600), (255,225,85), -1) 
cv2.rectangle(img, (0,600), (1350,700), (75,180,70), -1)
#sun
cv2.circle(img, (250,250), 60, (0,255,255), -1) 
cv2.circle(img, (250,250), 60, (100,255,255), 10)
cv2.imshow("tree", img)

cv2.waitKey(0)
cv2.destroyAllWindows()
309/2:
import cv2
import numpy as np

bg = np.zeros((700, 1350, 3), dtype=np. uint8) #background 600 900

img = cv2.imread('Examples5/pexels-photo-415351.jpeg')


cv2.imshow("tree", bg)

cv2.waitKey(0)
cv2.destroyAllWindows()
309/3:
import cv2
import numpy as np

bg = np.zeros((700, 1350, 3)*255, dtype=np. uint8) #background 600 900

img = cv2.imread('Examples5/pexels-photo-415351.jpeg')


cv2.imshow("tree", bg)

cv2.waitKey(0)
cv2.destroyAllWindows()
309/4:
import cv2
import numpy as np

bg = np.zeros((700, 1350, 3), dtype=np. uint8)*255 #background 600 900

img = cv2.imread('Examples5/pexels-photo-415351.jpeg')


cv2.imshow("tree", bg)

cv2.waitKey(0)
cv2.destroyAllWindows()
309/5:
import cv2
import numpy as np

bg = np.zeros((700, 1350, 3), dtype=np. uint8)+255 #background 600 900

img = cv2.imread('Examples5/pexels-photo-415351.jpeg')


cv2.imshow("tree", bg)

cv2.waitKey(0)
cv2.destroyAllWindows()
309/6:
import cv2
import numpy as np

bg = np.zeros((700, 1350, 3), dtype=np. uint8)+255 #background 600 900

img = cv2.imread('Examples5/pexels-photo-415351.jpeg')

res = np.union1d(bg, img)

cv2.imshow("tree", res)

cv2.waitKey(0)
cv2.destroyAllWindows()
309/7:
import cv2
import numpy as np

bg = np.zeros((700, 1350, 3), dtype=np. uint8)+255 #background 600 900

img = cv2.imread('Examples5/pexels-photo-415351.jpeg')

res = np.intersect1d(bg, img)

cv2.imshow("tree", res)

cv2.waitKey(0)
cv2.destroyAllWindows()
309/8:
def Reformat_Image(ImageFilePath):

    from PIL import Image
    image = Image.open(ImageFilePath, 'r')
    image_size = image.size
    width = image_size[0]
    height = image_size[1]

    if(width != height):
        bigside = width if width > height else height

        background = Image.new('RGBA', (bigside, bigside), (255, 255, 255, 255))
        offset = (int(round(((bigside - width) / 2), 0)), int(round(((bigside - height) / 2),0)))

        background.paste(image, offset)
        return background

    else:
        print("Image is already a square, it has not been resized !")
        
img = np.array(Reformat_Image('Examples5/pexels-photo-415351.jpeg'))
cv2.imshow("tree", img)

cv2.waitKey(0)
cv2.destroyAllWindows()
309/9:
def Reformat_Image(ImageFilePath):

    from PIL import Image
    image = Image.open(ImageFilePath, 'r')
    b, g, r = image.split()
    image = Image.merge("RGB", (r, g, b))
    image_size = image.size
    width = image_size[0]
    height = image_size[1]

    if(width != height):
        bigside = width if width > height else height

        background = Image.new('RGBA', (bigside, bigside), (255, 255, 255, 255))
        offset = (int(round(((bigside - width) / 2), 0)), int(round(((bigside - height) / 2),0)))

        background.paste(image, offset)
        return background

    else:
        print("Image is already a square, it has not been resized !")
        
img = np.array(Reformat_Image('Examples5/pexels-photo-415351.jpeg'))
cv2.imshow("tree", img)

cv2.waitKey(0)
cv2.destroyAllWindows()
309/10:
def Reformat_Image(ImageFilePath):

    from PIL import Image
    image = Image.open(ImageFilePath, 'r')
    b, g, r = image.split()
    image = Image.merge("RGB", (r, g, b))
    image_size = image.size
    width = image_size[0]
    height = image_size[1]

    if(width != height):
        bigside = width if width > height else height

        background = Image.new('RGBA', (bigside, bigside), (255, 255, 255, 255))
        offset = (int(round(((bigside - width) / 2), 0)), int(round(((bigside - height) / 2),0)))

        background.paste(image, offset)
        return background

    else:
        print("Image is already a square, it has not been resized !")

        
# lst = []
# for f in os.listdir('Examples5'):
#     lst.append('Examples5/'+f)
    
img = np.array(Reformat_Image('Examples5/pexels-photo-984923.jpeg'))
cv2.imshow("tree", img)

cv2.waitKey(0)
cv2.destroyAllWindows()
309/11:
lst = []
for f in os.listdir('Examples5'):
    lst.append('Examples5/'+f)
309/12:
import os
lst = []
for f in os.listdir('Examples5'):
    lst.append('Examples5/'+f)
309/13: max(lst)
309/14:
def Reformat_Image(ImageFilePath):

    from PIL import Image
    image = Image.open(ImageFilePath, 'r')
    b, g, r = image.split()
    image = Image.merge("RGB", (r, g, b))
    image_size = image.size
    width = image_size[0]
    height = image_size[1]

    if(width != height):
        bigside = width if width > height else height

        background = Image.new('RGBA', (bigside, bigside), (255, 255, 255, 255))
        offset = (int(round(((bigside - width) / 2), 0)), int(round(((bigside - height) / 2),0)))

        background.paste(image, offset)
        return background

    else:
        print("Image is already a square, it has not been resized !")

        
# lst = []
# for f in os.listdir('Examples5'):
#     lst.append('Examples5/'+f)
    
img = np.array(Reformat_Image('Examples5/pexels-photo-984950.jpeg'))
cv2.imshow("tree", img)

cv2.waitKey(0)
cv2.destroyAllWindows()
309/15: min(lst)
309/16:
def Reformat_Image(ImageFilePath):

    from PIL import Image
    image = Image.open(ImageFilePath, 'r')
    b, g, r = image.split()
    image = Image.merge("RGB", (r, g, b))
    image_size = image.size
    width = image_size[0]
    height = image_size[1]

    if(width != height):
        bigside = width if width > height else height

        background = Image.new('RGBA', (bigside, bigside), (255, 255, 255, 255))
        offset = (int(round(((bigside - width) / 2), 0)), int(round(((bigside - height) / 2),0)))

        background.paste(image, offset)
        return background

    else:
        print("Image is already a square, it has not been resized !")

        
# lst = []
# for f in os.listdir('Examples5'):
#     lst.append('Examples5/'+f)
    
img = np.array(Reformat_Image('Examples5/istockphoto-1272744431-612x612.jpg'))
cv2.imshow("tree", img)

cv2.waitKey(0)
cv2.destroyAllWindows()
309/17:
shape = []
for i in os.listdir('Examples5'):
    img = cv2.imread('Examples5/'+i)
    arr.append(img)
    shape.append(img.shape[:2])
309/18:
lst.clear()
lst
309/19:
import os
lst = []
for f in os.listdir('Examples5'):
    lst.append('Examples5/'+f)
309/20:
arr = []
shape = []
for i in os.listdir('Examples5'):
    img = cv2.imread('Examples5/'+i)
    arr.append(img)
    shape.append(img.shape[:2])
309/21: shape[0]
309/22: max(shape)
309/23:
for i in shape:
    if i[0] > 2351:
        print(i)
309/24:
for i in shape:
    if i[0] > 2350:
        print(i)
309/25:
for i in shape:
    if i[1] > 1540:
        print(i)
309/26:
from PIL import Image, ImageDraw
import random

# Define the dimensions of the final collage
canvas_width = 1000
canvas_height = 1000

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
shapes = [
    ((0, 0), (500, 500)),
    ((500, 0), (1000, 500)),
    ((0, 500), (500, 1000)),
    ((500, 500), (1000, 1000))
]

# Shuffle the shapes to add some randomness to the collage
random.shuffle(shapes)

# Load the images to be used in the collage
images = []
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    images.append(img)

# Paste the images onto the canvas
for i in range(len(images)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('collage.jpg')
309/27:
from PIL import Image, ImageDraw
import random

# Define the dimensions of the final collage
canvas_width = 1000
canvas_height = 1000

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
shapes = [
    ((0, 0), (500, 500)),
    ((500, 0), (1000, 500)),
    ((0, 500), (500, 1000)),
    ((500, 500), (1000, 1000))
]

# Shuffle the shapes to add some randomness to the collage
random.shuffle(shapes)

# Load the images to be used in the collage
# images = []
# for i in os.listdir('Examples5'):
#     img = Image.open('Examples5/'+i)
#     images.append(img)
images = [
    Image.open('Examples5/istockphoto-1287395025-612x612.jpg'),
    Image.open('Examples5/pexels-photo-3183172.jpeg'),
    Image.open('Examples5/pexels-photo-5971184.jpeg'),
    Image.open('Examples5/pexels-photo-14025704.jpeg'),
    Image.open('Examples5/pexels-photo-3580656.jpeg')
]

# Paste the images onto the canvas
for i in range(len(images)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('collage.jpg')
309/28:
from PIL import Image, ImageDraw
import random

# Define the dimensions of the final collage
canvas_width = 1000
canvas_height = 1000

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
shapes = [
    ((0, 0), (500, 500)),
    ((500, 0), (1000, 500)),
    ((0, 500), (500, 1000)),
    ((500, 500), (1000, 1000))
]

# Shuffle the shapes to add some randomness to the collage
random.shuffle(shapes)

# Load the images to be used in the collage
# images = []
# for i in os.listdir('Examples5'):
#     img = Image.open('Examples5/'+i)
#     images.append(img)
images = [
    Image.open('Examples5/istockphoto-1287395025-612x612.jpg'),
    Image.open('Examples5/pexels-photo-3183172.jpeg'),
    Image.open('Examples5/pexels-photo-5971184.jpeg'),
    Image.open('Examples5/pexels-photo-14025704.jpeg')
#     Image.open('Examples5/pexels-photo-3580656.jpeg')
]

# Paste the images onto the canvas
for i in range(len(images)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('collage.jpg')
309/29:
from PIL import Image
import numpy as np
import random

# Define the dimensions of the final collage
canvas_width = 1000
canvas_height = 1000

# Create a new array to use as the canvas
canvas_array = np.zeros((canvas_height, canvas_width, 3), dtype=np.uint8)
canvas_array[:, :, :] = 255  # Set the background color to white

# Define the shapes to use in the collage
shapes = [
    ((0, 0), (500, 500)),
    ((500, 0), (1000, 500)),
    ((0, 500), (500, 1000)),
    ((500, 500), (1000, 1000))
]

# Shuffle the shapes to add some randomness to the collage
random.shuffle(shapes)

# Load the images to be used in the collage
# images = [
#     Image.open('image1.jpg'),
#     Image.open('image2.jpg'),
#     Image.open('image3.jpg'),
#     Image.open('image4.jpg'),
#     Image.open('image5.jpg')
# ]

# Concatenate the images onto the canvas
for i in range(len(images)):
    x0, y0 = shapes[i][0]
    x1, y1 = shapes[i][1]
    image_array = np.array(images[i])
    canvas_array[y0:y1, x0:x1, :] = image_array

# Create a new image from the canvas array
canvas = Image.fromarray(canvas_array)

# Save the collage
canvas.save('collage2.jpg')
309/30:
from PIL import Image
import numpy as np
import random

# Define the dimensions of the final collage
canvas_width = 1000
canvas_height = 1000

# Create a new array to use as the canvas
canvas_array = np.zeros((canvas_height, canvas_width, 3), dtype=np.uint8)
canvas_array[:, :, :] = 255  # Set the background color to white

# Define the shapes to use in the collage
shapes = [
    ((0, 0), (500, 500)),
    ((500, 0), (1000, 500)),
    ((0, 500), (500, 1000)),
    ((500, 500), (1000, 1000))
]

# Shuffle the shapes to add some randomness to the collage
random.shuffle(shapes)

# Load the images to be used in the collage
# images = [
#     Image.open('image1.jpg'),
#     Image.open('image2.jpg'),
#     Image.open('image3.jpg'),
#     Image.open('image4.jpg'),
#     Image.open('image5.jpg')
# ]

# Concatenate the images onto the canvas
for i in range(len(images)):
    x0, y0 = shapes[i][0]
    x1, y1 = shapes[i][1]
    image_array = np.array(images[i])
#     canvas_array[y0:y1, x0:x1, :] = image_array

# Create a new image from the canvas array
canvas = Image.fromarray(image_array)

# Save the collage
canvas.save('collage2.jpg')
309/31:
from PIL import Image, ImageDraw
import random

# Define the dimensions of the final collage
canvas_width = 1000
canvas_height = 1000

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
shapes = [
    ((0, 0), (1000, 1000)),
    ((1000, 0), (2000, 1000)),
    ((0, 1000), (1000, 2000)),
    ((1000, 1000), (2000, 2000))
]

# Shuffle the shapes to add some randomness to the collage
random.shuffle(shapes)

# Load the images to be used in the collage
# images = []
# for i in os.listdir('Examples5'):
#     img = Image.open('Examples5/'+i)
#     images.append(img)
images = [
    Image.open('Examples5/istockphoto-1287395025-612x612.jpg'),
    Image.open('Examples5/pexels-photo-3183172.jpeg'),
    Image.open('Examples5/pexels-photo-5971184.jpeg'),
    Image.open('Examples5/pexels-photo-14025704.jpeg')
#     Image.open('Examples5/pexels-photo-3580656.jpeg')
]

# Paste the images onto the canvas
for i in range(len(images)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('collage2.jpg')
309/32:
from PIL import Image, ImageDraw
import random

# Define the dimensions of the final collage
canvas_width = 2000
canvas_height = 2000

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
shapes = [
    ((0, 0), (1000, 1000)),
    ((1000, 0), (2000, 1000)),
    ((0, 1000), (1000, 2000)),
    ((1000, 1000), (2000, 2000))
]

# Shuffle the shapes to add some randomness to the collage
random.shuffle(shapes)

# Load the images to be used in the collage
# images = []
# for i in os.listdir('Examples5'):
#     img = Image.open('Examples5/'+i)
#     images.append(img)
images = [
    Image.open('Examples5/istockphoto-1287395025-612x612.jpg'),
    Image.open('Examples5/pexels-photo-3183172.jpeg'),
    Image.open('Examples5/pexels-photo-5971184.jpeg'),
    Image.open('Examples5/pexels-photo-14025704.jpeg')
#     Image.open('Examples5/pexels-photo-3580656.jpeg')
]

# Paste the images onto the canvas
for i in range(len(images)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('collage2.jpg')
309/33:
from PIL import Image, ImageDraw
import random

# Define the dimensions of the final collage
canvas_width = 2000
canvas_height = 2000

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
shapes = [
    ((0, 0), (500, 500)),
    ((500, 0), (1000, 500)),
    ((0, 500), (500, 1000)),
    ((500, 500), (1000, 1000))
]

# Shuffle the shapes to add some randomness to the collage
random.shuffle(shapes)

# Load the images to be used in the collage
# images = []
# for i in os.listdir('Examples5'):
#     img = Image.open('Examples5/'+i)
#     images.append(img)
images = [
    Image.open('Examples5/istockphoto-1287395025-612x612.jpg'),
    Image.open('Examples5/pexels-photo-3183172.jpeg'),
    Image.open('Examples5/pexels-photo-5971184.jpeg'),
    Image.open('Examples5/pexels-photo-14025704.jpeg')
#     Image.open('Examples5/pexels-photo-3580656.jpeg')
]

# Paste the images onto the canvas
for i in range(len(images)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('collage.jpg')
309/34:
from PIL import Image, ImageDraw
import random

# Define the dimensions of the final collage
canvas_width = 1000
canvas_height = 1000

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
shapes = [
    ((0, 0), (500, 500)),
    ((500, 0), (1000, 500)),
    ((0, 500), (500, 1000)),
    ((500, 500), (1000, 1000))
]

# Shuffle the shapes to add some randomness to the collage
random.shuffle(shapes)

# Load the images to be used in the collage
# images = []
# for i in os.listdir('Examples5'):
#     img = Image.open('Examples5/'+i)
#     images.append(img)
images = [
    Image.open('Examples5/istockphoto-1287395025-612x612.jpg'),
    Image.open('Examples5/pexels-photo-3183172.jpeg'),
    Image.open('Examples5/pexels-photo-5971184.jpeg'),
    Image.open('Examples5/pexels-photo-14025704.jpeg')
#     Image.open('Examples5/pexels-photo-3580656.jpeg')
]

# Paste the images onto the canvas
for i in range(len(images)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('collage.jpg')
309/35:
from PIL import Image, ImageDraw
import random

# Define the dimensions of the final collage
canvas_width = 2000
canvas_height = 2000

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
x, y = 500, 1000
shapes = [
    ((0, 0), (x, x)),
    ((x, 0), (y, x)),
    ((0, x), (x, y)),
    ((x, x), (y, y))
]

# Shuffle the shapes to add some randomness to the collage
random.shuffle(shapes)

# Load the images to be used in the collage
# images = []
# for i in os.listdir('Examples5'):
#     img = Image.open('Examples5/'+i)
#     images.append(img)
images = [
    Image.open('Examples5/istockphoto-1287395025-612x612.jpg'),
    Image.open('Examples5/pexels-photo-3183172.jpeg'),
    Image.open('Examples5/pexels-photo-5971184.jpeg'),
    Image.open('Examples5/pexels-photo-14025704.jpeg')
#     Image.open('Examples5/pexels-photo-3580656.jpeg')
]

# Paste the images onto the canvas
for i in range(len(images)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('collage2.jpg')
310/1:
a = 'https://cdn.discordapp.com/attachments/1088289456794124390/1090511305011638343/DOS_BOOT_sea_view_coconut_trees_morning_wibes_cold_drinks_frien_53c381cb-dbf0-4f50-a52f-dc214891d505.png'
a.split('/')
310/2:
a = 'https://cdn.discordapp.com/attachments/1088289456794124390/1090511305011638343/DOS_BOOT_sea_view_coconut_trees_morning_wibes_cold_drinks_frien_53c381cb-dbf0-4f50-a52f-dc214891d505.png'
a.split('/')[-1][:-4]
314/1:
from PIL import Image
all_images, all_files = [],[]
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
#     img = img.convert(mode="RGB")
    b, g, r = img.split()
    img = Image.merge("RGB", (r, g, b))
    all_images.append(img)
    all_files.append(file_path)

# get the images Wikipedia
s = requests.Session()
url = "https://commons.wikimedia.org/w/api.php"
params = {
  "action": "query",
  "generator": "images",
  "prop": "imageinfo",
  "gimlimit": 500,
  "titles": keywords[j],
  "iiprop": "url|dimensions",
  "format": "json"
}
r = s.get(url=url, params=params)
data = r.json()
image_files = []
if "query" not in data.keys():
    continue
pages = data['query']['pages']
for k, v in pages.items():
    for info in v['imageinfo']:
        imurl = info["url"]
        h =  info["height"]
        w = info["width"]
        a = h * w
        if a >= 512*512 and imurl not in image_files and imurl.lower().endswith("jpg"):
        image_files.append(imurl)
random.shuffle(image_files)
for im in image_files[:num_images]:
    filename = im.split("/")[-1]
    download_file(im, "wiki_images")
    file_path = "wiki_images/" + filename
    if file_path not in all_files:
        # print(file_path)
        img = Image.open(file_path)
        img = img.convert(mode="RGB")
        all_images.append(img)
        all_files.append(file_path)
        num_wikiimages += 1

print("num openimages  ", num_openimages)
print("num wiki images ", num_wikiimages)
print("num total images", num_openimages+num_wikiimages)

input_resolution = 224
image_features = torch.empty((0, 512))

preprocess = Compose([
    Resize(input_resolution, interpolation=InterpolationMode.BICUBIC),
    CenterCrop(input_resolution),
    ToTensor()
])

images = [preprocess(im) for im in all_images]
image_input = torch.tensor(np.stack(images)).cuda()
with torch.no_grad():
    image_features = clip_model.encode_image(image_input).float().cpu()  
image_features /= image_features.norm(dim=-1, keepdim=True)

feature_text = "<|startoftext|> Image of " + prompt + " <|endoftext|>"
query_features = get_text_features(feature_text)

image_similarity = query_features.cpu().numpy() @ image_features.numpy().T
image_similarity = image_similarity[0]
print(len(all_files))

num_images = min(int(0.75*len(all_files)), 25)
image_scores, image_indices = get_top_N_semantic_similarity(image_similarity, N=num_images)
columns = 5
rows = num_images // columns + 1
fig=plt.figure(figsize=(columns*5, rows*5))
for i in range(1, columns*rows + 1):
    file_name = all_files[image_indices[i-1]]
    img = Image.open(file_name)
    img = img.convert(mode="RGB")
    fig.add_subplot(rows, columns, i)
    plt.margins(y=10)
    plt.imshow(img)
    plt.text(0, -30, str(i) + " " + file_name, fontsize=10)
    plt.axis("off")
    if i >= num_images:
        break
plt.show()

image_parts = []
parts_rgb = []
parts_a = []
part_sizes = {}
part_count = 0

preprocess_parts = Compose([
    ToTensor()
])

for i in range(num_images):
    image_file = all_files[image_indices[i]]
    print(i, file_name)

    input_image = Image.open(image_file).convert(mode="RGB")
    plt.figure(figsize=(10, 10))
    plt.axis("off")
    _ = plt.imshow(input_image)
    plt.show()

    result = inference_detector(groie_model, image_file)
    bbox_result, segm_result = result

    boxes = []
    overlaps = []
    scores = []
    labels = []
    mask_areas = []
    result_image = np.array(input_image.copy())
    count = 0

    # print()
    # print("objects")

    for label, boxscores in enumerate(bbox_result):
        for boxscore in boxscores:
            box = boxscore[:4]
            score = boxscore[4]

            overlapping = False
            for b in boxes:
                overlap = calculate_iou(box, b)
                # print("overlap", overlap)
                if  overlap > 0.85:
                    overlapping = True
                    # print("skipping")
                    break

            overlaps.append(overlapping)

            boxes.append(box)
            scores.append(score)
            labels.append(label)
            # print(label, coco_names[label], box, score)

            if overlapping:
                continue

            color = random.choice(colors)
            # print(count+1, coco_names[label], round(100*score.item(), 2))
            # draw box
            tl = round(0.001 * max(result_image.shape[0:2])) + 1  # line thickness
            c1, c2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))
            cv2.rectangle(result_image, c1, c2, color, thickness=tl)
            # draw text
            display_txt = "%s: %.1f%%" % (coco_names[label], 100*score)
            tf = max(tl - 1, 1)  # font thickness
            t_size = cv2.getTextSize(display_txt, 0, fontScale=tl / 3, thickness=tf)[0]
            c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3
            cv2.rectangle(result_image, c1, c2, color, -1)  # filled
            cv2.putText(result_image, display_txt, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)
            count += 1

    if count == 0:
        print("no objects found")
        continue

    plt.figure(figsize=(10, 10))
    plt.axis("off")
    _ = plt.imshow(result_image)
    plt.show()

    count = 0
    masks = []
    mask_accum = None
    for object_masks in segm_result:
        for mask in object_masks:
            mask_np = np.float32(mask)
            masks.append(mask_np)
            mask_area = mask_np.sum() / (input_image.width*input_image.height)
            mask_areas.append(mask_area)
            if mask_accum is None:
                mask_accum = mask
            else:
                mask_accum = np.maximum(mask_accum, mask)
            count += 1

    images = []

    img_wid = input_image.width
    img_hgt = input_image.height

    count = 0
    for box, overlapping, score, mask_np in zip(boxes, overlaps, scores, masks):
        # print(box)

    mask_np = np.expand_dims(mask_np, axis=2)
    num_mask_pixels = mask_np.sum()

    if overlapping or num_mask_pixels < 1000:
        continue

    mask_np = np.repeat(mask_np, 3, axis=2)
    image_np = np.array(input_image, dtype=np.float32)/255.0

    if mask_np.shape != image_np.shape:
        continue

    masked_image_np = mask_np * image_np

    box_lft = int(box[0].item())
    box_top = int(box[1].item())
    box_rgt = int(box[2].item())
    box_bot = int(box[3].item())

    cutout_image_np = masked_image_np[box_top:box_bot, box_lft:box_rgt]
    cutout_mask_np = mask_np[box_top:box_bot, box_lft:box_rgt]

    box_wid = box_rgt - box_lft
    box_hgt = box_bot - box_top

    if box_wid > box_hgt: # handle landscape images
        # print("landscape")
        pad = (box_wid - box_hgt) // 2
        padded_image_np = np.zeros((box_wid, box_wid, 3), dtype=cutout_image_np.dtype)
        padded_image_np[pad:pad+box_hgt, :] = cutout_image_np
    
    else: # handle portrait images
        # print("portrait")
        pad = (box_hgt - box_wid) // 2
        padded_image_np = np.zeros((box_hgt, box_hgt, 3), dtype=cutout_image_np.dtype)
        padded_image_np[:, pad:pad+box_wid] = cutout_image_np     

    image_PIL = Image.fromarray(np.uint8(padded_image_np*255))

    plt.figure(figsize=(6, 6))
    plt.axis("off")
    _ = plt.imshow(image_PIL)
    plt.show()

    image_parts.append(preprocess(image_PIL))

    part_PIL = Image.fromarray(np.uint8(cutout_image_np*255))
    w, h = part_PIL.size
    if w > h and w > 512:
        part_pil = part_PIL.resize((512, int(h*512/w)), Image.BICUBIC)
    elif h > 512:
        part_pil = part_PIL.resize((int(w*512/h), 512), Image.BICUBIC)

    parts_rgb.append(preprocess_parts(part_PIL))
    mask_PIL = Image.fromarray(np.uint8(cutout_mask_np*255))
    parts_a.append(preprocess_parts(mask_PIL))
    part_sizes[part_count] = num_mask_pixels
    count += 1
    part_count += 1

to_pil = T.ToPILImage()

num_parts = min(len(image_parts)//2,100)

part_input = torch.tensor(np.stack(image_parts)).cuda()
with torch.no_grad():
    part_features = clip_model.encode_image(part_input).float().cpu()  
part_features /= part_features.norm(dim=-1, keepdim=True)

feature_text = "<|startoftext|> Image of " + prompt + " <|endoftext|>"
query_features = get_text_features(feature_text)

part_similarity = query_features.cpu().numpy() @ part_features.numpy().T
part_similarity = part_similarity[0]

part_scores, part_indices = get_top_N_semantic_similarity(part_similarity, N=num_parts)
columns = 5
rows = num_parts // columns + 1
fig=plt.figure(figsize=(columns*5, rows*5))
for i in range(1, columns*rows + 1):
    img = to_pil(image_parts[part_indices[i-1]])
    fig.add_subplot(rows, columns, i)
    plt.margins(y=10)
    plt.imshow(img)
    plt.text(0, -5, str(i-1), fontsize=12)
    plt.axis("off")
    if i >= num_parts:
        break
plt.show()

ordered_part_indices = []
for p in part_indices[:num_parts]:
    size = part_sizes[p]
    ordered_part_indices.append((size, p))

ordered_part_indices.sort(reverse=True)
314/2:
from PIL import Image
all_images, all_files = [],[]
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
#     img = img.convert(mode="RGB")
    b, g, r = img.split()
    img = Image.merge("RGB", (r, g, b))
    all_images.append(img)
    all_files.append(file_path)

# get the images Wikipedia
s = requests.Session()
url = "https://commons.wikimedia.org/w/api.php"
params = {
  "action": "query",
  "generator": "images",
  "prop": "imageinfo",
  "gimlimit": 500,
  "titles": keywords[j],
  "iiprop": "url|dimensions",
  "format": "json"
}
r = s.get(url=url, params=params)
data = r.json()
image_files = []
if "query" not in data.keys():
    continue
pages = data['query']['pages']
for k, v in pages.items():
    for info in v['imageinfo']:
        imurl = info["url"]
        h =  info["height"]
        w = info["width"]
        a = h * w
        if a >= 512*512 and imurl not in image_files and imurl.lower().endswith("jpg"):
            image_files.append(imurl)
random.shuffle(image_files)
for im in image_files[:num_images]:
    filename = im.split("/")[-1]
    download_file(im, "wiki_images")
    file_path = "wiki_images/" + filename
    if file_path not in all_files:
        # print(file_path)
        img = Image.open(file_path)
        img = img.convert(mode="RGB")
        all_images.append(img)
        all_files.append(file_path)
        num_wikiimages += 1

print("num openimages  ", num_openimages)
print("num wiki images ", num_wikiimages)
print("num total images", num_openimages+num_wikiimages)

input_resolution = 224
image_features = torch.empty((0, 512))

preprocess = Compose([
    Resize(input_resolution, interpolation=InterpolationMode.BICUBIC),
    CenterCrop(input_resolution),
    ToTensor()
])

images = [preprocess(im) for im in all_images]
image_input = torch.tensor(np.stack(images)).cuda()
with torch.no_grad():
    image_features = clip_model.encode_image(image_input).float().cpu()  
image_features /= image_features.norm(dim=-1, keepdim=True)

feature_text = "<|startoftext|> Image of " + prompt + " <|endoftext|>"
query_features = get_text_features(feature_text)

image_similarity = query_features.cpu().numpy() @ image_features.numpy().T
image_similarity = image_similarity[0]
print(len(all_files))

num_images = min(int(0.75*len(all_files)), 25)
image_scores, image_indices = get_top_N_semantic_similarity(image_similarity, N=num_images)
columns = 5
rows = num_images // columns + 1
fig=plt.figure(figsize=(columns*5, rows*5))
for i in range(1, columns*rows + 1):
    file_name = all_files[image_indices[i-1]]
    img = Image.open(file_name)
    img = img.convert(mode="RGB")
    fig.add_subplot(rows, columns, i)
    plt.margins(y=10)
    plt.imshow(img)
    plt.text(0, -30, str(i) + " " + file_name, fontsize=10)
    plt.axis("off")
    if i >= num_images:
        break
plt.show()

image_parts = []
parts_rgb = []
parts_a = []
part_sizes = {}
part_count = 0

preprocess_parts = Compose([
    ToTensor()
])

for i in range(num_images):
    image_file = all_files[image_indices[i]]
    print(i, file_name)

    input_image = Image.open(image_file).convert(mode="RGB")
    plt.figure(figsize=(10, 10))
    plt.axis("off")
    _ = plt.imshow(input_image)
    plt.show()

    result = inference_detector(groie_model, image_file)
    bbox_result, segm_result = result

    boxes = []
    overlaps = []
    scores = []
    labels = []
    mask_areas = []
    result_image = np.array(input_image.copy())
    count = 0

    # print()
    # print("objects")

    for label, boxscores in enumerate(bbox_result):
        for boxscore in boxscores:
            box = boxscore[:4]
            score = boxscore[4]

            overlapping = False
            for b in boxes:
                overlap = calculate_iou(box, b)
                # print("overlap", overlap)
                if  overlap > 0.85:
                    overlapping = True
                    # print("skipping")
                    break

            overlaps.append(overlapping)

            boxes.append(box)
            scores.append(score)
            labels.append(label)
            # print(label, coco_names[label], box, score)

            if overlapping:
                continue

            color = random.choice(colors)
            # print(count+1, coco_names[label], round(100*score.item(), 2))
            # draw box
            tl = round(0.001 * max(result_image.shape[0:2])) + 1  # line thickness
            c1, c2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))
            cv2.rectangle(result_image, c1, c2, color, thickness=tl)
            # draw text
            display_txt = "%s: %.1f%%" % (coco_names[label], 100*score)
            tf = max(tl - 1, 1)  # font thickness
            t_size = cv2.getTextSize(display_txt, 0, fontScale=tl / 3, thickness=tf)[0]
            c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3
            cv2.rectangle(result_image, c1, c2, color, -1)  # filled
            cv2.putText(result_image, display_txt, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)
            count += 1

    if count == 0:
        print("no objects found")
        continue

    plt.figure(figsize=(10, 10))
    plt.axis("off")
    _ = plt.imshow(result_image)
    plt.show()

    count = 0
    masks = []
    mask_accum = None
    for object_masks in segm_result:
        for mask in object_masks:
            mask_np = np.float32(mask)
            masks.append(mask_np)
            mask_area = mask_np.sum() / (input_image.width*input_image.height)
            mask_areas.append(mask_area)
            if mask_accum is None:
                mask_accum = mask
            else:
                mask_accum = np.maximum(mask_accum, mask)
            count += 1

    images = []

    img_wid = input_image.width
    img_hgt = input_image.height

    count = 0
    for box, overlapping, score, mask_np in zip(boxes, overlaps, scores, masks):
        # print(box)

    mask_np = np.expand_dims(mask_np, axis=2)
    num_mask_pixels = mask_np.sum()

    if overlapping or num_mask_pixels < 1000:
        continue

    mask_np = np.repeat(mask_np, 3, axis=2)
    image_np = np.array(input_image, dtype=np.float32)/255.0

    if mask_np.shape != image_np.shape:
        continue

    masked_image_np = mask_np * image_np

    box_lft = int(box[0].item())
    box_top = int(box[1].item())
    box_rgt = int(box[2].item())
    box_bot = int(box[3].item())

    cutout_image_np = masked_image_np[box_top:box_bot, box_lft:box_rgt]
    cutout_mask_np = mask_np[box_top:box_bot, box_lft:box_rgt]

    box_wid = box_rgt - box_lft
    box_hgt = box_bot - box_top

    if box_wid > box_hgt: # handle landscape images
        # print("landscape")
        pad = (box_wid - box_hgt) // 2
        padded_image_np = np.zeros((box_wid, box_wid, 3), dtype=cutout_image_np.dtype)
        padded_image_np[pad:pad+box_hgt, :] = cutout_image_np
    
    else: # handle portrait images
        # print("portrait")
        pad = (box_hgt - box_wid) // 2
        padded_image_np = np.zeros((box_hgt, box_hgt, 3), dtype=cutout_image_np.dtype)
        padded_image_np[:, pad:pad+box_wid] = cutout_image_np     

    image_PIL = Image.fromarray(np.uint8(padded_image_np*255))

    plt.figure(figsize=(6, 6))
    plt.axis("off")
    _ = plt.imshow(image_PIL)
    plt.show()

    image_parts.append(preprocess(image_PIL))

    part_PIL = Image.fromarray(np.uint8(cutout_image_np*255))
    w, h = part_PIL.size
    if w > h and w > 512:
        part_pil = part_PIL.resize((512, int(h*512/w)), Image.BICUBIC)
    elif h > 512:
        part_pil = part_PIL.resize((int(w*512/h), 512), Image.BICUBIC)

    parts_rgb.append(preprocess_parts(part_PIL))
    mask_PIL = Image.fromarray(np.uint8(cutout_mask_np*255))
    parts_a.append(preprocess_parts(mask_PIL))
    part_sizes[part_count] = num_mask_pixels
    count += 1
    part_count += 1

to_pil = T.ToPILImage()

num_parts = min(len(image_parts)//2,100)

part_input = torch.tensor(np.stack(image_parts)).cuda()
with torch.no_grad():
    part_features = clip_model.encode_image(part_input).float().cpu()  
part_features /= part_features.norm(dim=-1, keepdim=True)

feature_text = "<|startoftext|> Image of " + prompt + " <|endoftext|>"
query_features = get_text_features(feature_text)

part_similarity = query_features.cpu().numpy() @ part_features.numpy().T
part_similarity = part_similarity[0]

part_scores, part_indices = get_top_N_semantic_similarity(part_similarity, N=num_parts)
columns = 5
rows = num_parts // columns + 1
fig=plt.figure(figsize=(columns*5, rows*5))
for i in range(1, columns*rows + 1):
    img = to_pil(image_parts[part_indices[i-1]])
    fig.add_subplot(rows, columns, i)
    plt.margins(y=10)
    plt.imshow(img)
    plt.text(0, -5, str(i-1), fontsize=12)
    plt.axis("off")
    if i >= num_parts:
        break
plt.show()

ordered_part_indices = []
for p in part_indices[:num_parts]:
    size = part_sizes[p]
    ordered_part_indices.append((size, p))

ordered_part_indices.sort(reverse=True)
314/3:
from PIL import Image
all_images, all_files = [],[]
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
#     img = img.convert(mode="RGB")
    b, g, r = img.split()
    img = Image.merge("RGB", (r, g, b))
    all_images.append(img)
    all_files.append(file_path)

# get the images Wikipedia
s = requests.Session()
url = "https://commons.wikimedia.org/w/api.php"
params = {
  "action": "query",
  "generator": "images",
  "prop": "imageinfo",
  "gimlimit": 500,
  "titles": keywords[j],
  "iiprop": "url|dimensions",
  "format": "json"
}
r = s.get(url=url, params=params)
data = r.json()
image_files = []
if "query" not in data.keys():
    continue
pages = data['query']['pages']
for k, v in pages.items():
    for info in v['imageinfo']:
        imurl = info["url"]
        h =  info["height"]
        w = info["width"]
        a = h * w
        if a >= 512*512 and imurl not in image_files and imurl.lower().endswith("jpg"):
            image_files.append(imurl)
random.shuffle(image_files)
for im in image_files[:num_images]:
    filename = im.split("/")[-1]
    download_file(im, "wiki_images")
    file_path = "wiki_images/" + filename
    if file_path not in all_files:
        # print(file_path)
        img = Image.open(file_path)
        img = img.convert(mode="RGB")
        all_images.append(img)
        all_files.append(file_path)
        num_wikiimages += 1

print("num openimages  ", num_openimages)
print("num wiki images ", num_wikiimages)
print("num total images", num_openimages+num_wikiimages)

input_resolution = 224
image_features = torch.empty((0, 512))

preprocess = Compose([
    Resize(input_resolution, interpolation=InterpolationMode.BICUBIC),
    CenterCrop(input_resolution),
    ToTensor()
])

images = [preprocess(im) for im in all_images]
image_input = torch.tensor(np.stack(images)).cuda()
with torch.no_grad():
    image_features = clip_model.encode_image(image_input).float().cpu()  
image_features /= image_features.norm(dim=-1, keepdim=True)

feature_text = "<|startoftext|> Image of " + prompt + " <|endoftext|>"
query_features = get_text_features(feature_text)

image_similarity = query_features.cpu().numpy() @ image_features.numpy().T
image_similarity = image_similarity[0]
print(len(all_files))

num_images = min(int(0.75*len(all_files)), 25)
image_scores, image_indices = get_top_N_semantic_similarity(image_similarity, N=num_images)
columns = 5
rows = num_images // columns + 1
fig=plt.figure(figsize=(columns*5, rows*5))
for i in range(1, columns*rows + 1):
    file_name = all_files[image_indices[i-1]]
    img = Image.open(file_name)
    img = img.convert(mode="RGB")
    fig.add_subplot(rows, columns, i)
    plt.margins(y=10)
    plt.imshow(img)
    plt.text(0, -30, str(i) + " " + file_name, fontsize=10)
    plt.axis("off")
    if i >= num_images:
        break
plt.show()

image_parts = []
parts_rgb = []
parts_a = []
part_sizes = {}
part_count = 0

preprocess_parts = Compose([
    ToTensor()
])

for i in range(num_images):
    image_file = all_files[image_indices[i]]
    print(i, file_name)

    input_image = Image.open(image_file).convert(mode="RGB")
    plt.figure(figsize=(10, 10))
    plt.axis("off")
    _ = plt.imshow(input_image)
    plt.show()

    result = inference_detector(groie_model, image_file)
    bbox_result, segm_result = result

    boxes = []
    overlaps = []
    scores = []
    labels = []
    mask_areas = []
    result_image = np.array(input_image.copy())
    count = 0

    # print()
    # print("objects")

    for label, boxscores in enumerate(bbox_result):
        for boxscore in boxscores:
            box = boxscore[:4]
            score = boxscore[4]

            overlapping = False
            for b in boxes:
                overlap = calculate_iou(box, b)
                # print("overlap", overlap)
                if  overlap > 0.85:
                    overlapping = True
                    # print("skipping")
                    break

            overlaps.append(overlapping)

            boxes.append(box)
            scores.append(score)
            labels.append(label)
            # print(label, coco_names[label], box, score)

            if overlapping:
                continue

            color = random.choice(colors)
            # print(count+1, coco_names[label], round(100*score.item(), 2))
            # draw box
            tl = round(0.001 * max(result_image.shape[0:2])) + 1  # line thickness
            c1, c2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))
            cv2.rectangle(result_image, c1, c2, color, thickness=tl)
            # draw text
            display_txt = "%s: %.1f%%" % (coco_names[label], 100*score)
            tf = max(tl - 1, 1)  # font thickness
            t_size = cv2.getTextSize(display_txt, 0, fontScale=tl / 3, thickness=tf)[0]
            c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3
            cv2.rectangle(result_image, c1, c2, color, -1)  # filled
            cv2.putText(result_image, display_txt, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)
            count += 1

    if count == 0:
        print("no objects found")
        continue

    plt.figure(figsize=(10, 10))
    plt.axis("off")
    _ = plt.imshow(result_image)
    plt.show()

    count = 0
    masks = []
    mask_accum = None
    for object_masks in segm_result:
        for mask in object_masks:
            mask_np = np.float32(mask)
            masks.append(mask_np)
            mask_area = mask_np.sum() / (input_image.width*input_image.height)
            mask_areas.append(mask_area)
            if mask_accum is None:
                mask_accum = mask
            else:
                mask_accum = np.maximum(mask_accum, mask)
            count += 1

    images = []

    img_wid = input_image.width
    img_hgt = input_image.height

    count = 0
    for box, overlapping, score, mask_np in zip(boxes, overlaps, scores, masks):
        # print(box)

        mask_np = np.expand_dims(mask_np, axis=2)
        num_mask_pixels = mask_np.sum()

        if overlapping or num_mask_pixels < 1000:
            continue

        mask_np = np.repeat(mask_np, 3, axis=2)
        image_np = np.array(input_image, dtype=np.float32)/255.0

        if mask_np.shape != image_np.shape:
            continue

        masked_image_np = mask_np * image_np

        box_lft = int(box[0].item())
        box_top = int(box[1].item())
        box_rgt = int(box[2].item())
        box_bot = int(box[3].item())

        cutout_image_np = masked_image_np[box_top:box_bot, box_lft:box_rgt]
        cutout_mask_np = mask_np[box_top:box_bot, box_lft:box_rgt]

        box_wid = box_rgt - box_lft
        box_hgt = box_bot - box_top

        if box_wid > box_hgt: # handle landscape images
            # print("landscape")
            pad = (box_wid - box_hgt) // 2
            padded_image_np = np.zeros((box_wid, box_wid, 3), dtype=cutout_image_np.dtype)
            padded_image_np[pad:pad+box_hgt, :] = cutout_image_np

        else: # handle portrait images
            # print("portrait")
            pad = (box_hgt - box_wid) // 2
            padded_image_np = np.zeros((box_hgt, box_hgt, 3), dtype=cutout_image_np.dtype)
            padded_image_np[:, pad:pad+box_wid] = cutout_image_np     

        image_PIL = Image.fromarray(np.uint8(padded_image_np*255))

        plt.figure(figsize=(6, 6))
        plt.axis("off")
        _ = plt.imshow(image_PIL)
        plt.show()

        image_parts.append(preprocess(image_PIL))

        part_PIL = Image.fromarray(np.uint8(cutout_image_np*255))
        w, h = part_PIL.size
        if w > h and w > 512:
            part_pil = part_PIL.resize((512, int(h*512/w)), Image.BICUBIC)
        elif h > 512:
            part_pil = part_PIL.resize((int(w*512/h), 512), Image.BICUBIC)

        parts_rgb.append(preprocess_parts(part_PIL))
        mask_PIL = Image.fromarray(np.uint8(cutout_mask_np*255))
        parts_a.append(preprocess_parts(mask_PIL))
        part_sizes[part_count] = num_mask_pixels
        count += 1
        part_count += 1

to_pil = T.ToPILImage()

num_parts = min(len(image_parts)//2,100)

part_input = torch.tensor(np.stack(image_parts)).cuda()
with torch.no_grad():
    part_features = clip_model.encode_image(part_input).float().cpu()  
part_features /= part_features.norm(dim=-1, keepdim=True)

feature_text = "<|startoftext|> Image of " + prompt + " <|endoftext|>"
query_features = get_text_features(feature_text)

part_similarity = query_features.cpu().numpy() @ part_features.numpy().T
part_similarity = part_similarity[0]

part_scores, part_indices = get_top_N_semantic_similarity(part_similarity, N=num_parts)
columns = 5
rows = num_parts // columns + 1
fig=plt.figure(figsize=(columns*5, rows*5))
for i in range(1, columns*rows + 1):
    img = to_pil(image_parts[part_indices[i-1]])
    fig.add_subplot(rows, columns, i)
    plt.margins(y=10)
    plt.imshow(img)
    plt.text(0, -5, str(i-1), fontsize=12)
    plt.axis("off")
    if i >= num_parts:
        break
plt.show()

ordered_part_indices = []
for p in part_indices[:num_parts]:
    size = part_sizes[p]
    ordered_part_indices.append((size, p))

ordered_part_indices.sort(reverse=True)
314/4:
from PIL import Image
import os
all_images, all_files = [],[]
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
#     img = img.convert(mode="RGB")
    b, g, r = img.split()
    img = Image.merge("RGB", (r, g, b))
    all_images.append(img)
    all_files.append(file_path)

# get the images Wikipedia
s = requests.Session()
url = "https://commons.wikimedia.org/w/api.php"
params = {
  "action": "query",
  "generator": "images",
  "prop": "imageinfo",
  "gimlimit": 500,
  "titles": keywords[j],
  "iiprop": "url|dimensions",
  "format": "json"
}
r = s.get(url=url, params=params)
data = r.json()
image_files = []
if "query" not in data.keys():
    continue
pages = data['query']['pages']
for k, v in pages.items():
    for info in v['imageinfo']:
        imurl = info["url"]
        h =  info["height"]
        w = info["width"]
        a = h * w
        if a >= 512*512 and imurl not in image_files and imurl.lower().endswith("jpg"):
            image_files.append(imurl)
random.shuffle(image_files)
for im in image_files[:num_images]:
    filename = im.split("/")[-1]
    download_file(im, "wiki_images")
    file_path = "wiki_images/" + filename
    if file_path not in all_files:
        # print(file_path)
        img = Image.open(file_path)
        img = img.convert(mode="RGB")
        all_images.append(img)
        all_files.append(file_path)
        num_wikiimages += 1

print("num openimages  ", num_openimages)
print("num wiki images ", num_wikiimages)
print("num total images", num_openimages+num_wikiimages)

input_resolution = 224
image_features = torch.empty((0, 512))

preprocess = Compose([
    Resize(input_resolution, interpolation=InterpolationMode.BICUBIC),
    CenterCrop(input_resolution),
    ToTensor()
])

images = [preprocess(im) for im in all_images]
image_input = torch.tensor(np.stack(images)).cuda()
with torch.no_grad():
    image_features = clip_model.encode_image(image_input).float().cpu()  
image_features /= image_features.norm(dim=-1, keepdim=True)

feature_text = "<|startoftext|> Image of " + prompt + " <|endoftext|>"
query_features = get_text_features(feature_text)

image_similarity = query_features.cpu().numpy() @ image_features.numpy().T
image_similarity = image_similarity[0]
print(len(all_files))

num_images = min(int(0.75*len(all_files)), 25)
image_scores, image_indices = get_top_N_semantic_similarity(image_similarity, N=num_images)
columns = 5
rows = num_images // columns + 1
fig=plt.figure(figsize=(columns*5, rows*5))
for i in range(1, columns*rows + 1):
    file_name = all_files[image_indices[i-1]]
    img = Image.open(file_name)
    img = img.convert(mode="RGB")
    fig.add_subplot(rows, columns, i)
    plt.margins(y=10)
    plt.imshow(img)
    plt.text(0, -30, str(i) + " " + file_name, fontsize=10)
    plt.axis("off")
    if i >= num_images:
        break
plt.show()

image_parts = []
parts_rgb = []
parts_a = []
part_sizes = {}
part_count = 0

preprocess_parts = Compose([
    ToTensor()
])

for i in range(num_images):
    image_file = all_files[image_indices[i]]
    print(i, file_name)

    input_image = Image.open(image_file).convert(mode="RGB")
    plt.figure(figsize=(10, 10))
    plt.axis("off")
    _ = plt.imshow(input_image)
    plt.show()

    result = inference_detector(groie_model, image_file)
    bbox_result, segm_result = result

    boxes = []
    overlaps = []
    scores = []
    labels = []
    mask_areas = []
    result_image = np.array(input_image.copy())
    count = 0

    # print()
    # print("objects")

    for label, boxscores in enumerate(bbox_result):
        for boxscore in boxscores:
            box = boxscore[:4]
            score = boxscore[4]

            overlapping = False
            for b in boxes:
                overlap = calculate_iou(box, b)
                # print("overlap", overlap)
                if  overlap > 0.85:
                    overlapping = True
                    # print("skipping")
                    break

            overlaps.append(overlapping)

            boxes.append(box)
            scores.append(score)
            labels.append(label)
            # print(label, coco_names[label], box, score)

            if overlapping:
                continue

            color = random.choice(colors)
            # print(count+1, coco_names[label], round(100*score.item(), 2))
            # draw box
            tl = round(0.001 * max(result_image.shape[0:2])) + 1  # line thickness
            c1, c2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))
            cv2.rectangle(result_image, c1, c2, color, thickness=tl)
            # draw text
            display_txt = "%s: %.1f%%" % (coco_names[label], 100*score)
            tf = max(tl - 1, 1)  # font thickness
            t_size = cv2.getTextSize(display_txt, 0, fontScale=tl / 3, thickness=tf)[0]
            c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3
            cv2.rectangle(result_image, c1, c2, color, -1)  # filled
            cv2.putText(result_image, display_txt, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)
            count += 1

    if count == 0:
        print("no objects found")
        continue

    plt.figure(figsize=(10, 10))
    plt.axis("off")
    _ = plt.imshow(result_image)
    plt.show()

    count = 0
    masks = []
    mask_accum = None
    for object_masks in segm_result:
        for mask in object_masks:
            mask_np = np.float32(mask)
            masks.append(mask_np)
            mask_area = mask_np.sum() / (input_image.width*input_image.height)
            mask_areas.append(mask_area)
            if mask_accum is None:
                mask_accum = mask
            else:
                mask_accum = np.maximum(mask_accum, mask)
            count += 1

    images = []

    img_wid = input_image.width
    img_hgt = input_image.height

    count = 0
    for box, overlapping, score, mask_np in zip(boxes, overlaps, scores, masks):
        # print(box)

        mask_np = np.expand_dims(mask_np, axis=2)
        num_mask_pixels = mask_np.sum()

        if overlapping or num_mask_pixels < 1000:
            continue

        mask_np = np.repeat(mask_np, 3, axis=2)
        image_np = np.array(input_image, dtype=np.float32)/255.0

        if mask_np.shape != image_np.shape:
            continue

        masked_image_np = mask_np * image_np

        box_lft = int(box[0].item())
        box_top = int(box[1].item())
        box_rgt = int(box[2].item())
        box_bot = int(box[3].item())

        cutout_image_np = masked_image_np[box_top:box_bot, box_lft:box_rgt]
        cutout_mask_np = mask_np[box_top:box_bot, box_lft:box_rgt]

        box_wid = box_rgt - box_lft
        box_hgt = box_bot - box_top

        if box_wid > box_hgt: # handle landscape images
            # print("landscape")
            pad = (box_wid - box_hgt) // 2
            padded_image_np = np.zeros((box_wid, box_wid, 3), dtype=cutout_image_np.dtype)
            padded_image_np[pad:pad+box_hgt, :] = cutout_image_np

        else: # handle portrait images
            # print("portrait")
            pad = (box_hgt - box_wid) // 2
            padded_image_np = np.zeros((box_hgt, box_hgt, 3), dtype=cutout_image_np.dtype)
            padded_image_np[:, pad:pad+box_wid] = cutout_image_np     

        image_PIL = Image.fromarray(np.uint8(padded_image_np*255))

        plt.figure(figsize=(6, 6))
        plt.axis("off")
        _ = plt.imshow(image_PIL)
        plt.show()

        image_parts.append(preprocess(image_PIL))

        part_PIL = Image.fromarray(np.uint8(cutout_image_np*255))
        w, h = part_PIL.size
        if w > h and w > 512:
            part_pil = part_PIL.resize((512, int(h*512/w)), Image.BICUBIC)
        elif h > 512:
            part_pil = part_PIL.resize((int(w*512/h), 512), Image.BICUBIC)

        parts_rgb.append(preprocess_parts(part_PIL))
        mask_PIL = Image.fromarray(np.uint8(cutout_mask_np*255))
        parts_a.append(preprocess_parts(mask_PIL))
        part_sizes[part_count] = num_mask_pixels
        count += 1
        part_count += 1

to_pil = T.ToPILImage()

num_parts = min(len(image_parts)//2,100)

part_input = torch.tensor(np.stack(image_parts)).cuda()
with torch.no_grad():
    part_features = clip_model.encode_image(part_input).float().cpu()  
part_features /= part_features.norm(dim=-1, keepdim=True)

feature_text = "<|startoftext|> Image of " + prompt + " <|endoftext|>"
query_features = get_text_features(feature_text)

part_similarity = query_features.cpu().numpy() @ part_features.numpy().T
part_similarity = part_similarity[0]

part_scores, part_indices = get_top_N_semantic_similarity(part_similarity, N=num_parts)
columns = 5
rows = num_parts // columns + 1
fig=plt.figure(figsize=(columns*5, rows*5))
for i in range(1, columns*rows + 1):
    img = to_pil(image_parts[part_indices[i-1]])
    fig.add_subplot(rows, columns, i)
    plt.margins(y=10)
    plt.imshow(img)
    plt.text(0, -5, str(i-1), fontsize=12)
    plt.axis("off")
    if i >= num_parts:
        break
plt.show()

ordered_part_indices = []
for p in part_indices[:num_parts]:
    size = part_sizes[p]
    ordered_part_indices.append((size, p))

ordered_part_indices.sort(reverse=True)
314/5:
from PIL import Image
import os
all_images, all_files = [],[]
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
#     img = img.convert(mode="RGB")
    b, g, r = img.split()
    img = Image.merge("RGB", (r, g, b))
    all_images.append(img)
    all_files.append(i)

# get the images Wikipedia
s = requests.Session()
url = "https://commons.wikimedia.org/w/api.php"
params = {
  "action": "query",
  "generator": "images",
  "prop": "imageinfo",
  "gimlimit": 500,
  "titles": keywords[j],
  "iiprop": "url|dimensions",
  "format": "json"
}
r = s.get(url=url, params=params)
data = r.json()
image_files = []
if "query" not in data.keys():
    continue
pages = data['query']['pages']
for k, v in pages.items():
    for info in v['imageinfo']:
        imurl = info["url"]
        h =  info["height"]
        w = info["width"]
        a = h * w
        if a >= 512*512 and imurl not in image_files and imurl.lower().endswith("jpg"):
            image_files.append(imurl)
random.shuffle(image_files)
for im in image_files[:num_images]:
    filename = im.split("/")[-1]
    download_file(im, "wiki_images")
    file_path = "wiki_images/" + filename
    if file_path not in all_files:
        # print(file_path)
        img = Image.open(file_path)
        img = img.convert(mode="RGB")
        all_images.append(img)
        all_files.append(file_path)
        num_wikiimages += 1

print("num openimages  ", num_openimages)
print("num wiki images ", num_wikiimages)
print("num total images", num_openimages+num_wikiimages)

input_resolution = 224
image_features = torch.empty((0, 512))

preprocess = Compose([
    Resize(input_resolution, interpolation=InterpolationMode.BICUBIC),
    CenterCrop(input_resolution),
    ToTensor()
])

images = [preprocess(im) for im in all_images]
image_input = torch.tensor(np.stack(images)).cuda()
with torch.no_grad():
    image_features = clip_model.encode_image(image_input).float().cpu()  
image_features /= image_features.norm(dim=-1, keepdim=True)

feature_text = "<|startoftext|> Image of " + prompt + " <|endoftext|>"
query_features = get_text_features(feature_text)

image_similarity = query_features.cpu().numpy() @ image_features.numpy().T
image_similarity = image_similarity[0]
print(len(all_files))

num_images = min(int(0.75*len(all_files)), 25)
image_scores, image_indices = get_top_N_semantic_similarity(image_similarity, N=num_images)
columns = 5
rows = num_images // columns + 1
fig=plt.figure(figsize=(columns*5, rows*5))
for i in range(1, columns*rows + 1):
    file_name = all_files[image_indices[i-1]]
    img = Image.open(file_name)
    img = img.convert(mode="RGB")
    fig.add_subplot(rows, columns, i)
    plt.margins(y=10)
    plt.imshow(img)
    plt.text(0, -30, str(i) + " " + file_name, fontsize=10)
    plt.axis("off")
    if i >= num_images:
        break
plt.show()

image_parts = []
parts_rgb = []
parts_a = []
part_sizes = {}
part_count = 0

preprocess_parts = Compose([
    ToTensor()
])

for i in range(num_images):
    image_file = all_files[image_indices[i]]
    print(i, file_name)

    input_image = Image.open(image_file).convert(mode="RGB")
    plt.figure(figsize=(10, 10))
    plt.axis("off")
    _ = plt.imshow(input_image)
    plt.show()

    result = inference_detector(groie_model, image_file)
    bbox_result, segm_result = result

    boxes = []
    overlaps = []
    scores = []
    labels = []
    mask_areas = []
    result_image = np.array(input_image.copy())
    count = 0

    # print()
    # print("objects")

    for label, boxscores in enumerate(bbox_result):
        for boxscore in boxscores:
            box = boxscore[:4]
            score = boxscore[4]

            overlapping = False
            for b in boxes:
                overlap = calculate_iou(box, b)
                # print("overlap", overlap)
                if  overlap > 0.85:
                    overlapping = True
                    # print("skipping")
                    break

            overlaps.append(overlapping)

            boxes.append(box)
            scores.append(score)
            labels.append(label)
            # print(label, coco_names[label], box, score)

            if overlapping:
                continue

            color = random.choice(colors)
            # print(count+1, coco_names[label], round(100*score.item(), 2))
            # draw box
            tl = round(0.001 * max(result_image.shape[0:2])) + 1  # line thickness
            c1, c2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))
            cv2.rectangle(result_image, c1, c2, color, thickness=tl)
            # draw text
            display_txt = "%s: %.1f%%" % (coco_names[label], 100*score)
            tf = max(tl - 1, 1)  # font thickness
            t_size = cv2.getTextSize(display_txt, 0, fontScale=tl / 3, thickness=tf)[0]
            c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3
            cv2.rectangle(result_image, c1, c2, color, -1)  # filled
            cv2.putText(result_image, display_txt, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)
            count += 1

    if count == 0:
        print("no objects found")
        continue

    plt.figure(figsize=(10, 10))
    plt.axis("off")
    _ = plt.imshow(result_image)
    plt.show()

    count = 0
    masks = []
    mask_accum = None
    for object_masks in segm_result:
        for mask in object_masks:
            mask_np = np.float32(mask)
            masks.append(mask_np)
            mask_area = mask_np.sum() / (input_image.width*input_image.height)
            mask_areas.append(mask_area)
            if mask_accum is None:
                mask_accum = mask
            else:
                mask_accum = np.maximum(mask_accum, mask)
            count += 1

    images = []

    img_wid = input_image.width
    img_hgt = input_image.height

    count = 0
    for box, overlapping, score, mask_np in zip(boxes, overlaps, scores, masks):
        # print(box)

        mask_np = np.expand_dims(mask_np, axis=2)
        num_mask_pixels = mask_np.sum()

        if overlapping or num_mask_pixels < 1000:
            continue

        mask_np = np.repeat(mask_np, 3, axis=2)
        image_np = np.array(input_image, dtype=np.float32)/255.0

        if mask_np.shape != image_np.shape:
            continue

        masked_image_np = mask_np * image_np

        box_lft = int(box[0].item())
        box_top = int(box[1].item())
        box_rgt = int(box[2].item())
        box_bot = int(box[3].item())

        cutout_image_np = masked_image_np[box_top:box_bot, box_lft:box_rgt]
        cutout_mask_np = mask_np[box_top:box_bot, box_lft:box_rgt]

        box_wid = box_rgt - box_lft
        box_hgt = box_bot - box_top

        if box_wid > box_hgt: # handle landscape images
            # print("landscape")
            pad = (box_wid - box_hgt) // 2
            padded_image_np = np.zeros((box_wid, box_wid, 3), dtype=cutout_image_np.dtype)
            padded_image_np[pad:pad+box_hgt, :] = cutout_image_np

        else: # handle portrait images
            # print("portrait")
            pad = (box_hgt - box_wid) // 2
            padded_image_np = np.zeros((box_hgt, box_hgt, 3), dtype=cutout_image_np.dtype)
            padded_image_np[:, pad:pad+box_wid] = cutout_image_np     

        image_PIL = Image.fromarray(np.uint8(padded_image_np*255))

        plt.figure(figsize=(6, 6))
        plt.axis("off")
        _ = plt.imshow(image_PIL)
        plt.show()

        image_parts.append(preprocess(image_PIL))

        part_PIL = Image.fromarray(np.uint8(cutout_image_np*255))
        w, h = part_PIL.size
        if w > h and w > 512:
            part_pil = part_PIL.resize((512, int(h*512/w)), Image.BICUBIC)
        elif h > 512:
            part_pil = part_PIL.resize((int(w*512/h), 512), Image.BICUBIC)

        parts_rgb.append(preprocess_parts(part_PIL))
        mask_PIL = Image.fromarray(np.uint8(cutout_mask_np*255))
        parts_a.append(preprocess_parts(mask_PIL))
        part_sizes[part_count] = num_mask_pixels
        count += 1
        part_count += 1

to_pil = T.ToPILImage()

num_parts = min(len(image_parts)//2,100)

part_input = torch.tensor(np.stack(image_parts)).cuda()
with torch.no_grad():
    part_features = clip_model.encode_image(part_input).float().cpu()  
part_features /= part_features.norm(dim=-1, keepdim=True)

feature_text = "<|startoftext|> Image of " + prompt + " <|endoftext|>"
query_features = get_text_features(feature_text)

part_similarity = query_features.cpu().numpy() @ part_features.numpy().T
part_similarity = part_similarity[0]

part_scores, part_indices = get_top_N_semantic_similarity(part_similarity, N=num_parts)
columns = 5
rows = num_parts // columns + 1
fig=plt.figure(figsize=(columns*5, rows*5))
for i in range(1, columns*rows + 1):
    img = to_pil(image_parts[part_indices[i-1]])
    fig.add_subplot(rows, columns, i)
    plt.margins(y=10)
    plt.imshow(img)
    plt.text(0, -5, str(i-1), fontsize=12)
    plt.axis("off")
    if i >= num_parts:
        break
plt.show()

ordered_part_indices = []
for p in part_indices[:num_parts]:
    size = part_sizes[p]
    ordered_part_indices.append((size, p))

ordered_part_indices.sort(reverse=True)
314/6: import requests
314/7:
from PIL import Image
import os
import requests
all_images, all_files = [],[]
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
#     img = img.convert(mode="RGB")
    b, g, r = img.split()
    img = Image.merge("RGB", (r, g, b))
    all_images.append(img)
    all_files.append(i)

# get the images Wikipedia
s = requests.Session()
url = "https://commons.wikimedia.org/w/api.php"
params = {
  "action": "query",
  "generator": "images",
  "prop": "imageinfo",
  "gimlimit": 500,
  "titles": keywords[j],
  "iiprop": "url|dimensions",
  "format": "json"
}
r = s.get(url=url, params=params)
data = r.json()
image_files = []
if "query" not in data.keys():
    continue
pages = data['query']['pages']
for k, v in pages.items():
    for info in v['imageinfo']:
        imurl = info["url"]
        h =  info["height"]
        w = info["width"]
        a = h * w
        if a >= 512*512 and imurl not in image_files and imurl.lower().endswith("jpg"):
            image_files.append(imurl)
random.shuffle(image_files)
for im in image_files[:num_images]:
    filename = im.split("/")[-1]
    download_file(im, "wiki_images")
    file_path = "wiki_images/" + filename
    if file_path not in all_files:
        # print(file_path)
        img = Image.open(file_path)
        img = img.convert(mode="RGB")
        all_images.append(img)
        all_files.append(file_path)
        num_wikiimages += 1

print("num openimages  ", num_openimages)
print("num wiki images ", num_wikiimages)
print("num total images", num_openimages+num_wikiimages)

input_resolution = 224
image_features = torch.empty((0, 512))

preprocess = Compose([
    Resize(input_resolution, interpolation=InterpolationMode.BICUBIC),
    CenterCrop(input_resolution),
    ToTensor()
])

images = [preprocess(im) for im in all_images]
image_input = torch.tensor(np.stack(images)).cuda()
with torch.no_grad():
    image_features = clip_model.encode_image(image_input).float().cpu()  
image_features /= image_features.norm(dim=-1, keepdim=True)

feature_text = "<|startoftext|> Image of " + prompt + " <|endoftext|>"
query_features = get_text_features(feature_text)

image_similarity = query_features.cpu().numpy() @ image_features.numpy().T
image_similarity = image_similarity[0]
print(len(all_files))

num_images = min(int(0.75*len(all_files)), 25)
image_scores, image_indices = get_top_N_semantic_similarity(image_similarity, N=num_images)
columns = 5
rows = num_images // columns + 1
fig=plt.figure(figsize=(columns*5, rows*5))
for i in range(1, columns*rows + 1):
    file_name = all_files[image_indices[i-1]]
    img = Image.open(file_name)
    img = img.convert(mode="RGB")
    fig.add_subplot(rows, columns, i)
    plt.margins(y=10)
    plt.imshow(img)
    plt.text(0, -30, str(i) + " " + file_name, fontsize=10)
    plt.axis("off")
    if i >= num_images:
        break
plt.show()

image_parts = []
parts_rgb = []
parts_a = []
part_sizes = {}
part_count = 0

preprocess_parts = Compose([
    ToTensor()
])

for i in range(num_images):
    image_file = all_files[image_indices[i]]
    print(i, file_name)

    input_image = Image.open(image_file).convert(mode="RGB")
    plt.figure(figsize=(10, 10))
    plt.axis("off")
    _ = plt.imshow(input_image)
    plt.show()

    result = inference_detector(groie_model, image_file)
    bbox_result, segm_result = result

    boxes = []
    overlaps = []
    scores = []
    labels = []
    mask_areas = []
    result_image = np.array(input_image.copy())
    count = 0

    # print()
    # print("objects")

    for label, boxscores in enumerate(bbox_result):
        for boxscore in boxscores:
            box = boxscore[:4]
            score = boxscore[4]

            overlapping = False
            for b in boxes:
                overlap = calculate_iou(box, b)
                # print("overlap", overlap)
                if  overlap > 0.85:
                    overlapping = True
                    # print("skipping")
                    break

            overlaps.append(overlapping)

            boxes.append(box)
            scores.append(score)
            labels.append(label)
            # print(label, coco_names[label], box, score)

            if overlapping:
                continue

            color = random.choice(colors)
            # print(count+1, coco_names[label], round(100*score.item(), 2))
            # draw box
            tl = round(0.001 * max(result_image.shape[0:2])) + 1  # line thickness
            c1, c2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))
            cv2.rectangle(result_image, c1, c2, color, thickness=tl)
            # draw text
            display_txt = "%s: %.1f%%" % (coco_names[label], 100*score)
            tf = max(tl - 1, 1)  # font thickness
            t_size = cv2.getTextSize(display_txt, 0, fontScale=tl / 3, thickness=tf)[0]
            c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3
            cv2.rectangle(result_image, c1, c2, color, -1)  # filled
            cv2.putText(result_image, display_txt, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)
            count += 1

    if count == 0:
        print("no objects found")
        continue

    plt.figure(figsize=(10, 10))
    plt.axis("off")
    _ = plt.imshow(result_image)
    plt.show()

    count = 0
    masks = []
    mask_accum = None
    for object_masks in segm_result:
        for mask in object_masks:
            mask_np = np.float32(mask)
            masks.append(mask_np)
            mask_area = mask_np.sum() / (input_image.width*input_image.height)
            mask_areas.append(mask_area)
            if mask_accum is None:
                mask_accum = mask
            else:
                mask_accum = np.maximum(mask_accum, mask)
            count += 1

    images = []

    img_wid = input_image.width
    img_hgt = input_image.height

    count = 0
    for box, overlapping, score, mask_np in zip(boxes, overlaps, scores, masks):
        # print(box)

        mask_np = np.expand_dims(mask_np, axis=2)
        num_mask_pixels = mask_np.sum()

        if overlapping or num_mask_pixels < 1000:
            continue

        mask_np = np.repeat(mask_np, 3, axis=2)
        image_np = np.array(input_image, dtype=np.float32)/255.0

        if mask_np.shape != image_np.shape:
            continue

        masked_image_np = mask_np * image_np

        box_lft = int(box[0].item())
        box_top = int(box[1].item())
        box_rgt = int(box[2].item())
        box_bot = int(box[3].item())

        cutout_image_np = masked_image_np[box_top:box_bot, box_lft:box_rgt]
        cutout_mask_np = mask_np[box_top:box_bot, box_lft:box_rgt]

        box_wid = box_rgt - box_lft
        box_hgt = box_bot - box_top

        if box_wid > box_hgt: # handle landscape images
            # print("landscape")
            pad = (box_wid - box_hgt) // 2
            padded_image_np = np.zeros((box_wid, box_wid, 3), dtype=cutout_image_np.dtype)
            padded_image_np[pad:pad+box_hgt, :] = cutout_image_np

        else: # handle portrait images
            # print("portrait")
            pad = (box_hgt - box_wid) // 2
            padded_image_np = np.zeros((box_hgt, box_hgt, 3), dtype=cutout_image_np.dtype)
            padded_image_np[:, pad:pad+box_wid] = cutout_image_np     

        image_PIL = Image.fromarray(np.uint8(padded_image_np*255))

        plt.figure(figsize=(6, 6))
        plt.axis("off")
        _ = plt.imshow(image_PIL)
        plt.show()

        image_parts.append(preprocess(image_PIL))

        part_PIL = Image.fromarray(np.uint8(cutout_image_np*255))
        w, h = part_PIL.size
        if w > h and w > 512:
            part_pil = part_PIL.resize((512, int(h*512/w)), Image.BICUBIC)
        elif h > 512:
            part_pil = part_PIL.resize((int(w*512/h), 512), Image.BICUBIC)

        parts_rgb.append(preprocess_parts(part_PIL))
        mask_PIL = Image.fromarray(np.uint8(cutout_mask_np*255))
        parts_a.append(preprocess_parts(mask_PIL))
        part_sizes[part_count] = num_mask_pixels
        count += 1
        part_count += 1

to_pil = T.ToPILImage()

num_parts = min(len(image_parts)//2,100)

part_input = torch.tensor(np.stack(image_parts)).cuda()
with torch.no_grad():
    part_features = clip_model.encode_image(part_input).float().cpu()  
part_features /= part_features.norm(dim=-1, keepdim=True)

feature_text = "<|startoftext|> Image of " + prompt + " <|endoftext|>"
query_features = get_text_features(feature_text)

part_similarity = query_features.cpu().numpy() @ part_features.numpy().T
part_similarity = part_similarity[0]

part_scores, part_indices = get_top_N_semantic_similarity(part_similarity, N=num_parts)
columns = 5
rows = num_parts // columns + 1
fig=plt.figure(figsize=(columns*5, rows*5))
for i in range(1, columns*rows + 1):
    img = to_pil(image_parts[part_indices[i-1]])
    fig.add_subplot(rows, columns, i)
    plt.margins(y=10)
    plt.imshow(img)
    plt.text(0, -5, str(i-1), fontsize=12)
    plt.axis("off")
    if i >= num_parts:
        break
plt.show()

ordered_part_indices = []
for p in part_indices[:num_parts]:
    size = part_sizes[p]
    ordered_part_indices.append((size, p))

ordered_part_indices.sort(reverse=True)
314/8:
from PIL import Image
import os
import requests
all_images, all_files = [],[]
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
#     img = img.convert(mode="RGB")
    b, g, r = img.split()
    img = Image.merge("RGB", (r, g, b))
    all_images.append(img)
    all_files.append(i)

# get the images Wikipedia
s = requests.Session()
url = "https://commons.wikimedia.org/w/api.php"
params = {
  "action": "query",
  "generator": "images",
  "prop": "imageinfo",
  "gimlimit": 500,
  "titles": keywords[j],
  "iiprop": "url|dimensions",
  "format": "json"
}
r = s.get(url=url, params=params)
data = r.json()
image_files = []
if "query" not in data.keys():
    continue
pages = data['query']['pages']
for k, v in pages.items():
    for info in v['imageinfo']:
        imurl = info["url"]
        h =  info["height"]
        w = info["width"]
        a = h * w
        if a >= 512*512 and imurl not in image_files and imurl.lower().endswith("jpg"):
            image_files.append(imurl)
random.shuffle(image_files)
for im in image_files[:num_images]:
    filename = im.split("/")[-1]
    download_file(im, "wiki_images")
    file_path = "wiki_images/" + filename
    if file_path not in all_files:
        # print(file_path)
        img = Image.open(file_path)
        img = img.convert(mode="RGB")
        all_images.append(img)
        all_files.append(file_path)
        num_wikiimages += 1

print("num openimages  ", num_openimages)
print("num wiki images ", num_wikiimages)
print("num total images", num_openimages+num_wikiimages)

input_resolution = 224
image_features = torch.empty((0, 512))

preprocess = Compose([
    Resize(input_resolution, interpolation=InterpolationMode.BICUBIC),
    CenterCrop(input_resolution),
    ToTensor()
])

images = [preprocess(im) for im in all_images]
image_input = torch.tensor(np.stack(images)).cuda()
with torch.no_grad():
    image_features = clip_model.encode_image(image_input).float().cpu()  
image_features /= image_features.norm(dim=-1, keepdim=True)

feature_text = "<|startoftext|> Image of " + prompt + " <|endoftext|>"
query_features = get_text_features(feature_text)

image_similarity = query_features.cpu().numpy() @ image_features.numpy().T
image_similarity = image_similarity[0]
print(len(all_files))

num_images = min(int(0.75*len(all_files)), 25)
image_scores, image_indices = get_top_N_semantic_similarity(image_similarity, N=num_images)
columns = 5
rows = num_images // columns + 1
fig=plt.figure(figsize=(columns*5, rows*5))
for i in range(1, columns*rows + 1):
    file_name = all_files[image_indices[i-1]]
    img = Image.open(file_name)
    img = img.convert(mode="RGB")
    fig.add_subplot(rows, columns, i)
    plt.margins(y=10)
    plt.imshow(img)
    plt.text(0, -30, str(i) + " " + file_name, fontsize=10)
    plt.axis("off")
    if i >= num_images:
        break
plt.show()

image_parts = []
parts_rgb = []
parts_a = []
part_sizes = {}
part_count = 0

preprocess_parts = Compose([
    ToTensor()
])

for i in range(num_images):
    image_file = all_files[image_indices[i]]
    print(i, file_name)

    input_image = Image.open(image_file).convert(mode="RGB")
    plt.figure(figsize=(10, 10))
    plt.axis("off")
    _ = plt.imshow(input_image)
    plt.show()

    result = inference_detector(groie_model, image_file)
    bbox_result, segm_result = result

    boxes = []
    overlaps = []
    scores = []
    labels = []
    mask_areas = []
    result_image = np.array(input_image.copy())
    count = 0

    # print()
    # print("objects")

    for label, boxscores in enumerate(bbox_result):
        for boxscore in boxscores:
            box = boxscore[:4]
            score = boxscore[4]

            overlapping = False
            for b in boxes:
                overlap = calculate_iou(box, b)
                # print("overlap", overlap)
                if  overlap > 0.85:
                    overlapping = True
                    # print("skipping")
                    break

            overlaps.append(overlapping)

            boxes.append(box)
            scores.append(score)
            labels.append(label)
            # print(label, coco_names[label], box, score)

            if overlapping:
                continue

            color = random.choice(colors)
            # print(count+1, coco_names[label], round(100*score.item(), 2))
            # draw box
            tl = round(0.001 * max(result_image.shape[0:2])) + 1  # line thickness
            c1, c2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))
            cv2.rectangle(result_image, c1, c2, color, thickness=tl)
            # draw text
            display_txt = "%s: %.1f%%" % (coco_names[label], 100*score)
            tf = max(tl - 1, 1)  # font thickness
            t_size = cv2.getTextSize(display_txt, 0, fontScale=tl / 3, thickness=tf)[0]
            c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3
            cv2.rectangle(result_image, c1, c2, color, -1)  # filled
            cv2.putText(result_image, display_txt, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)
            count += 1

    if count == 0:
        print("no objects found")
        continue

    plt.figure(figsize=(10, 10))
    plt.axis("off")
    _ = plt.imshow(result_image)
    plt.show()

    count = 0
    masks = []
    mask_accum = None
    for object_masks in segm_result:
        for mask in object_masks:
            mask_np = np.float32(mask)
            masks.append(mask_np)
            mask_area = mask_np.sum() / (input_image.width*input_image.height)
            mask_areas.append(mask_area)
            if mask_accum is None:
                mask_accum = mask
            else:
                mask_accum = np.maximum(mask_accum, mask)
            count += 1

    images = []

    img_wid = input_image.width
    img_hgt = input_image.height

    count = 0
    for box, overlapping, score, mask_np in zip(boxes, overlaps, scores, masks):
        # print(box)

        mask_np = np.expand_dims(mask_np, axis=2)
        num_mask_pixels = mask_np.sum()

        if overlapping or num_mask_pixels < 1000:
            continue

        mask_np = np.repeat(mask_np, 3, axis=2)
        image_np = np.array(input_image, dtype=np.float32)/255.0

        if mask_np.shape != image_np.shape:
            continue

        masked_image_np = mask_np * image_np

        box_lft = int(box[0].item())
        box_top = int(box[1].item())
        box_rgt = int(box[2].item())
        box_bot = int(box[3].item())

        cutout_image_np = masked_image_np[box_top:box_bot, box_lft:box_rgt]
        cutout_mask_np = mask_np[box_top:box_bot, box_lft:box_rgt]

        box_wid = box_rgt - box_lft
        box_hgt = box_bot - box_top

        if box_wid > box_hgt: # handle landscape images
            # print("landscape")
            pad = (box_wid - box_hgt) // 2
            padded_image_np = np.zeros((box_wid, box_wid, 3), dtype=cutout_image_np.dtype)
            padded_image_np[pad:pad+box_hgt, :] = cutout_image_np

        else: # handle portrait images
            # print("portrait")
            pad = (box_hgt - box_wid) // 2
            padded_image_np = np.zeros((box_hgt, box_hgt, 3), dtype=cutout_image_np.dtype)
            padded_image_np[:, pad:pad+box_wid] = cutout_image_np     

        image_PIL = Image.fromarray(np.uint8(padded_image_np*255))

        plt.figure(figsize=(6, 6))
        plt.axis("off")
        _ = plt.imshow(image_PIL)
        plt.show()

        image_parts.append(preprocess(image_PIL))

        part_PIL = Image.fromarray(np.uint8(cutout_image_np*255))
        w, h = part_PIL.size
        if w > h and w > 512:
            part_pil = part_PIL.resize((512, int(h*512/w)), Image.BICUBIC)
        elif h > 512:
            part_pil = part_PIL.resize((int(w*512/h), 512), Image.BICUBIC)

        parts_rgb.append(preprocess_parts(part_PIL))
        mask_PIL = Image.fromarray(np.uint8(cutout_mask_np*255))
        parts_a.append(preprocess_parts(mask_PIL))
        part_sizes[part_count] = num_mask_pixels
        count += 1
        part_count += 1

to_pil = T.ToPILImage()

num_parts = min(len(image_parts)//2,100)

part_input = torch.tensor(np.stack(image_parts)).cuda()
with torch.no_grad():
    part_features = clip_model.encode_image(part_input).float().cpu()  
part_features /= part_features.norm(dim=-1, keepdim=True)

feature_text = "<|startoftext|> Image of " + prompt + " <|endoftext|>"
query_features = get_text_features(feature_text)

part_similarity = query_features.cpu().numpy() @ part_features.numpy().T
part_similarity = part_similarity[0]

part_scores, part_indices = get_top_N_semantic_similarity(part_similarity, N=num_parts)
columns = 5
rows = num_parts // columns + 1
fig=plt.figure(figsize=(columns*5, rows*5))
for i in range(1, columns*rows + 1):
    img = to_pil(image_parts[part_indices[i-1]])
    fig.add_subplot(rows, columns, i)
    plt.margins(y=10)
    plt.imshow(img)
    plt.text(0, -5, str(i-1), fontsize=12)
    plt.axis("off")
    if i >= num_parts:
        break
plt.show()

ordered_part_indices = []
for p in part_indices[:num_parts]:
    size = part_sizes[p]
    ordered_part_indices.append((size, p))

ordered_part_indices.sort(reverse=True)
314/9:
resize_factor = 4.0
new_img_size = int(img_size*resize_factor)

img_0 = interp(bg_x.cpu(), bgvals[0].cpu(), bg_xs.cpu()).to(device)
img_1 = interp(bg_x.cpu(), bgvals[1].cpu(), bg_xs.cpu()).to(device)
img_2 = interp(bg_x.cpu(), bgvals[2].cpu(), bg_xs.cpu()).to(device)
img = torch.vstack([img_0, img_1, img_2])

img = img.permute(1,0)
img = img.tile((img_size, 1, 1))
img = img.unsqueeze(0)
img = img.permute(0, 3, 2, 1) # NHWC -> NCHW
img = torch.nn.functional.interpolate(img, scale_factor=resize_factor, mode="bilinear")

for index, params in zip(ordered_part_indices, partvals):
    # get the part
    part = parts_rgb[index[1]].to(device)
    mask = parts_a[index[1]].to(device)

    # scale
    scale_factor = torch.tensor([new_img_size/2000.0, new_img_size/2000.0]).to(device)
    part = kornia.geometry.transform.scale(part[None, :], scale_factor[None, :]).squeeze()
    mask = kornia.geometry.transform.scale(mask[None, :], scale_factor[None, :]).squeeze()

    # pad
    h = part.shape[1]
    w = part.shape[2]
    lft_pad = (new_img_size - w)//2
    top_pad = (new_img_size - h)//2
    rgt_pad = new_img_size - w - lft_pad
    bot_pad = new_img_size - h - top_pad
    part = T.functional.pad(part, (lft_pad, top_pad, rgt_pad, bot_pad))
    mask = T.functional.pad(mask, (lft_pad, top_pad, rgt_pad, bot_pad))

    # translate
    # trans = (params-0.5) * 150 * resize_factor
    trans = params * new_img_size
    part = kornia.geometry.transform.translate(part[None, :], trans[None, :]).squeeze()
    mask = kornia.geometry.transform.translate(mask[None, :], trans[None, :]).squeeze()

    # composite the part
    img *= 1-mask
    img += part

image = img.detach().cpu().numpy()
image = np.transpose(image, (0, 2, 3, 1))[0]
image = np.clip(image*255, 0, 255).astype(np.uint8)
image_pil = Image.fromarray(image)
image_pil.save("out.png")

import IPython
IPython.display.Image("out.png")
314/10:
num_steps = 100
color_lr = 0.005
parts_lr = 0.01
num_augmentations = 32

text_features = get_text_features(prompt)

augment_trans = T.Compose([
  T.RandomPerspective(fill=1, p=1, distortion_scale=0.5),
  T.RandomResizedCrop(img_size, scale=(0.7,0.9)),
  T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),
  T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))
])

bg_x = torch.linspace(0, img_size-1, num_ctrl_ponts).to(device)
bgvals = (0.5 + init_rand_amount/2.0 * torch.rand(size=(3, num_ctrl_ponts))).to(device) 
bgvals.requires_grad = True
bg_xs = torch.linspace(0, img_size-1, img_size).to(device)
# print(bgvals)

partvals = param_list[layout_indices[0]].clone()
partvals.requires_grad = True

bg_optim = torch.optim.Adam([{'params': bgvals,   'lr': color_lr},
                             {'params': partvals, 'lr': parts_lr}])
loss_fn = torch.nn.CosineEmbeddingLoss()
target = torch.full((1,32), fill_value=1.0).squeeze().to(device)

# Run the main optimization loop
for t in range(num_steps+1):
    bg_optim.zero_grad()

    img_0 = interp(bg_x.cpu(), bgvals[0].cpu(), bg_xs.cpu()).to(device)
    img_1 = interp(bg_x.cpu(), bgvals[1].cpu(), bg_xs.cpu()).to(device)
    img_2 = interp(bg_x.cpu(), bgvals[2].cpu(), bg_xs.cpu()).to(device)
    img = torch.vstack([img_0, img_1, img_2])

    img = img.permute(1,0)
    img = img.tile((img_size, 1, 1))
    img = img.unsqueeze(0)
    img = img.permute(0, 3, 2, 1) # NHWC -> NCHW

    for index, params in zip(ordered_part_indices[:num_shapes], partvals):
        # get the part
        part = parts_rgb[index[1]].to(device)
        mask = parts_a[index[1]].to(device)

        # scale
        scale_factor = torch.tensor([img_size/2000.0, img_size/2000.0]).to(device)
        part = kornia.geometry.transform.scale(part[None, :], scale_factor[None, :]).squeeze()
        mask = kornia.geometry.transform.scale(mask[None, :], scale_factor[None, :]).squeeze()

        # pad
        h = part.shape[1]
        w = part.shape[2]
        lft_pad = (img_size - w)//2
        top_pad = (img_size - h)//2
        rgt_pad = img_size - w - lft_pad
        bot_pad = img_size - h - top_pad
        part = T.functional.pad(part, (lft_pad, top_pad, rgt_pad, bot_pad))
        mask = T.functional.pad(mask, (lft_pad, top_pad, rgt_pad, bot_pad))

        # translate
        # trans = (params-0.5) * 150
        trans = params * img_size
        part = kornia.geometry.transform.translate(part[None, :], trans[None, :]).squeeze()
        mask = kornia.geometry.transform.translate(mask[None, :], trans[None, :]).squeeze()

        # composite the part
        img *= 1-mask
        img += part

    img_augs = []
    for n in range(num_augmentations):
        img_augs.append(augment_trans(img))
    im_batch = torch.cat(img_augs)
    image_features = clip_model.encode_image(im_batch)
    loss = loss_fn(image_features, text_features, target)

    loss.backward()
    bg_optim.step()
    if t % 10 == 0:
        print("-" * 10)
        image = img.detach().cpu().numpy()
        image = np.transpose(image, (0, 2, 3, 1))[0]
        image = np.clip(image*255, 0, 255).astype(np.uint8)
        image_pil = Image.fromarray(image)
        print('render loss:', loss.item())
        print('iteration:', t)
        plt.figure(figsize=(5, 5))
        img = plt.imshow(image_pil)
        plt.axis('off')
        plt.show()
314/11:
from PIL import Image, ImageDraw
import random

# Define the dimensions of the final collage
canvas_width = 2000
canvas_height = 2000

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
x, y = 500, 1000
shapes = [
    ((0, 0), (x, x)),
    ((x, 0), (y, x)),
    ((0, x), (x, y)),
    ((x, x), (y, y))
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Load the images to be used in the collage
# images = []
# for i in os.listdir('Examples5'):
#     img = Image.open('Examples5/'+i)
#     images.append(img)
images = [
    Image.open('Examples5/istockphoto-1287395025-612x612.jpg'),
    Image.open('Examples5/pexels-photo-3183172.jpeg'),
    Image.open('Examples5/pexels-photo-5971184.jpeg'),
    Image.open('Examples5/pexels-photo-14025704.jpeg')
#     Image.open('Examples5/pexels-photo-3580656.jpeg')
]

# Paste the images onto the canvas
for i in range(len(images)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('collage3.jpg')
315/1:
import discord import os

client discord.Client()
@client.event
async def on_ready():
    print('We have logged in as {0.user} '.format(client))
@client.event
async def on_message(message):
    if message.author == client.user:
        return
    if message.content.startswith('Shello'): await message.channel.send('Hello!')
client.run('MTA5MzQ0Njk1MjM2NjEyNTA3Ng.GIMtme.AZfMd_-vBSWm5kcHKwyMkcEdKnJfFkFMBP1Vqw')
315/2:
import discord
import os

client discord.Client()
@client.event
async def on_ready():
    print('We have logged in as {0.user} '.format(client))
@client.event
async def on_message(message):
    if message.author == client.user:
        return
    if message.content.startswith('Shello'): await message.channel.send('Hello!')
client.run('MTA5MzQ0Njk1MjM2NjEyNTA3Ng.GIMtme.AZfMd_-vBSWm5kcHKwyMkcEdKnJfFkFMBP1Vqw')
315/3:
import discord
import os

client = discord.Client()
@client.event
async def on_ready():
    print('We have logged in as {0.user} '.format(client))
@client.event
async def on_message(message):
    if message.author == client.user:
        return
    if message.content.startswith('Shello'): await message.channel.send('Hello!')
client.run('MTA5MzQ0Njk1MjM2NjEyNTA3Ng.GIMtme.AZfMd_-vBSWm5kcHKwyMkcEdKnJfFkFMBP1Vqw')
315/4:
import discord
import os

client = discord.Client(intents=discord.Intents.default())
@client.event
async def on_ready():
    print('We have logged in as {0.user} '.format(client))
@client.event
async def on_message(message):
    if message.author == client.user:
        return
    if message.content.startswith('Shello'): await message.channel.send('Hello!')
client.run('MTA5MzQ0Njk1MjM2NjEyNTA3Ng.GIMtme.AZfMd_-vBSWm5kcHKwyMkcEdKnJfFkFMBP1Vqw')
316/1:
for i in range(4):
    i
316/2:
for i in range(4):
    print(i)
317/1:
imgs = os.listdir('output')
imgs
317/2:
import os
imgs = os.listdir('output')
imgs
317/3:
imgs.pop[0]
imgs
317/4:
imgs.pop(0)
imgs
317/5:
import os
import random
imgs = os.listdir('output')
random.choice(imgs)
317/6:
import os
import random
imgs = os.listdir('output')
random.choice(imgs)
317/7:
import os
import random
imgs = os.listdir('output')
random.choice(imgs)
317/8:
import os
import random
imgs = os.listdir('output')
random.choice(imgs)
317/9:
import os
import random
imgs = os.listdir('output')
random.choice(imgs)
317/10:
import os
import random
imgs = os.listdir('output')
random.choice(imgs)
317/11:
import os
import random
imgs = os.listdir('output')
random.choice(imgs)
317/12:
import os
import random
imgs = os.listdir('output')
random.choice(imgs)
317/13:
import os
import random
imgs = os.listdir('output')
random.choice(imgs)
317/14:
import os
import random
imgs = os.listdir('output')
random.choice(imgs)
317/15:
import os
import random
imgs = os.listdir('output')
random.choice(imgs)
317/16:
import os
import random
imgs = os.listdir('output')
random.choice(imgs)
317/17:
import os
import random
imgs = os.listdir('output')
random.choice(imgs)
317/18:
from PIL import Image
import glob

# Create the frames
frames = []
imgs = glob.glob("output/*.png")
for i in imgs:
    new_frame = Image.open(i)
    frames.append(new_frame)

# Save into a GIF file that loops forever
frames[0].save('png_to_gif.gif', format='GIF',
    append_images=frames[1:],
    save_all=True,
    duration=300, Loop=0)
317/19:
from PIL import Image
import glob

# Create the frames
frames = []
imgs = glob.glob("output/*.png")
for i in imgs:
    new_frame = Image.open(i)
    frames.append(new_frame)

# Save into a GIF file that loops forever
frames[0].save('png_to_gif.gif', format='GIF',
    append_images=frames[1:],
    save_all=True,
    duration=1000, Loop=0)
317/20:
c = '2 min'
c.split(' ')
317/21:
c = '2 min'
int(c.split(' ')[0])
318/1:
c = 'http://localhost:8888/notebooks/HACKERSPACE/Discord_Midjourney/Untitled.ipynb#'
c.split('/')[-1]
318/2:
import shutil
shutil.move('output', 'surprise')
318/3:
import shutil
import os

for r in os.listdir('output'):
    shutil.move('output/'+r, 'surprise')
318/4:
import shutil
import os

for r in os.listdir('output'):
    shutil.move('output/'+r, 'surprise')
318/5:
import shutil
import os

for r in os.listdir('output'):
    shutil.move('output/'+r, 'surprise')
318/6: os.scandir('surprise')
318/7: any(os.scandir('surprise'))
318/8: any(os.scandir('output'))
319/1:
import os
import tkinter as tk
from PIL import Image, ImageTk

# Set the folder path containing the PNG images
folder_path = "output"

# Create a list of the PNG file paths in the folder
png_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(".png")]

# Initialize the tkinter window
root = tk.Tk()
root.attributes("-fullscreen", True)  # Set the window to full-screen mode

# Set up the slideshow
slideshow_images = []
for png_file in png_files:
    img = Image.open(png_file)
    img = img.resize((root.winfo_screenwidth(), root.winfo_screenheight()))  # Resize the image to fit the screen
    img_tk = ImageTk.PhotoImage(img)
    slideshow_images.append(img_tk)

# Create the slideshow label
slideshow_label = tk.Label(root)
slideshow_label.pack(fill=tk.BOTH, expand=True)

# Define the slideshow function
def slideshow(index):
    slideshow_label.config(image=slideshow_images[index])
    root.after(5000, slideshow, (index + 1) % len(slideshow_images))

# Start the slideshow
slideshow(0)

# Run the tkinter event loop
root.mainloop()
319/2:
import os
import tkinter as tk
from PIL import Image, ImageTk

# Set the folder path containing the PNG images
folder_path = "output"

# Create a list of the PNG file paths in the folder
png_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(".png")]

# Initialize the tkinter window
root = tk.Tk()
root.attributes("-fullscreen", True)  # Set the window to full-screen mode

# Set up the slideshow
slideshow_images = []
for png_file in png_files:
    img = Image.open(png_file)
    #img = img.resize((root.winfo_screenwidth(), root.winfo_screenheight()))  # Resize the image to fit the screen
    img_tk = ImageTk.PhotoImage(img)
    slideshow_images.append(img_tk)

# Create the slideshow label
slideshow_label = tk.Label(root)
slideshow_label.pack(fill=tk.BOTH, expand=True)

# Define the slideshow function
def slideshow(index):
    slideshow_label.config(image=slideshow_images[index])
    root.after(5000, slideshow, (index + 1) % len(slideshow_images))

# Start the slideshow
slideshow(0)

# Run the tkinter event loop
root.mainloop()
319/3:
import os
import tkinter as tk
from PIL import Image, ImageTk

# Set the folder path containing the PNG images
folder_path = "output"

# Create a list of the PNG file paths in the folder
png_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(".png")]

# Initialize the tkinter window
root = tk.Tk()
root.attributes("-fullscreen", True)  # Set the window to full-screen mode

# Set up the slideshow
slideshow_images = []
for png_file in png_files:
    img = Image.open(png_file)
    #img = img.resize((root.winfo_screenwidth(), root.winfo_screenheight()))  # Resize the image to fit the screen
    img_tk = ImageTk.PhotoImage(img)
    slideshow_images.append(img_tk)

# Create the slideshow label
slideshow_label = tk.Label(root)
slideshow_label.pack(fill=tk.BOTH, expand=True)

# Define the slideshow function
def slideshow(index):
    slideshow_label.config(image=slideshow_images[index])
    root.after(5000, slideshow, (index + 1) % len(slideshow_images))

# Start the slideshow
slideshow(0)

# Run the tkinter event loop
root.mainloop()
319/4:
import os
import tkinter as tk
from PIL import Image, ImageTk

# Set the folder path containing the PNG images
folder_path = "output"

# Create a list of the PNG file paths in the folder
png_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(".png")]

# Initialize the tkinter window
root = tk.Tk()
root.attributes("-fullscreen", True)  # Set the window to full-screen mode

# Set up the slideshow
slideshow_images = []
for png_file in png_files:
    img = Image.open(png_file)
    #img = img.resize((root.winfo_screenwidth(), root.winfo_screenheight()))  # Resize the image to fit the screen
    img_tk = ImageTk.PhotoImage(img)
    slideshow_images.append(img_tk)

# Create the slideshow label
slideshow_label = tk.Label(root)
slideshow_label.pack(fill=tk.BOTH, expand=True)

# Define the slideshow function
def slideshow(index):
    slideshow_label.config(image=slideshow_images[index])
    root.after(5000, slideshow, (index + 1) % len(slideshow_images))

# Start the slideshow
slideshow(0)

# Run the tkinter event loop
root.mainloop()
320/1:
i = 'https://cdn.discordapp.com/attachments/1088289456794124390/1095329243946291270/DOS_BOOT_a_modified_old_mercedies_car_black_car_shiny_latest_up_042484bc-42a2-4971-99cd-bee99eecce33.png'
i.split('/')[-1]
321/1:
for i in range(4):
    print(i)
321/2:
for i in range(1,4):
    print(i)
321/3:
for i in range(1,5):
    print(i)
322/1:
from PIL import Image, ImageDraw
import random

# Define the dimensions of the final collage
canvas_width = 2000
canvas_height = 2000

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
x, y = 500, 1000
shapes = [
    ((0, 0), (x, x)),
    ((x, 0), (y, x)),
    ((0, x), (x, y)),
    ((x, x), (y, y))
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Load the images to be used in the collage
images = []
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    images.append(img)
# images = [
#     Image.open('Examples5/istockphoto-1287395025-612x612.jpg'),
#     Image.open('Examples5/pexels-photo-3183172.jpeg'),
#     Image.open('Examples5/pexels-photo-5971184.jpeg'),
#     Image.open('Examples5/pexels-photo-14025704.jpeg')
# #     Image.open('Examples5/pexels-photo-3580656.jpeg')
# ]

# Paste the images onto the canvas
for i in range(len(images)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('collage3.jpg')
323/1:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

# Define the dimensions of the final collage
canvas_width = 3000
canvas_height = 3000

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
x, y, z = 500, 1000, 1500
shapes = [
    ((0, 0), (x, x)), ((x, 0), (y, x)), ((y, 0), (z, x)),
    ((0, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z))
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Load the images to be used in the collage
images = []
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    images.append(img)
# images = [
#     Image.open('Examples5/istockphoto-1287395025-612x612.jpg'),
#     Image.open('Examples5/pexels-photo-3183172.jpeg'),
#     Image.open('Examples5/pexels-photo-5971184.jpeg'),
#     Image.open('Examples5/pexels-photo-14025704.jpeg')
# #     Image.open('Examples5/pexels-photo-3580656.jpeg')
# ]

# Paste the images onto the canvas
for i in range(len(images)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('collage4.jpg')
323/2:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

# Define the dimensions of the final collage
canvas_width = 3000
canvas_height = 3000

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
x, y, z = 500, 1000, 1500
shapes = [
    ((0, 0), (x, x)), ((x, 0), (y, x)), ((y, 0), (z, x)),
    ((0, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z))
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Load the images to be used in the collage
images = []
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    images.append(img)
# images = [
#     Image.open('Examples5/istockphoto-1287395025-612x612.jpg'),
#     Image.open('Examples5/pexels-photo-3183172.jpeg'),
#     Image.open('Examples5/pexels-photo-5971184.jpeg'),
#     Image.open('Examples5/pexels-photo-14025704.jpeg')
# #     Image.open('Examples5/pexels-photo-3580656.jpeg')
# ]

# Paste the images onto the canvas
for i in range(len(shapes)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('collage4.jpg')
323/3:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

# Define the dimensions of the final collage
canvas_width = 1800
canvas_height = 1800

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
x, y, z = 300, 600, 900
shapes = [
    ((0, 0), (x, x)), ((x, 0), (y, x)), ((y, 0), (z, x)),
    ((0, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z))
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Load the images to be used in the collage
images = []
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    images.append(img)
# images = [
#     Image.open('Examples5/istockphoto-1287395025-612x612.jpg'),
#     Image.open('Examples5/pexels-photo-3183172.jpeg'),
#     Image.open('Examples5/pexels-photo-5971184.jpeg'),
#     Image.open('Examples5/pexels-photo-14025704.jpeg')
# #     Image.open('Examples5/pexels-photo-3580656.jpeg')
# ]

# Paste the images onto the canvas
for i in range(len(shapes)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('collage4.jpg')
323/4:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

# Define the dimensions of the final collage
canvas_width = 3000
canvas_height = 3000

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
x, y, z = 500, 1000, 1500
shapes = [
    ((0, 0), (x, x)), ((x, 0), (y, x)), ((y, 0), (z, x)),
    ((0, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z))
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Load the images to be used in the collage
images = []
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    images.append(img)
# images = [
#     Image.open('Examples5/istockphoto-1287395025-612x612.jpg'),
#     Image.open('Examples5/pexels-photo-3183172.jpeg'),
#     Image.open('Examples5/pexels-photo-5971184.jpeg'),
#     Image.open('Examples5/pexels-photo-14025704.jpeg')
# #     Image.open('Examples5/pexels-photo-3580656.jpeg')
# ]

# Paste the images onto the canvas
for i in range(len(shapes)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('collage4.jpg')
323/5:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

# Define the dimensions of the final collage
canvas_width = 4500
canvas_height = 4500

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Load the images to be used in the collage
images = []
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    images.append(img)
# images = [
#     Image.open('Examples5/istockphoto-1287395025-612x612.jpg'),
#     Image.open('Examples5/pexels-photo-3183172.jpeg'),
#     Image.open('Examples5/pexels-photo-5971184.jpeg'),
#     Image.open('Examples5/pexels-photo-14025704.jpeg')
# #     Image.open('Examples5/pexels-photo-3580656.jpeg')
# ]

# Paste the images onto the canvas
for i in range(len(shapes)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('heart.jpg')
324/1:
images, img_size = [],[]
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size = img.size
    images.append(img)
324/2:
import os
images, img_size = [],[]
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size = img.size
    images.append(img)
324/3:
from PIL import Image
import os
images, img_size = [],[]
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size = img.size
    images.append(img)
324/4: img_size[0]
324/5: img_size
324/6:
from PIL import Image
import os
images, img_size = [],[]
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    images.append(img)
324/7: img_size[0]
324/8: images
324/9: images[0]
324/10:
from PIL import Image
import os
images, img_size = [],[]
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size[0])
    images.append(img)
324/11: img_size[0]
324/12: min(img_size)
324/13:
from PIL import Image

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image

normalize_image_size('Examples5/istockphoto-1287395025-612x612.jpg', 500)
324/14:
from PIL import Image

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image

normalize_image_size('Examples5/istockphoto-1287395025-612x612.jpg', 100)
324/15:
from PIL import Image

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image

normalize_image_size('Examples5/istockphoto-1287395025-612x612.jpg', 68)
324/16:
from PIL import Image

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image

normalize_image_size('Examples5/pexels-photo-207896.jpeg', 68)
324/17:
from PIL import Image

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image

normalize_image_size('Examples5/pexels-photo-207896.jpeg', 500)
324/18:
from PIL import Image

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image

normalize_image_size('Examples5/pexels-photo-207896.jpeg', 100)
324/19:
from PIL import Image

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image

normalize_image_size('Examples5/pexels-photo-2536585.jpeg', 100)
324/20:
from PIL import Image

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image

normalize_image_size(r'Examples5\pexels-photo-3693861.jpeg', 100)
324/21: img
324/22: images
324/23:
images.clear()
images
324/24: min(wid)
324/25:

images, wid = [],[]
for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    wid.append(img.size)
324/26: min(wid)
324/27: max(min(wid))
324/28:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image


# Define the dimensions of the final collage
canvas_width = 4500
canvas_height = 4500

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Load the images to be used in the collage
images, img_size, wid = [],[],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    wid.append(img.size[0])

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size = normalize_image_size(img, max(min(wid)))
    images.append(img)
# images = [
#     Image.open('Examples5/istockphoto-1287395025-612x612.jpg'),
#     Image.open('Examples5/pexels-photo-3183172.jpeg'),
#     Image.open('Examples5/pexels-photo-5971184.jpeg'),
#     Image.open('Examples5/pexels-photo-14025704.jpeg')
# #     Image.open('Examples5/pexels-photo-3580656.jpeg')
# ]

# Paste the images onto the canvas
for i in range(len(shapes)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('heart2.jpg')
324/29:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image


# Define the dimensions of the final collage
canvas_width = 4500
canvas_height = 4500

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Load the images to be used in the collage
images, img_size, wid = [],[],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    wid.append(img.size[0])
    
size = max(min(wid))

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)
# images = [
#     Image.open('Examples5/istockphoto-1287395025-612x612.jpg'),
#     Image.open('Examples5/pexels-photo-3183172.jpeg'),
#     Image.open('Examples5/pexels-photo-5971184.jpeg'),
#     Image.open('Examples5/pexels-photo-14025704.jpeg')
# #     Image.open('Examples5/pexels-photo-3580656.jpeg')
# ]

# Paste the images onto the canvas
for i in range(len(shapes)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('heart2.jpg')
324/30:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image


# Define the dimensions of the final collage
canvas_width = 4500
canvas_height = 4500

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)
# images = [
#     Image.open('Examples5/istockphoto-1287395025-612x612.jpg'),
#     Image.open('Examples5/pexels-photo-3183172.jpeg'),
#     Image.open('Examples5/pexels-photo-5971184.jpeg'),
#     Image.open('Examples5/pexels-photo-14025704.jpeg')
# #     Image.open('Examples5/pexels-photo-3580656.jpeg')
# ]

# Paste the images onto the canvas
for i in range(len(shapes)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('heart2.jpg')
324/31:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size*2, size*3, size*4, size*5, size*6, size*7, size*8, size*9
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*10
canvas_height = size*10

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))


for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
for i in range(len(shapes)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('heart2.jpg')
324/32:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*10
canvas_height = size*10

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))


for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
for i in range(len(shapes)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('heart3.jpg')
324/33:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))


for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
for i in range(len(shapes)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('heart3.jpg')
324/34:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

heart = [0,1,1,0,0,1,1,0,
         0,1,1,0,0,1,1,0,
         1,1,1,1,1,1,1,1,
         1,1,1,1,1,1,1,1,
         0,1,1,1,1,1,1,0,
         0,0,1,1,1,1,0,0,
         0,0,0,1,1,1,0,0,
         0,0,0,0,1,0,0,0
        ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
for i in range(len(shapes)):
    if heart == 1:
        canvas.paste(images[i], shapes[i][0])
    else:
        i+=1

# Save the collage
canvas.save('heart4.jpg')
324/35:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

heart = [0,1,1,0,0,1,1,0,
         0,1,1,0,0,1,1,0,
         1,1,1,1,1,1,1,1,
         1,1,1,1,1,1,1,1,
         0,1,1,1,1,1,1,0,
         0,0,1,1,1,1,0,0,
         0,0,0,1,1,1,0,0,
         0,0,0,0,1,0,0,0
        ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
for i in range(len(shapes)):
    if heart[i] == 1:
        canvas.paste(images[i], shapes[i][0])
    else:
        i+=1

# Save the collage
canvas.save('heart4.jpg')
324/36:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

heart = [0,1,1,0,0,1,1,0,
         0,1,1,0,0,1,1,0,
         1,1,1,1,1,1,1,1,
         1,1,1,1,1,1,1,1,
         0,1,1,1,1,1,1,0,
         0,0,1,1,1,1,0,0,
         0,0,0,1,1,1,0,0,
         0,0,0,0,1,0,0,0
        ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
for i,j in zip(len(shapes),len(shapes)):
    if heart[i] == 1:
        canvas.paste(images[j], shapes[i][0])
    else:
        i+=1

# Save the collage
canvas.save('heart4.jpg')
324/37:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

heart = [0,1,1,0,0,1,1,0,
         0,1,1,0,0,1,1,0,
         1,1,1,1,1,1,1,1,
         1,1,1,1,1,1,1,1,
         0,1,1,1,1,1,1,0,
         0,0,1,1,1,1,0,0,
         0,0,0,1,1,1,0,0,
         0,0,0,0,1,0,0,0
        ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
for i,j in zip((0, len(shapes)+1),(0, len(shapes)+1)):
    if heart[i] == 1:
        canvas.paste(images[j], shapes[i][0])
    else:
        i+=1

# Save the collage
canvas.save('heart4.jpg')
324/38:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

heart = [0,1,1,0,0,1,1,0,
         0,1,1,0,0,1,1,0,
         1,1,1,1,1,1,1,1,
         1,1,1,1,1,1,1,1,
         0,1,1,1,1,1,1,0,
         0,0,1,1,1,1,0,0,
         0,0,0,1,1,1,0,0,
         0,0,0,0,1,0,0,0
        ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
for i,j in zip((0, len(shapes)),(0, len(shapes))):
    if heart[i] == 1:
        canvas.paste(images[j], shapes[i][0])
    else:
        i+=1

# Save the collage
canvas.save('heart4.jpg')
324/39: len(shape)
324/40:
for i range(0,4):
    print(i)
324/41:
for i in range(0,4):
    print(i)
324/42:
for i in range(1,5):
    print(i)
324/43:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

heart = [0,1,1,0,0,1,1,0,
         0,1,1,0,0,1,1,0,
         1,1,1,1,1,1,1,1,
         1,1,1,1,1,1,1,1,
         0,1,1,1,1,1,1,0,
         0,0,1,1,1,1,0,0,
         0,0,0,1,1,1,0,0,
         0,0,0,0,1,0,0,0
        ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
for i,j in zip((1, len(shapes)+1),(1, len(shapes)+1)):
    if heart[i] == 1:
        canvas.paste(images[j], shapes[i][0])
    else:
        i+=1

# Save the collage
canvas.save('heart4.jpg')
324/44:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

heart = [0,1,1,0,0,1,1,0,
         0,1,1,0,0,1,1,0,
         1,1,1,1,1,1,1,1,
         1,1,1,1,1,1,1,1,
         0,1,1,1,1,1,1,0,
         0,0,1,1,1,1,0,0,
         0,0,0,1,1,1,0,0,
         0,0,0,0,1,0,0,0
        ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
for i,j in zip((1, len(shapes)+1),(1, len(shapes)+1)):
    if i<65 and heart[i] == 1:
        canvas.paste(images[j], shapes[i][0])
    else:
        i+=1

# Save the collage
canvas.save('heart4.jpg')
324/45: heart
324/46: heart.count(1)
324/47: heart.count(0)
324/48: 38+26#heart.count(0)
324/49: heart.count(1)
324/50:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

heart = [0,1,1,0,0,1,1,0,
         0,1,1,0,0,1,1,0,
         1,1,1,1,1,1,1,1,
         1,1,1,1,1,1,1,1,
         0,1,1,1,1,1,1,0,
         0,0,1,1,1,1,0,0,
         0,0,0,1,1,1,0,0,
         0,0,0,0,1,0,0,0
        ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
for i in range(heart.count(1)):
    canvas.paste(images[i], shapes[i][0])

# Save the collage
canvas.save('heart4.jpg')
324/51:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

heart = [0,1,1,0,0,1,1,0,
         0,1,1,0,0,1,1,0,
         1,1,1,1,1,1,1,1,
         1,1,1,1,1,1,1,1,
         0,1,1,1,1,1,1,0,
         0,0,1,1,1,1,0,0,
         0,0,0,1,1,1,0,0,
         0,0,0,0,1,0,0,0
        ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
for i in range(len(shapes)):
    if heart[i] == 1:
        canvas.paste(images[i], shapes[i][0])
    else:
        i+=1

# Save the collage
canvas.save('heart4.jpg')
324/52:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

heart = [0,1,1,0,0,1,1,0,
         0,1,1,0,0,1,1,0,
         1,1,1,1,1,1,1,1,
         1,1,1,1,1,1,1,1,
         0,1,1,1,1,1,1,0,
         0,0,1,1,1,1,0,0,
         0,0,0,1,1,1,0,0,
         0,0,0,0,1,0,0,0
        ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
for i,j in zip(range(len(shapes),heart.count(1))):
    if heart[i] == 1:
        canvas.paste(images[j], shapes[i][0])
    else:
        i+=1

# Save the collage
canvas.save('heart4.jpg')
324/53:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

heart = [0,1,1,0,0,1,1,0,
         0,1,1,0,0,1,1,0,
         1,1,1,1,1,1,1,1,
         1,1,1,1,1,1,1,1,
         0,1,1,1,1,1,1,0,
         0,0,1,1,1,1,0,0,
         0,0,0,1,1,1,0,0,
         0,0,0,0,1,0,0,0
        ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
for i,j in zip(range(len(shapes)),heart.count(1)):
    if heart[i] == 1:
        canvas.paste(images[j], shapes[i][0])
    else:
        i+=1

# Save the collage
canvas.save('heart4.jpg')
324/54:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

heart = [0,1,1,0,0,1,1,0,
         0,1,1,0,0,1,1,0,
         1,1,1,1,1,1,1,1,
         1,1,1,1,1,1,1,1,
         0,1,1,1,1,1,1,0,
         0,0,1,1,1,1,0,0,
         0,0,0,1,1,1,0,0,
         0,0,0,0,1,0,0,0
        ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
for i,j in zip(range(len(shapes)),range(len(heart.count(1)))):
    if heart[i] == 1:
        canvas.paste(images[j], shapes[i][0])
    else:
        i+=1

# Save the collage
canvas.save('heart4.jpg')
324/55:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

heart = [0,1,1,0,0,1,1,0,
         0,1,1,0,0,1,1,0,
         1,1,1,1,1,1,1,1,
         1,1,1,1,1,1,1,1,
         0,1,1,1,1,1,1,0,
         0,0,1,1,1,1,0,0,
         0,0,0,1,1,1,0,0,
         0,0,0,0,1,0,0,0
        ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
for i,j in zip(range(len(shapes)),range(heart.count(1))):
    if heart[i] == 1:
        canvas.paste(images[j], shapes[i][0])
    else:
        i+=1

# Save the collage
canvas.save('heart4.jpg')
324/56:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

heart = [0,1,1,0,0,1,1,0,
         0,1,1,0,0,1,1,0,
         1,1,1,1,1,1,1,1,
         1,1,1,1,1,1,1,1,
         0,1,1,1,1,1,1,0,
         0,0,1,1,1,1,0,0,
         0,0,0,1,1,1,0,0,
         0,0,0,0,1,0,0,0
        ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
j = 0
for i in range(len(shapes)):
    if heart[i] == 1:
        j+=1
        canvas.paste(images[j], shapes[i][0])
    else:
        i+=1

# Save the collage
canvas.save('heart4.jpg')
324/57: j
324/58:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

heart = [0,1,1,0,0,1,1,0,
         0,1,1,0,0,1,1,0,
         1,1,1,1,1,1,1,1,
         1,1,1,1,1,1,1,1,
         0,1,1,1,1,1,1,0,
         0,0,1,1,1,1,0,0,
         0,0,0,1,1,1,0,0,
         0,0,0,0,1,0,0,0
        ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
j = 0
for i in range(len(shapes)):
    if heart[i] == 1:
        canvas.paste(images[j], shapes[i][0])
        j+=1
    else:
        i+=1

# Save the collage
canvas.save('heart4.jpg')
325/1:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# heart = [0,1,1,0,0,1,1,0,
#          0,1,1,0,0,1,1,0,
#          1,1,1,1,1,1,1,1,
#          1,1,1,1,1,1,1,1,
#          0,1,1,1,1,1,1,0,
#          0,0,1,1,1,1,0,0,
#          0,0,0,1,1,1,0,0,
#          0,0,0,0,1,0,0,0
#         ]
circle = [0,0,0,1,1,0,0,0,
          0,0,1,1,1,1,0,0,
          0,1,1,1,1,1,1,0,
          1,1,1,1,1,1,1,1,
          1,1,1,1,1,1,1,1,
          0,1,1,1,1,1,1,0,
          0,0,1,1,1,1,0,0,
          0,0,0,1,1,0,0,0
        ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
j = 0
for i in range(len(shapes)):
    if circle[i] == 1:
        canvas.paste(images[j], shapes[i][0])
        j+=1
    else:
        i+=1

# Save the collage
canvas.save('circle.jpg')
325/2:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# heart = [0,1,1,0,0,1,1,0,
#          0,1,1,0,0,1,1,0,
#          1,1,1,1,1,1,1,1,
#          1,1,1,1,1,1,1,1,
#          0,1,1,1,1,1,1,0,
#          0,0,1,1,1,1,0,0,
#          0,0,0,1,1,1,0,0,
#          0,0,0,0,1,0,0,0
#         ]
# circle = [0,0,0,1,1,0,0,0,
#           0,0,1,1,1,1,0,0,
#           0,1,1,1,1,1,1,0,
#           1,1,1,1,1,1,1,1,
#           1,1,1,1,1,1,1,1,
#           0,1,1,1,1,1,1,0,
#           0,0,1,1,1,1,0,0,
#           0,0,0,1,1,0,0,0
#          ]
pentagon = [0,0,0,1,0,0,0,0,
            0,0,1,1,1,1,0,0,
            0,1,1,1,1,1,1,0,
            1,1,1,1,1,1,1,1,
            0,1,1,1,1,1,1,0,
            0,0,1,1,1,1,0,0,
            0,0,1,1,1,1,0,0,
            0,0,1,1,1,1,0,0
           ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
j = 0
for i in range(len(shapes)):
    if pentagon[i] == 1:
        canvas.paste(images[j], shapes[i][0])
        j+=1
    else:
        i+=1

# Save the collage
canvas.save('pentagon.jpg')
325/3:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# heart = [0,1,1,0,0,1,1,0,
#          0,1,1,0,0,1,1,0,
#          1,1,1,1,1,1,1,1,
#          1,1,1,1,1,1,1,1,
#          0,1,1,1,1,1,1,0,
#          0,0,1,1,1,1,0,0,
#          0,0,0,1,1,1,0,0,
#          0,0,0,0,1,0,0,0
#         ]
# circle = [0,0,0,1,1,0,0,0,
#           0,0,1,1,1,1,0,0,
#           0,1,1,1,1,1,1,0,
#           1,1,1,1,1,1,1,1,
#           1,1,1,1,1,1,1,1,
#           0,1,1,1,1,1,1,0,
#           0,0,1,1,1,1,0,0,
#           0,0,0,1,1,0,0,0
#          ]
pentagon = [0,0,0,0,1,0,0,0,
            0,0,1,1,1,1,0,0,
            0,1,1,1,1,1,1,0,
            1,1,1,1,1,1,1,1,
            0,1,1,1,1,1,1,0,
            0,0,1,1,1,1,1,0,
            0,0,1,1,1,1,0,0,
            0,0,1,1,1,1,0,0
           ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
j = 0
for i in range(len(shapes)):
    if pentagon[i] == 1:
        canvas.paste(images[j], shapes[i][0])
        j+=1
    else:
        i+=1

# Save the collage
canvas.save('pentagon.jpg')
325/4:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# heart = [0,1,1,0,0,1,1,0,
#          0,1,1,0,0,1,1,0,
#          1,1,1,1,1,1,1,1,
#          1,1,1,1,1,1,1,1,
#          0,1,1,1,1,1,1,0,
#          0,0,1,1,1,1,0,0,
#          0,0,0,1,1,1,0,0,
#          0,0,0,0,1,0,0,0
#         ]
# circle = [0,0,0,1,1,0,0,0,
#           0,0,1,1,1,1,0,0,
#           0,1,1,1,1,1,1,0,
#           1,1,1,1,1,1,1,1,
#           1,1,1,1,1,1,1,1,
#           0,1,1,1,1,1,1,0,
#           0,0,1,1,1,1,0,0,
#           0,0,0,1,1,0,0,0
#          ]
pentagon = [0,0,0,1,0,0,0,0,
            0,0,1,1,1,1,0,0,
            0,1,1,1,1,1,1,0,
            1,1,1,1,1,1,1,1,
            0,1,1,1,1,1,1,0,
            0,0,1,1,1,1,1,0,
            0,0,1,1,1,1,0,0,
            0,0,1,1,1,1,0,0
           ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
j = 0
for i in range(len(shapes)):
    if pentagon[i] == 1:
        canvas.paste(images[j], shapes[i][0])
        j+=1
    else:
        i+=1

# Save the collage
canvas.save('pentagon.jpg')
325/5:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# heart = [0,1,1,0,0,1,1,0,
#          0,1,1,0,0,1,1,0,
#          1,1,1,1,1,1,1,1,
#          1,1,1,1,1,1,1,1,
#          0,1,1,1,1,1,1,0,
#          0,0,1,1,1,1,0,0,
#          0,0,0,1,1,1,0,0,
#          0,0,0,0,1,0,0,0
#         ]
# circle = [0,0,0,1,1,0,0,0,
#           0,0,1,1,1,1,0,0,
#           0,1,1,1,1,1,1,0,
#           1,1,1,1,1,1,1,1,
#           1,1,1,1,1,1,1,1,
#           0,1,1,1,1,1,1,0,
#           0,0,1,1,1,1,0,0,
#           0,0,0,1,1,0,0,0
#          ]
# pentagon = [0,0,0,1,0,0,0,0,
#             0,0,1,1,1,1,0,0,
#             0,1,1,1,1,1,1,0,
#             1,1,1,1,1,1,1,1,
#             0,1,1,1,1,1,1,0,
#             0,0,1,1,1,1,1,0,
#             0,0,1,1,1,1,0,0,
#             0,0,1,1,1,1,0,0
#            ]
rectangle = [0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,
             1,1,1,1,1,1,1,1,
             1,1,1,1,1,1,1,1,
             1,1,1,1,1,1,1,1,
             1,1,1,1,1,1,1,1,
             0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,
            ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
j = 0
for i in range(len(shapes)):
    if rectangle[i] == 1:
        canvas.paste(rectangle[j], shapes[i][0])
        j+=1
    else:
        i+=1

# Save the collage
canvas.save('rectangle.jpg')
325/6:
from PIL import Image, ImageDraw
import numpy as np
import random
import os

def normalize_image_size(image_path, size):
    # Load image from file
    image = Image.open(image_path)
    
    # Calculate new size while maintaining aspect ratio
    width, height = image.size
    if width > height:
        new_width = size
        new_height = int(height * size / width)
    else:
        new_height = size
        new_width = int(width * size / height)
    
    # Resize image
    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
    
    return resized_image



# Load the images to be used in the collage
images, img_size = [],[]

for i in os.listdir('Examples5'):
    img = Image.open('Examples5/'+i)
    img_size.append(img.size)
    
size = max(min(img_size))

# Define the shapes to use in the collage
# shapes = [
#     ((0, 0), (500, 500)),
#     ((500, 0), (1000, 500)),
#     ((0, 500), (500, 1000)),
#     ((500, 500), (1000, 1000))
# ]
# s, t, u, v, w, x, y, z = 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000
s, t, u, v, w, x, y, z = size, size*2, size*3, size*4, size*5, size*6, size*7, size*8
shapes = [
    ((0, 0), (s, s)), ((s, 0), (t, s)), ((t, 0), (u, s)), ((u, 0), (v, s)), ((v, 0), (w, s)), ((w, 0), (x, s)), ((x, 0), (y, s)), ((y, 0), (z, s)),
    ((0, s), (s, t)), ((s, s), (t, t)), ((t, s), (u, t)), ((u, s), (v, t)), ((v, s), (w, t)), ((w, s), (x, t)), ((x, s), (y, t)), ((y, s), (z, t)),
    ((0, t), (s, u)), ((s, t), (t, u)), ((t, t), (u, u)), ((u, t), (v, u)), ((v, t), (w, u)), ((w, t), (x, u)), ((x, t), (y, u)), ((y, t), (z, u)),
    ((0, u), (s, v)), ((s, u), (t, v)), ((t, u), (u, v)), ((u, u), (v, v)), ((v, u), (w, v)), ((w, u), (x, v)), ((x, u), (y, v)), ((y, u), (z, v)),
    ((0, v), (s, w)), ((s, v), (t, w)), ((t, v), (u, w)), ((u, v), (v, w)), ((v, v), (w, w)), ((w, v), (x, w)), ((x, v), (y, w)), ((y, v), (z, w)),
    ((0, w), (s, x)), ((s, w), (t, x)), ((t, w), (u, x)), ((u, w), (v, x)), ((v, w), (w, x)), ((w, w), (x, x)), ((x, w), (y, x)), ((y, w), (z, x)),
    ((0, x), (s, y)), ((s, x), (t, y)), ((t, x), (u, y)), ((u, x), (v, y)), ((v, x), (w, y)), ((w, x), (x, y)), ((x, x), (y, y)), ((y, x), (z, y)),
    ((0, y), (s, z)), ((s, y), (t, z)), ((t, y), (u, z)), ((u, y), (v, z)), ((v, y), (w, z)), ((w, y), (x, z)), ((x, y), (y, z)), ((y, y), (z, z)),
]

# Shuffle the shapes to add some randomness to the collage
# random.shuffle(shapes)

# Define the dimensions of the final collage
canvas_width = size*9
canvas_height = size*9

# Create a new image to use as the canvas
canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))

# heart = [0,1,1,0,0,1,1,0,
#          0,1,1,0,0,1,1,0,
#          1,1,1,1,1,1,1,1,
#          1,1,1,1,1,1,1,1,
#          0,1,1,1,1,1,1,0,
#          0,0,1,1,1,1,0,0,
#          0,0,0,1,1,1,0,0,
#          0,0,0,0,1,0,0,0
#         ]
# circle = [0,0,0,1,1,0,0,0,
#           0,0,1,1,1,1,0,0,
#           0,1,1,1,1,1,1,0,
#           1,1,1,1,1,1,1,1,
#           1,1,1,1,1,1,1,1,
#           0,1,1,1,1,1,1,0,
#           0,0,1,1,1,1,0,0,
#           0,0,0,1,1,0,0,0
#          ]
# pentagon = [0,0,0,1,0,0,0,0,
#             0,0,1,1,1,1,0,0,
#             0,1,1,1,1,1,1,0,
#             1,1,1,1,1,1,1,1,
#             0,1,1,1,1,1,1,0,
#             0,0,1,1,1,1,1,0,
#             0,0,1,1,1,1,0,0,
#             0,0,1,1,1,1,0,0
#            ]
rectangle = [0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,
             1,1,1,1,1,1,1,1,
             1,1,1,1,1,1,1,1,
             1,1,1,1,1,1,1,1,
             1,1,1,1,1,1,1,1,
             0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,
            ]

for i in os.listdir('Examples5'):
    img = normalize_image_size('Examples5/'+i, size)
    images.append(img)

# Paste the images onto the canvas
j = 0
for i in range(len(shapes)):
    if rectangle[i] == 1:
        canvas.paste(images[j], shapes[i][0])
        j+=1
    else:
        i+=1

# Save the collage
canvas.save('rectangle.jpg')
325/7:
import math

def nearest_square(num):
    root = int(math.sqrt(num))
    return root ** 2 if num - root ** 2 <= (root + 1) ** 2 - num else (root + 1) ** 2

# Example usage:
print(nearest_square(27)) # Output: 25
print(nearest_square(33)) # Output: 36
325/8: 36-25
325/9:
for i in range(25,36):
    print(i)
325/10:
for i in range(25,37):
    print(i)
325/11:
import math

def nearest_square(num):
    root = int(math.sqrt(num))
    return root ** 2 if num - root ** 2 <= (root + 1) ** 2 - num else (root + 1) ** 2

# Example usage:
print(nearest_square(27)) # Output: 25
print(nearest_square(30)) # Output: 36
325/12:
import math

def nearest_square(num):
    root = int(math.sqrt(num))
    return root ** 2 if num - root ** 2 <= (root + 1) ** 2 - num else (root + 1) ** 2

# Example usage:
print(nearest_square(27)) # Output: 25
print(nearest_square(31)) # Output: 36
325/13:
import math

def nearest_square(num):
    root = int(math.sqrt(num))
    return root ** 2 if num - root ** 2 >= (root + 1) ** 2 - num else (root + 1) ** 2

# Example usage:
print(nearest_square(27)) # Output: 25
print(nearest_square(31)) # Output: 36
325/14:
import math

def nearest_square(num):
    root = int(math.sqrt(num))
    return root ** 2 if num - root ** 2 <= (root + 1) ** 2 - num else (root + 1) ** 2

# Example usage:
print(nearest_square(27)) # Output: 25
print(nearest_square(31)) # Output: 36
325/15:
import math

def lower_square(num):
    root = int(math.sqrt(num))
    return root ** 2

# Example usage:
print(lower_square(27)) # Output: 25
print(lower_square(33)) # Output: 25
325/16:
import math

def lower_square(num):
    root = int(math.sqrt(num))
    return root ** 2

# Example usage:
print(lower_square(27)) # Output: 25
print(lower_square(35)) # Output: 25
325/17:
import math

def lower_square(num):
    root = int(math.sqrt(num))
    return root ** 2

# Example usage:
print(lower_square(27)) # Output: 25
print(lower_square(36)) # Output: 25
325/18: math.square(num)
325/19:
import math

def higher_square(num):
    root = int(math.sqrt(num))
    return (root + 1) ** 2

# Example usage:
print(higher_square(27)) # Output: 36
print(higher_square(33)) # Output: 36
325/20:
import math

def higher_square(num):
    root = int(math.sqrt(num))
    return (root + 1) ** 2

# Example usage:
print(higher_square(27)) # Output: 36
print(higher_square(35)) # Output: 36
325/21:
import math

def higher_square(num):
    root = int(math.sqrt(num))
    return (root + 1) ** 2

# Example usage:
print(higher_square(27)) # Output: 36
print(higher_square(37)) # Output: 36
325/22:

import math

def nearest_square(num):
    root = int(math.sqrt(num))
    return root ** 2 if num - root ** 2 <= (root + 1) ** 2 - num else (root + 1) ** 2

# Example usage:
print(nearest_square(27)) # Output: 25
print(nearest_square(31)) # Output: 36
325/23:
import math

def higher_square(num):
    root = int(math.sqrt(num))
    return (root + 1) ** 2

# Example usage:
print(higher_square(151))
325/24:
import math

def higher_square(num):
    root = int(math.sqrt(num))
    return (root + 1) #** 2

# Example usage:
print(higher_square(151))
325/25:
for i in range(0, 13):
    print(i)
325/26:
for i in range(13):
    print(i)
325/27:
import math

def higher_square(num):
    root = int(math.sqrt(num))
    return (root + 1) #** 2

tuples_lst = []
var = []
siz = 68

length = higher_square(151)

for i in range(length):
    i+=1
    var.append(siz*i)
var
# tuples_lst.append()
325/28:
import math

def higher_square(num):
    root = int(math.sqrt(num))
    return (root + 1) #** 2

tuples_lst = [0]
var = []
siz = 68

length = higher_square(151)

for i in range(length):
    i+=1
    var.append(siz*i)

for j in 
tuples_lst.append()
325/29:
import math

def higher_square(num):
    root = int(math.sqrt(num))
    return (root + 1) #** 2

tuples_lst = [0]
var = []
siz = 68

length = higher_square(151)

for i in range(length):
    i+=1
    var.append(siz*i)
var
# tuples_lst.append()
325/30:
import math

def higher_square(num):
    root = int(math.sqrt(num))
    return (root + 1) #** 2

tuples_lst = [0,]
var = []
siz = 68

length = higher_square(151)

for i in range(length):
    i+=1
    var.append(siz*i)
var
# tuples_lst.append()
325/31:
import math

def higher_square(num):
    root = int(math.sqrt(num))
    return (root + 1) #** 2

tuples_lst = []
tuples_lst.append(0)
var = []
siz = 68

length = higher_square(151)

for i in range(length):
    i+=1
    var.append(siz*i)
var
# tuples_lst.append()
325/32:
import math

def higher_square(num):
    root = int(math.sqrt(num))
    return (root + 1) #** 2

tuples_lst = []
tuples_lst.insert(0,0)
var = []
siz = 68

length = higher_square(151)

for i in range(length):
    i+=1
    var.append(siz*i)
var
# tuples_lst.append()
325/33:
import math

def higher_square(num):
    root = int(math.sqrt(num))
    return (root + 1) #** 2

tuples_lst = []
var = []
siz = 68

length = higher_square(151)

for i in range(length):
    i+=1
    var.append(siz*i)
tuples_lst.insert(0,0)
var
# tuples_lst.append()
325/34:
import math

def higher_square(num):
    root = int(math.sqrt(num))
    return (root + 1) #** 2

tuples_lst = []
var = []
siz = 68

length = higher_square(151)

for i in range(length):
    i+=1
    var.append(siz*i)
tuples_lst.append(0)
var
# tuples_lst.append()
325/35:
import math

def higher_square(num):
    root = int(math.sqrt(num))
    return (root + 1) #** 2

tuples_lst = []
var = []
siz = 68

length = higher_square(151)

for i in range(length):
    i+=1
    var.append(siz*i)
tuples_lst.append(str(0))
var
# tuples_lst.append()
325/36:
import math

def higher_square(num):
    root = int(math.sqrt(num))
    return (root + 1) #** 2

tuples_lst = []
var = []
siz = 68

length = higher_square(151)

for i in range(length):
    i+=1
    var.append(siz*i)
tuples_lst.append('0')
var
# tuples_lst.append()
325/37:
import math

def higher_square(num):
    root = int(math.sqrt(num))
    return (root + 1) #** 2

tuples_lst = []
var = [0]
siz = 68

length = higher_square(151)

for i in range(length):
    i+=1
    var.append(siz*i)
var
# tuples_lst.append()
326/1:
for i in range([1,1,1,1]):
    print(i)
326/2:
for i in [1,1,1,1]:
    print(i)
326/3:
gg = open("login.txt", "r+")
inc = next(gg)
print(inc)
326/4:
gg = open("login.txt", "r+")
inc = next(gg)
print(inc)
326/5: gg.readlines()
326/6: gg.readline()
326/7: gg.readlines()
326/8: gg.readlines()
326/9:
gg = open("login.txt", "r+")
inc = next(gg)
print(inc)
326/10: gg.readlines()
326/11:
gg = open("login.txt", "r+")
gg.readlines()
326/12: gg.readlines()[0]
326/13: gg.readlines(0)
326/14: gg.readlines()
326/15:
gg = open("login.txt", "r+")
gg.readlines()
326/16: gg.readlines()
326/17:
gg = open("login.txt", "r+")
gg.readlines()[0]
326/18:
gg = open("login.txt", "r+")
gg.readlines()[0][:-2]
326/19:
gg = open("login.txt", "r+")
gg.readlines()[0][:-1]
326/20:
gg = open("login.txt", "r+")
print(gg.readlines()[0][:-1])
print(gg.readlines()[2])
326/21:
gg = open("login.txt", "r+")
a,b = gg.readlines()[0][:-1], gg.readlines()[2]
print(a)
print(b)
326/22:
gg = open("login.txt", "r+")
a = gg.readlines()
print(a[0][:-1])
print(a[2])
326/23:
log = open("login.txt", "r+")
a = logg.readlines()
print(a[0][:-1])
print(a[2])
326/24:
log = open("login.txt", "r+")
a = log.readlines()
print(a[0][:-1])
print(a[2])
326/25:
log = open("login.txt", "r+")
a = log.readlines()
print(a[0][:-1])
print(a[2])
326/26:
log = open("login.txt", "r+")
a = log.readlines()
print(a[0][:-1])
print(a[1])
326/27:
log = open("login.txt", "r+")
log = log.readlines()
print(log[0][:-1])
print(log[1])
328/1:
import configparser

config_writer = configparser.ConfigParser()

config_writer['Default'] = {
    'email' : 'Config Parser',
    'password' : 'Error'
}

with open('login.ini', 'w') as configfile:
    config_writer.write(configfile)
328/2:
config_reader = configparser.ConfigParser()
config_reader.read('login.ini')
config_reader.sections()
328/3:
config_reader = configparser.ConfigParser()
config_reader.read('login.ini')
config_reader['Default']
328/4:
config_reader = configparser.ConfigParser()
config_reader.read('login.ini')
config_reader['Default']['email']
328/5:
config_reader = configparser.ConfigParser()
config_reader.read('login.ini')
print(config_reader['Default']['email'])
print(config_reader['Default']['password'])
   1: %history -g -f Untitled.ipynb
   2: %history -g -f Untitled.ipynb
